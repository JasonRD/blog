<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>jason's 博客</title><link>https://jasonrd.github.io/blog/</link><description>Recent content on jason's 博客</description><generator>Hugo -- gohugo.io</generator><language>en</language><lastBuildDate>Sat, 28 May 2022 00:00:00 +0000</lastBuildDate><atom:link href="https://jasonrd.github.io/blog/index.xml" rel="self" type="application/rss+xml"/><item><title>Archives</title><link>https://jasonrd.github.io/blog/archives/</link><pubDate>Sat, 28 May 2022 00:00:00 +0000</pubDate><guid>https://jasonrd.github.io/blog/archives/</guid><description/></item><item><title>从 kubectl exec 异常问题开始</title><link>https://jasonrd.github.io/blog/p/kubectl-exec-deepin/</link><pubDate>Tue, 01 Mar 2022 00:00:00 +0000</pubDate><guid>https://jasonrd.github.io/blog/p/kubectl-exec-deepin/</guid><description>&lt;img src="https://jasonrd.github.io/blog/p/kubectl-exec-deepin/helena-hertz-wWZzXlDpMog-unsplash.jpg" alt="Featured image of post 从 kubectl exec 异常问题开始" />&lt;h2 id="问题现象">问题现象&lt;/h2>
&lt;p>最近，在工作想要使用 kubectl exec 进入容器排查问题，结果返回下面异常：&lt;/p>
&lt;p>&lt;img src="https://jasonrd.github.io/blog/blog/p/kubectl-exec-deepin/img/exec-failure.png"
width="2828"
height="158"
srcset="https://jasonrd.github.io/blog/blog/p/kubectl-exec-deepin/img/exec-failure_hufe0606b0bd44839a8e6bff7d797c72f2_189000_480x0_resize_box_3.png 480w, https://jasonrd.github.io/blog/blog/p/kubectl-exec-deepin/img/exec-failure_hufe0606b0bd44839a8e6bff7d797c72f2_189000_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="1789"
data-flex-basis="4295px"
>&lt;/p>
&lt;h2 id="排查过程">排查过程&lt;/h2>
&lt;p>我们知道 kubectl exec 的执行链路是 client -&amp;gt; kube-apiserver -&amp;gt; kubelet -&amp;gt; docker。&lt;/p>
&lt;p>登录 Kubelet 宿主机查看 kubelet 错误日志，发现有相同的报错日志，这说明是 kubelet 和 docker 之间链路又问题。通过 kubelet 日志中不能定为到问题具体原因。然后，我们试图通过抓包，希望在数据包中能发现一些线索。&lt;/p>
&lt;p>在抓包数据结果中我们发现关键字为 exec 的请求，该会话的目的地址为 A:20880, http header 中 Host 为 B:10250 （也就是是物理机上 kubelet 的 httpserver 地址）。我们查询 A 这个IP，发现是业务应用的容器 IP。&lt;/p>
&lt;p>这就比较奇怪了，正常 apiserver 发送 exec 请求为什么转发到了容器的 20880 端口。并且数据包中包含 kubectl (&amp;ldquo;User-Agent: kubectl&amp;rdquo;) http header。难道 kubectl exec 请求发送到 docker 的请求（xxxx/exec/token）被转发到了容器。通过再次尝试执行 kubectl exec 并抓包，发现执行命令和发送到 20880 端口请求匹配，这验证了我们的猜测。&lt;/p>
&lt;p>到此就把问题范围缩小到宿主机网络上，我们知道 kube-proxy 会通过 ipvs 或 iptables 对创建的 nodeport 或 service vip 的请求进行拦截和转发。我们查看 conntrack 请求记录：&lt;/p>
&lt;p>&lt;img src="https://jasonrd.github.io/blog/blog/p/kubectl-exec-deepin/img/exec-conntrack.png"
width="2706"
height="64"
srcset="https://jasonrd.github.io/blog/blog/p/kubectl-exec-deepin/img/exec-conntrack_huca2d8b10dc5e6cf266293d7184a5d6ec_84921_480x0_resize_box_3.png 480w, https://jasonrd.github.io/blog/blog/p/kubectl-exec-deepin/img/exec-conntrack_huca2d8b10dc5e6cf266293d7184a5d6ec_84921_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="4228"
data-flex-basis="10147px"
>&lt;/p>
&lt;p>发现一条请求 127.0.0.1:33589 的记录，并且转发到的地址 A:20880 也和我们抓包的结果匹配。然后查看 33589 端口，发现该端口就是被 kubelet 占用。然后，我们查询 service，发现 33589 端口同时是 B 容器的应用 service 的 nodeport。到此问题根本原因定位到了，nodeport 端口和 kubelet 启动的转发端口冲突了，导致发送 exec 请求转发到了应用容器的 20880 端口（dubbo端口）。&lt;/p>
&lt;h2 id="继续深挖">继续深挖&lt;/h2>
&lt;p>事情到此并没有结束，上面我们只是定位到了具体问题原因。其实还存在两个问题：&lt;/p>
&lt;ol>
&lt;li>对 kubectl exec 的执行过程还没没挖透；&lt;/li>
&lt;li>如何避免该问题？&lt;/li>
&lt;/ol>
&lt;h3 id="kubectl-exec-的执行过程">kubectl exec 的执行过程&lt;/h3>
&lt;p>问题没有快速定位，主要原因还是对 kubectl exec 执行流程不熟。下面来了解一下 kubectl 是怎么执行的。&lt;/p>
&lt;p>本文基于 1.14.6 源码进行研究。&lt;/p>
&lt;p>首先，简单了解一下 kubelet 架构：&lt;/p>
&lt;p>&lt;img src="https://jasonrd.github.io/blog/blog/p/kubectl-exec-deepin/img/kubelet-arch.png"
width="1576"
height="368"
srcset="https://jasonrd.github.io/blog/blog/p/kubectl-exec-deepin/img/kubelet-arch_hu6568dec694f664b38c0d8a13d58d2338_47592_480x0_resize_box_3.png 480w, https://jasonrd.github.io/blog/blog/p/kubectl-exec-deepin/img/kubelet-arch_hu6568dec694f664b38c0d8a13d58d2338_47592_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="428"
data-flex-basis="1027px"
>&lt;/p>
&lt;p>kubelet 中有上面几个部分：container manager、dockershim、http server、streaming server。kubelet 早期直接调用 docker api 管理容器，后来为了适配更多的 runtime 抽象出了一个接入层 cri。同时，为了兼容 docker 的 API，kubelet 代码中实现了这个叫 dockershim 的部分。这样就对上层屏蔽了底层 runtime。http server 通常使用 10250 对外提供 API 服务。streaming server 是需要和容器进行交互时的一个代理服务。&lt;/p>
&lt;p>在默认情况下，用户执行 kubectl exec 简化流程如下：&lt;/p>
&lt;p>&lt;img src="https://jasonrd.github.io/blog/blog/p/kubectl-exec-deepin/img/kubelet-streamer.png"
width="1494"
height="1000"
srcset="https://jasonrd.github.io/blog/blog/p/kubectl-exec-deepin/img/kubelet-streamer_hu77ee1ab196b176433e041293050f2664_131743_480x0_resize_box_3.png 480w, https://jasonrd.github.io/blog/blog/p/kubectl-exec-deepin/img/kubelet-streamer_hu77ee1ab196b176433e041293050f2664_131743_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="149"
data-flex-basis="358px"
>&lt;/p>
&lt;ol>
&lt;li>terminal 中键入 kubectl exec xxx 指令，kubectl 发送请求到 apiserver https://apiserver/api/v1/namespaces/{ns}/pods/{pod}/exec?command=bash&amp;amp;container=dragon-claw&amp;amp;stdin=true&amp;amp;stdout=true&amp;amp;tty=true；&lt;/li>
&lt;li>apiserver 接到请求后，将请求转发到 kubelet， node:10250/api/v1/exec/{ns}/{podid}/{container}。kubelet httpserver 接收到请求后：
&lt;ul>
&lt;li>首先，向 dockershim 发起 getExec 请求，返回一个流地址 url （exec/{token}）；&lt;/li>
&lt;li>然后，kubelet 请求 exec/xxxx url 到 streaming server，streaming server 接收到请求后，response upgrade 将连接升级成为 spdy 或 ws 连接；&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>kubelet 收到 upgrade reponse 后，将该 reponse 直接返回给 apiserver，到此 apiserver -&amp;gt; kubelet -&amp;gt; streaming server -&amp;gt; docker 之间整个通道建立完成；&lt;/li>
&lt;li>到此，用户可以在 terminal 中键入命令在容器中执行；&lt;/li>
&lt;/ol>
&lt;p>&lt;img src="https://jasonrd.github.io/blog/blog/p/kubectl-exec-deepin/img/exec-uml.jpg"
width="2536"
height="1506"
srcset="https://jasonrd.github.io/blog/blog/p/kubectl-exec-deepin/img/exec-uml_hu2f1e02e515e351c311ec896043222feb_281732_480x0_resize_q75_box.jpg 480w, https://jasonrd.github.io/blog/blog/p/kubectl-exec-deepin/img/exec-uml_hu2f1e02e515e351c311ec896043222feb_281732_1024x0_resize_q75_box.jpg 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="168"
data-flex-basis="404px"
>&lt;/p>
&lt;p>其中， streaming server 是 kubelet 和 docker 之间的一个桥梁，他负责将请求转发给 docker（或者其他 runtime）。kubelet 访问 streaming server 的地址就是 127.0.0.1:{streaming sever port}。&lt;/p>
&lt;p>&lt;strong>而我们遇到问题中端口冲突，就是 streaming server 端口和 nodeport 冲突。kubelet 拿到 exec url 后，命中本地 iptables 规则，然后请求被转发到了 nodeport 关联的容器，返回上述错误。&lt;/strong>&lt;/p>
&lt;p>在源码研究过程中，参数 &amp;ndash;redirect-container-streaming 引起了我们的注意：&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">func (s *Server) getExec(request *restful.Request, response *restful.Response) {
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> .... 省略若干代码....
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> url, err := s.host.GetExec(podFullName, params.podUID, params.containerName, params.cmd, *streamOpts)
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> if err != nil {
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> streaming.WriteError(err, response.ResponseWriter)
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> return
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> }
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> if s.redirectContainerStreaming {
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> http.Redirect(response.ResponseWriter, request.Request, url.String(), http.StatusFound)
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> return
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> }
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> proxyStream(response.ResponseWriter, request.Request, url)
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">}
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>该参数开启后会并不会进行 proxyStream。而是直接向 apiserver 发送 302 跳转，流程变为如下：&lt;/p>
&lt;p>&lt;img src="https://jasonrd.github.io/blog/blog/p/kubectl-exec-deepin/img/kubelet-nostreamer.png"
width="1508"
height="1000"
srcset="https://jasonrd.github.io/blog/blog/p/kubectl-exec-deepin/img/kubelet-nostreamer_hu45498cf6e0b95b67adcec0e537e81e4b_116430_480x0_resize_box_3.png 480w, https://jasonrd.github.io/blog/blog/p/kubectl-exec-deepin/img/kubelet-nostreamer_hu45498cf6e0b95b67adcec0e537e81e4b_116430_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="150"
data-flex-basis="361px"
>&lt;/p>
&lt;p>该参数说明，如下：&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">--redirect-container-streaming Enables container streaming redirect. If false, kubelet will proxy container streaming data between apiserver and container runtime; if true, kubelet will return an http redirect to apiserver, and apiserver will access container runtime directly. The proxy approach is more secure, but introduces some overhead. The redirect approach is more performant, but less secure because the connection between apiserver and container runtime may not be authenticated.
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>另外，在 1.18 版本中我们发现该参数即将废弃，社区中已经在 kep &lt;a class="link" href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-node/1558-streaming-proxy-redirects#dependence-on-apiserver-redirects" target="_blank" rel="noopener"
>Cleaning up container streaming requests&lt;/a>中详细说明了后续下线计划（1.18 进行下线提示、1.20版本参数失效、1.22 参数被删除）。后续 apiserver 无法直接和 dockershim 通信。&lt;/p>
&lt;h3 id="如何避免端口冲突">如何避免端口冲突&lt;/h3>
&lt;p>经过源码阅读，我们了解了执行 kube exec 的流程，通过关闭 streaming server 可以避免 streaming server 端口和 nodeport 冲突。但是该方案只能在 1.20 版本前的集群中使用。&lt;/p>
&lt;p>另外，进一步思考，如果其他进程使用了一个随机端口是否也会出现该问题呢？&lt;/p>
&lt;p>还是有一定冲突概率的，在 &lt;a class="link" href="https://github.com/kubernetes/kubernetes/issues/85418" target="_blank" rel="noopener"
>#85418&lt;/a> issue 中就有人提出了该问题，从相关讨论中推荐解决方法是通过宿主机预留端口（net.ipv4.ip_local_port_range）解决。k8s apiserver 默认的 nodeport 端口范围为 30000-32767 （通过 &amp;ndash;service-node-port-range 参数配置），一般宿主机 net.ipv4.ip_local_port_range 默认范围为 32768-60999。而我们出现冲突，因为使用的某云 k8s 集群修改了 apiserver 参数为 30000-50000，导致出现端口冲突问题。&lt;/p>
&lt;p>其实，kube-proxy 为了避免端口冲突的问题，运行过程会监听所有的 nodeport 端口。但是，这存在一个鸡生蛋的问题。如果某个 nodeport 分配前已经被其他应用占用，或者 kube-proxy 重启，还是会存在端口冲突的问题。在 &lt;a class="link" href="https://github.com/kubernetes/kubernetes/issues/100643" target="_blank" rel="noopener"
>#100643&lt;/a> issue 中也进行了相关讨论，希望后续能有完美的解决方案。&lt;/p>
&lt;p>综上，目前解决方案下面几种：&lt;/p>
&lt;ol>
&lt;li>1.20 前版本可以通过 &amp;ndash;redirect-container-streaming 关闭 steaming server，避免 kubelet 和 nodeport 端口冲突；&lt;/li>
&lt;li>修改系统参数和 apiserver 端口范围，保证和宿主机随机端口范围不重合；&lt;/li>
&lt;li>其他技术，例如 &lt;a class="link" href="https://github.com/kubernetes/kubernetes/issues/100643" target="_blank" rel="noopener"
>#100643&lt;/a> issue 中提出的 ebpf。&lt;/li>
&lt;/ol>
&lt;h2 id="总结">总结&lt;/h2>
&lt;p>通过一个生产 &lt;code>kubectl exec&lt;/code>异常问题，我们了解了执行 exec 命令后，整个底层转发逻辑：&lt;/p>
&lt;ol>
&lt;li>apiserver 查询到 pod 所在 node ip，通过 nodeip:10250 端口向 kubelet 发起请求；&lt;/li>
&lt;li>kubelet 接收到请求后，向本地 runtime 获取 exec url。然后，1.20 之前会基于参数 &amp;ndash;redirect-container-streaming 有两种处理流程：
&lt;ol>
&lt;li>开启参数，通过 302 跳转方式，将 apiserver 请求重定向到 exec url；&lt;/li>
&lt;li>关闭参数，会先直接和 runtime 建立 exec 通道，然后将 apiserver 请求升级为 spdy 或 ws 连接；&lt;/li>
&lt;/ol>
&lt;/li>
&lt;li>后续 apisever 和 runtime 通道建立完成，client 就可以在 terminal 上执行命令了。&lt;/li>
&lt;/ol></description></item><item><title>云上 pod 下线引起短时服务不可用</title><link>https://jasonrd.github.io/blog/p/service-cause-failure/</link><pubDate>Mon, 01 Nov 2021 00:00:00 +0000</pubDate><guid>https://jasonrd.github.io/blog/p/service-cause-failure/</guid><description>&lt;img src="https://jasonrd.github.io/blog/p/service-cause-failure/luca-bravo-alS7ewQ41M8-unsplash.jpg" alt="Featured image of post 云上 pod 下线引起短时服务不可用" />&lt;h2 id="1-背景">1. 背景&lt;/h2>
&lt;p>近日，接连收到多个云上站点业务出现 502 问题反馈：&lt;/p>
&lt;p>&lt;img src="https://jasonrd.github.io/blog/blog/p/service-cause-failure/img/502-error-1.png"
width="2980"
height="934"
srcset="https://jasonrd.github.io/blog/blog/p/service-cause-failure/img/502-error-1_hucce7954739b6677286789b82e0e95e76_227221_480x0_resize_box_3.png 480w, https://jasonrd.github.io/blog/blog/p/service-cause-failure/img/502-error-1_hucce7954739b6677286789b82e0e95e76_227221_1024x0_resize_box_3.png 1024w"
loading="lazy"
alt="502-error-1"
class="gallery-image"
data-flex-grow="319"
data-flex-basis="765px"
>&lt;img src="https://jasonrd.github.io/blog/blog/p/service-cause-failure/img/502-error-2.png"
width="1858"
height="1096"
srcset="https://jasonrd.github.io/blog/blog/p/service-cause-failure/img/502-error-2_hub2cf4d7668d03fc5d2263866213f9d32_182813_480x0_resize_box_3.png 480w, https://jasonrd.github.io/blog/blog/p/service-cause-failure/img/502-error-2_hub2cf4d7668d03fc5d2263866213f9d32_182813_1024x0_resize_box_3.png 1024w"
loading="lazy"
alt="502-error-2"
class="gallery-image"
data-flex-grow="169"
data-flex-basis="406px"
>&lt;/p>
&lt;p>和业务负责人沟通后，应用确认加入了优雅下线逻辑。&lt;/p>
&lt;h2 id="2-排查过程">2. 排查过程&lt;/h2>
&lt;p>首先，查看网关日志：&lt;/p>
&lt;p>&lt;img src="https://jasonrd.github.io/blog/blog/p/service-cause-failure/img/troubleshoot-1.png"
width="2848"
height="364"
srcset="https://jasonrd.github.io/blog/blog/p/service-cause-failure/img/troubleshoot-1_hu34d8d5f831126b5a732d35045ac8e4b8_1033303_480x0_resize_box_3.png 480w, https://jasonrd.github.io/blog/blog/p/service-cause-failure/img/troubleshoot-1_hu34d8d5f831126b5a732d35045ac8e4b8_1033303_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="782"
data-flex-basis="1877px"
>&lt;/p>
&lt;p>&lt;img src="https://jasonrd.github.io/blog/blog/p/service-cause-failure/img/troubleshoot-2.png"
width="2812"
height="386"
srcset="https://jasonrd.github.io/blog/blog/p/service-cause-failure/img/troubleshoot-2_huc895b7a875e203a5b190847dbae686e8_916199_480x0_resize_box_3.png 480w, https://jasonrd.github.io/blog/blog/p/service-cause-failure/img/troubleshoot-2_huc895b7a875e203a5b190847dbae686e8_916199_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="728"
data-flex-basis="1748px"
>&lt;/p>
&lt;p>两次日志，都是请求 LB IP 出现 503 错误码后，然后网关将 LB IP 摘掉。&lt;/p>
&lt;p>分析为什么出现 503 错误码前，先了解一下容器下线逻辑：容器进行下线时，会调用 prestop 脚本执行下线前的操作。&lt;/p>
&lt;p>在 prestop 脚本中，首先 sleep 15s （不要问我为什么），然后调用 http://127.0.0.1:${APP_PORT}/ok.htm?down=true 接口通知 java 进程进行优雅下线。该接口调用成功后，再请求应用 ok 页面，进入下面逻辑：&lt;/p>
&lt;p>&lt;img src="https://jasonrd.github.io/blog/blog/p/service-cause-failure/img/troubleshoot-prestop.png"
width="1055"
height="614"
srcset="https://jasonrd.github.io/blog/blog/p/service-cause-failure/img/troubleshoot-prestop_hu07c34cbbf302e9275effdebb12b729a9_402200_480x0_resize_box_3.png 480w, https://jasonrd.github.io/blog/blog/p/service-cause-failure/img/troubleshoot-prestop_hu07c34cbbf302e9275effdebb12b729a9_402200_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="171"
data-flex-basis="412px"
>&lt;/p>
&lt;p>也就是返回 halting 数据和503状态码。&lt;/p>
&lt;p>k8s 中在将 Pod 进行下线（标记为 Terminating 状态）时，k8s endpoint controller 就将该 Pod ip 从 lb 或 service 后端列表中摘除。既然 lb/svc 已经将在该 pod IP 摘除，为什么仍然请求到 halting Pod 呢？&lt;/p>
&lt;p>在进入应用容器中进行抓包，并和应用负责人确认后，网关 -&amp;gt; lb -&amp;gt; pod 是使用 http 长连接方式。&lt;/p>
&lt;p>&lt;img src="https://jasonrd.github.io/blog/blog/p/service-cause-failure/img/troubleshoot-netflow.png"
width="1762"
height="788"
srcset="https://jasonrd.github.io/blog/blog/p/service-cause-failure/img/troubleshoot-netflow_hu2cc8ce7b54497507118a4db625573fcb_324383_480x0_resize_box_3.png 480w, https://jasonrd.github.io/blog/blog/p/service-cause-failure/img/troubleshoot-netflow_hu2cc8ce7b54497507118a4db625573fcb_324383_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="223"
data-flex-basis="536px"
>&lt;/p>
&lt;p>在 Pod 处于 terminating 状态时，通过 svc 请求时新建立的连接将不会转发到该 pod，但是已经建立的连接在 Pod 完全删除前仍可继续通信。所以，虽然 service 将 Pod IP 摘除，但是为了保证容器的优雅下线，已经建立的连接仍然可以继续处理业务，直到容器彻底被删除。&lt;/p>
&lt;p>我们用 python 简单写了一个使用 http 长连接客户端，进行一下测试：&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="kn">import&lt;/span> &lt;span class="nn">requests&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="kn">import&lt;/span> &lt;span class="nn">time&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">client&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">requests&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">session&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">headers&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">{&lt;/span>&lt;span class="s1">&amp;#39;Content-Type&amp;#39;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="s1">&amp;#39;application/json&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="s1">&amp;#39;Connection&amp;#39;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="s1">&amp;#39;keep-alive&amp;#39;&lt;/span>&lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">while&lt;/span> &lt;span class="kc">True&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">r&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">client&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">get&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s2">&amp;#34;http://xx.xx.16.137:8088/ok.htm&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">headers&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">headers&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1"># xx.xx.16.137 为应用 lb IP &lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nb">print&lt;/span> &lt;span class="n">r&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">status_code&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nb">print&lt;/span> &lt;span class="n">r&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">content&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">time&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">sleep&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>python client 请求应用容器：&lt;/p>
&lt;p>&lt;img src="https://jasonrd.github.io/blog/blog/p/service-cause-failure/img/troubleshoot-3.png"
width="1770"
height="592"
srcset="https://jasonrd.github.io/blog/blog/p/service-cause-failure/img/troubleshoot-3_hu29f5ad8b11850c039f2c666148aa389d_770926_480x0_resize_box_3.png 480w, https://jasonrd.github.io/blog/blog/p/service-cause-failure/img/troubleshoot-3_hu29f5ad8b11850c039f2c666148aa389d_770926_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="298"
data-flex-basis="717px"
>&lt;/p>
&lt;p>&lt;img src="https://jasonrd.github.io/blog/blog/p/service-cause-failure/img/troubleshoot-4.png"
width="1772"
height="570"
srcset="https://jasonrd.github.io/blog/blog/p/service-cause-failure/img/troubleshoot-4_hu7194f66dd6011e9b2627f0ff6ec5b69b_848304_480x0_resize_box_3.png 480w, https://jasonrd.github.io/blog/blog/p/service-cause-failure/img/troubleshoot-4_hu7194f66dd6011e9b2627f0ff6ec5b69b_848304_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="310"
data-flex-basis="746px"
>&lt;/p>
&lt;p>上面可以看到，在 15:30:06 将 pod 进行 kill 后，通过长连接仍然可以将请求转发到处于 terminating 的应用容器。直到 15s 后调用下线接口，请求返回 503。请求处于 halting 状态的 pod，server 端会主动 close 请求：&lt;/p>
&lt;p>&lt;img src="https://jasonrd.github.io/blog/blog/p/service-cause-failure/img/troubleshoot-5.png"
width="1298"
height="580"
srcset="https://jasonrd.github.io/blog/blog/p/service-cause-failure/img/troubleshoot-5_hub914d7d26274e94e5b363dd62be85e28_370469_480x0_resize_box_3.png 480w, https://jasonrd.github.io/blog/blog/p/service-cause-failure/img/troubleshoot-5_hub914d7d26274e94e5b363dd62be85e28_370469_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="223"
data-flex-basis="537px"
>&lt;/p>
&lt;p>另外，我们测试应用只有单个实例，也就是实例被删除后，lb 后端实例为 0，请求 lb 的新连接无法建立。所以，测试脚本会出现 connect refused 报错。&lt;/p>
&lt;h2 id="3-解决方法">3. 解决方法&lt;/h2>
&lt;p>综上可知，问题原因是 kill pod 后仍然会有流量进入到 terminating 状态的 pod，然后 15s 后 prestop 脚本通知进程进行下线逻辑（也就是 ok 页面返回 halting 和 503 状态码），当网关继续请求到该 pod 就会认为 lb 出现异常，将唯一的 lb 标记为不健康，从而出现 502 异常。&lt;/p>
&lt;p>其他站点未出现该问题原因是：网关直接转发到 pod ip，摘掉的是出现异常的 pod ip 。而有问题的站点对接的只是一个公有云 LB ip。&lt;/p>
&lt;p>&lt;img src="https://jasonrd.github.io/blog/blog/p/service-cause-failure/img/solution-1.png"
width="888"
height="768"
srcset="https://jasonrd.github.io/blog/blog/p/service-cause-failure/img/solution-1_huf2a69d255e32c6a8b07aa332757aa208_267530_480x0_resize_box_3.png 480w, https://jasonrd.github.io/blog/blog/p/service-cause-failure/img/solution-1_huf2a69d255e32c6a8b07aa332757aa208_267530_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="115"
data-flex-basis="277px"
>&lt;/p>
&lt;p>具体优化逻辑如下(如上图，针对单个pod):&lt;/p>
&lt;ol>
&lt;li>应用增加 connection filter，在应用进入优雅下线（被调用 http://127.0.0.1:${APP_PORT}/ok.htm?down=true）后，所有请求的 http 响应头中增加 connection:close（也就是使用短连接）；
&lt;ol>
&lt;li>因为 pod 处于 terminating 时，新连接不会进入该 pod；&lt;/li>
&lt;li>参考代码如下：
&lt;img src="https://jasonrd.github.io/blog/blog/p/service-cause-failure/img/solution-2.png"
width="1738"
height="1016"
srcset="https://jasonrd.github.io/blog/blog/p/service-cause-failure/img/solution-2_hu8faba495cdcf5af6c2ac4f3e2d23be71_653499_480x0_resize_box_3.png 480w, https://jasonrd.github.io/blog/blog/p/service-cause-failure/img/solution-2_hu8faba495cdcf5af6c2ac4f3e2d23be71_653499_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="171"
data-flex-basis="410px"
>&lt;/li>
&lt;/ol>
&lt;/li>
&lt;li>健康检查接口，修改为去掉 503 异常，避免网关检测到 503 异常时，直接摘掉 lb ip。
&lt;ol>
&lt;li>参考代码如下：
&lt;img src="https://jasonrd.github.io/blog/blog/p/service-cause-failure/img/solution-3.png"
width="1482"
height="696"
srcset="https://jasonrd.github.io/blog/blog/p/service-cause-failure/img/solution-3_hu095a7d17af90f9d81e0a9c0715e7a55d_439334_480x0_resize_box_3.png 480w, https://jasonrd.github.io/blog/blog/p/service-cause-failure/img/solution-3_hu095a7d17af90f9d81e0a9c0715e7a55d_439334_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="212"
data-flex-basis="511px"
>&lt;/li>
&lt;/ol>
&lt;/li>
&lt;/ol></description></item><item><title>Links</title><link>https://jasonrd.github.io/blog/links/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://jasonrd.github.io/blog/links/</guid><description>&lt;p>To use this feature, add &lt;code>links&lt;/code> section to frontmatter.&lt;/p>
&lt;p>This page&amp;rsquo;s frontmatter:&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;span class="lnt">5
&lt;/span>&lt;span class="lnt">6
&lt;/span>&lt;span class="lnt">7
&lt;/span>&lt;span class="lnt">8
&lt;/span>&lt;span class="lnt">9
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-yaml" data-lang="yaml">&lt;span class="line">&lt;span class="cl">&lt;span class="nt">links&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>- &lt;span class="nt">title&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">GitHub&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">description&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">GitHub is the world&amp;#39;s largest software development platform.&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">website&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">https://github.com&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">image&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">https://github.githubassets.com/images/modules/logos_page/GitHub-Mark.png&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>- &lt;span class="nt">title&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">TypeScript&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">description&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">TypeScript is a typed superset of JavaScript that compiles to plain JavaScript.&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">website&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">https://www.typescriptlang.org&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">image&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">ts-logo-128.jpg&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>&lt;code>image&lt;/code> field accepts both local and external images.&lt;/p></description></item><item><title>Search</title><link>https://jasonrd.github.io/blog/search/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://jasonrd.github.io/blog/search/</guid><description/></item><item><title>关于</title><link>https://jasonrd.github.io/blog/%E5%85%B3%E4%BA%8E/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://jasonrd.github.io/blog/%E5%85%B3%E4%BA%8E/</guid><description>&lt;p>欢迎来到我的小屋🛖&lt;/p></description></item></channel></rss>