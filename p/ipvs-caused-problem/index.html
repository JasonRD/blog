<!doctype html><html lang=en dir=ltr><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="上篇中分析 vip 转发实现，本文深度探究 ipvs 的不合理参数配置，会导致哪些问题"><title>k8s 环境中 ipvs 带来的问题</title><link rel=canonical href=https://jasonrd.github.io/blog/p/ipvs-caused-problem/><link rel=stylesheet href=/blog/scss/style.min.8191399262444ab68b72a18c97392f5349be20a1615d77445be51e974c144cff.css><meta property="og:title" content="k8s 环境中 ipvs 带来的问题"><meta property="og:description" content="上篇中分析 vip 转发实现，本文深度探究 ipvs 的不合理参数配置，会导致哪些问题"><meta property="og:url" content="https://jasonrd.github.io/blog/p/ipvs-caused-problem/"><meta property="og:site_name" content="jason's 博客"><meta property="og:type" content="article"><meta property="article:section" content="Post"><meta property="article:published_time" content="2022-01-05T00:00:00+00:00"><meta property="article:modified_time" content="2022-01-05T00:00:00+00:00"><meta property="og:image" content="https://jasonrd.github.io/blog/p/ipvs-caused-problem/matt-le-SJSpo9hQf7s-unsplash.jpg"><meta name=twitter:site content="@jasonxie666"><meta name=twitter:creator content="@jasonxie666"><meta name=twitter:title content="k8s 环境中 ipvs 带来的问题"><meta name=twitter:description content="上篇中分析 vip 转发实现，本文深度探究 ipvs 的不合理参数配置，会导致哪些问题"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://jasonrd.github.io/blog/p/ipvs-caused-problem/matt-le-SJSpo9hQf7s-unsplash.jpg"></head><body class=article-page><script>(function(){const e="StackColorScheme";localStorage.getItem(e)||localStorage.setItem(e,"auto")})()</script><script>(function(){const t="StackColorScheme",e=localStorage.getItem(t),n=window.matchMedia("(prefers-color-scheme: dark)").matches===!0;e=="dark"||e==="auto"&&n?document.documentElement.dataset.scheme="dark":document.documentElement.dataset.scheme="light"})()</script><div class="container main-container flex on-phone--column extended"><aside class="sidebar left-sidebar sticky"><button class="hamburger hamburger--spin" type=button id=toggle-menu aria-label=切换菜单>
<span class=hamburger-box><span class=hamburger-inner></span></span></button><header><figure class=site-avatar><a href=/blog/><img src=/blog/img/avatar_hu92a1f0f4dac05816138a9efb01424f59_787601_300x0_resize_box_3.png width=300 height=300 class=site-logo loading=lazy alt=Avatar></a>
<span class=emoji>🍥</span></figure><div class=site-meta><h1 class=site-name><a href=/blog>jason's 博客</a></h1><h2 class=site-description>Welcom</h2></div></header><ol class=social-menu><li><a href=https://github.com/JasonRD target=_blank title=GitHub rel=me><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-brand-github" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M9 19c-4.3 1.4-4.3-2.5-6-3m12 5v-3.5c0-1 .1-1.4-.5-2 2.8-.3 5.5-1.4 5.5-6a4.6 4.6.0 00-1.3-3.2 4.2 4.2.0 00-.1-3.2s-1.1-.3-3.5 1.3a12.3 12.3.0 00-6.2.0C6.5 2.8 5.4 3.1 5.4 3.1a4.2 4.2.0 00-.1 3.2A4.6 4.6.0 004 9.5c0 4.6 2.7 5.7 5.5 6-.6.6-.6 1.2-.5 2V21"/></svg></a></li><li><a href=https://twitter.com/jasonxie666 target=_blank title=Twitter rel=me><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-brand-twitter" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M22 4.01c-1 .49-1.98.689-3 .99-1.121-1.265-2.783-1.335-4.38-.737S11.977 6.323 12 8v1c-3.245.083-6.135-1.395-8-4 0 0-4.182 7.433 4 11-1.872 1.247-3.739 2.088-6 2 3.308 1.803 6.913 2.423 10.034 1.517 3.58-1.04 6.522-3.723 7.651-7.742a13.84 13.84.0 00.497-3.753C20.18 7.773 21.692 5.25 22 4.009z"/></svg></a></li></ol><ol class=menu id=main-menu><li><a href=/blog/><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-home" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><polyline points="5 12 3 12 12 3 21 12 19 12"/><path d="M5 12v7a2 2 0 002 2h10a2 2 0 002-2v-7"/><path d="M9 21v-6a2 2 0 012-2h2a2 2 0 012 2v6"/></svg><span>主页</span></a></li><li><a href=/blog/%E5%85%B3%E4%BA%8E/><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-user" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="7" r="4"/><path d="M6 21v-2a4 4 0 014-4h4a4 4 0 014 4v2"/></svg><span>关于</span></a></li><li><a href=/blog/archives/><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-archive" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><rect x="3" y="4" width="18" height="4" rx="2"/><path d="M5 8v10a2 2 0 002 2h10a2 2 0 002-2V8"/><line x1="10" y1="12" x2="14" y2="12"/></svg><span>Archives</span></a></li><li><a href=/blog/search/><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-search" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="10" cy="10" r="7"/><line x1="21" y1="21" x2="15" y2="15"/></svg><span>Search</span></a></li><li><a href=/blog/links/><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-link" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M10 14a3.5 3.5.0 005 0l4-4a3.5 3.5.0 00-5-5l-.5.5"/><path d="M14 10a3.5 3.5.0 00-5 0l-4 4a3.5 3.5.0 005 5l.5-.5"/></svg><span>Links</span></a></li><div class=menu-bottom-section><li id=i18n-switch><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-language" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M4 5h7"/><path d="M9 3v2c0 4.418-2.239 8-5 8"/><path d="M5 9c-.003 2.144 2.952 3.908 6.7 4"/><path d="M12 20l4-9 4 9"/><path d="M19.1 18h-6.2"/></svg><select name=language onchange="window.location.href=this.selectedOptions[0].value"><option value=https://jasonrd.github.io/blog/en/>English</option><option value=https://jasonrd.github.io/blog/ selected>中文</option></select></li><li id=dark-mode-toggle><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-toggle-left" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="8" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-toggle-right" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="16" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg><span>暗色模式</span></li></div></ol></aside><aside class="sidebar right-sidebar sticky"><section class="widget archives"><div class=widget-icon><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-hash" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><line x1="5" y1="9" x2="19" y2="9"/><line x1="5" y1="15" x2="19" y2="15"/><line x1="11" y1="4" x2="7" y2="20"/><line x1="17" y1="4" x2="13" y2="20"/></svg></div><h2 class="widget-title section-title">目录</h2><div class=widget--toc><nav id=TableOfContents><ol><li><a href=#背景>背景</a></li><li><a href=#分析过程>分析过程</a></li><li><a href=#进一步深挖>进一步深挖</a><ol><li><a href=#netipv4vsconn_reuse_mode-开启和关闭影响>net.ipv4.vs.conn_reuse_mode 开启和关闭影响</a></li><li><a href=#单核性能更优>单核性能更优？</a></li></ol></li><li><a href=#总结>总结</a></li><li><a href=#参考>参考</a></li></ol></nav></div></section></aside><main class="main full-width"><article class="has-image main-article"><header class=article-header><div class=article-image><a href=/blog/p/ipvs-caused-problem/><img src=/blog/p/ipvs-caused-problem/matt-le-SJSpo9hQf7s-unsplash_hu958d513eeefe5556a31d065479ecc5ac_14205_800x0_resize_q75_box.jpg srcset="/blog/p/ipvs-caused-problem/matt-le-SJSpo9hQf7s-unsplash_hu958d513eeefe5556a31d065479ecc5ac_14205_800x0_resize_q75_box.jpg 800w, /blog/p/ipvs-caused-problem/matt-le-SJSpo9hQf7s-unsplash_hu958d513eeefe5556a31d065479ecc5ac_14205_1600x0_resize_q75_box.jpg 1600w" width=800 height=533 loading=lazy alt="Featured image of post k8s 环境中 ipvs 带来的问题"></a></div><div class=article-details><header class=article-category><a href=/blog/categories/kubernetes/ style=background-color:#2a9d8f;color:#fff>kubernetes</a>
<a href=/blog/categories/ipvs/>ipvs</a></header><div class=article-title-wrapper><h2 class=article-title><a href=/blog/p/ipvs-caused-problem/>k8s 环境中 ipvs 带来的问题</a></h2><h3 class=article-subtitle>上篇中分析 vip 转发实现，本文深度探究 ipvs 的不合理参数配置，会导致哪些问题</h3></div><footer class=article-time><div><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-calendar-time" width="56" height="56" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M11.795 21H5a2 2 0 01-2-2V7a2 2 0 012-2h12a2 2 0 012 2v4"/><circle cx="18" cy="18" r="4"/><path d="M15 3v4"/><path d="M7 3v4"/><path d="M3 11h16"/><path d="M18 16.496V18l1 1"/></svg><time class=article-time--published>Jan 05, 2022</time></div><div><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-clock" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><polyline points="12 7 12 12 15 15"/></svg><time class=article-time--reading>阅读时长: 6 分钟</time></div></footer></div></header><section class=article-content><h2 id=背景>背景</h2><p>近日，收到业务同学反馈，在进行在线推理业务时发现：部署到 Kubernetes 的服务，压测试性能出现数量级下降问题（只能达到 200 QPS），业务性能将难以满足客户实际需求。</p><p></p><p>我们对该问题进行了详细分析，通过对内核参数优化，最终容器环境性能可以达到 2000 QPS。</p><p>下面，我们将会分享整个问题分析和优化的过程，并且对其中涉及到的部分关键性问题进行剖析。另外，为何 kubernetes 优化后最高达到 2000 qps，而 docker run 环境能够达到 4000 qps？下文中也会给出答案。</p><h2 id=分析过程>分析过程</h2><p>根据业务同学反馈，压测端（10.58.14.13）使用 kubernetes service 和 nodeport 两种方式，都会出现 QPS 急剧降低的问题。</p><p>观察 QPS 降低为 200 时 ab 压测输出结果，可以看出 rt 大部分消耗在 connect 阶段（最大达到1s），也就是压测机（10.58.14.13）和 10.58.14.15 上的 Kubernetes 容器建立连接的过程：</p><p><img src=/blog/p/ipvs-caused-problem/img/ipvs-analyze-1.png width=1026 height=526 srcset="/blog/p/ipvs-caused-problem/img/ipvs-analyze-1_hua09781a0f876d3e05b8d14a9bbb027ac_301023_480x0_resize_box_3.png 480w, /blog/p/ipvs-caused-problem/img/ipvs-analyze-1_hua09781a0f876d3e05b8d14a9bbb027ac_301023_1024x0_resize_box_3.png 1024w" loading=lazy class=gallery-image data-flex-grow=195 data-flex-basis=468px></p><p>出现这种现象，可以从下面两个方面进行分析：</p><ol><li>14.13 -> 14.15 之间网络存在问题；</li><li>14.15 系统层面问题；</li></ol><p>为了排除网络方面的问题，我们在 14.15 主机上使用 Pod IP 进行压测，压测 QPS 为 2000 左右。然后，同样在 14.15 主机上使用 service ip 进行压测，我们发现在大约 30000 请求后，和之前业务同学描述一致：QPS 由 2000 降低到不到 120。压测时观察系统负载和业务容器 cpu 都非常低，这说明问题和 Kubernetes 网络架构有关。先看一下使用 Kubernetes service 请求和 Pod ip 两种方式有哪些不同：</p><p><img src=/blog/p/ipvs-caused-problem/img/ipvs-analyze-2.png width=880 height=316 srcset="/blog/p/ipvs-caused-problem/img/ipvs-analyze-2_hu75f0acaa5e34183215e50bea0bf5348b_87843_480x0_resize_box_3.png 480w, /blog/p/ipvs-caused-problem/img/ipvs-analyze-2_hu75f0acaa5e34183215e50bea0bf5348b_87843_1024x0_resize_box_3.png 1024w" loading=lazy class=gallery-image data-flex-grow=278 data-flex-basis=668px></p><p>Service ip 是 Kubernetes 在 IP 池中选取的一个 VIP，每个 VIP 会关联多个 POD 实例。为了能够通过 VIP 请求到具体的容器，Kubernetes 网络插件会在每个节点上做一些处理，目前常用的两种模式是 iptables 或 ipvs。我们本次出问题场景使用的是 ipvs 模式。在 ipvs 模式下，当客户端使用 VIP 请求时，会经过内核 ipvs 模块进行数据处理，才将流量转发到具体的容器实例。</p><p>通过对比发现，我们本次出现问题应该就是 ipvs 模块上。</p><p>为了方便排查问题，我们在 Kubernetes 中部署了一个简单的 http server Pod，然后在 Pod 所在主机上进行压测来进行问题分析。</p><p>在 linux 系统中有很多工具可以方便我们来查看 ipvs 管理的连接，在压测过程中使用 ipvsadm 观察看到 vip 关联的 rs 后端连接数的变化：</p><p><img src=/blog/p/ipvs-caused-problem/img/ipvs-analyze-3.png width=3058 height=1260 srcset="/blog/p/ipvs-caused-problem/img/ipvs-analyze-3_huc84b6d47cb01a67fddf9dc8103a4ec0c_1927321_480x0_resize_box_3.png 480w, /blog/p/ipvs-caused-problem/img/ipvs-analyze-3_huc84b6d47cb01a67fddf9dc8103a4ec0c_1927321_1024x0_resize_box_3.png 1024w" loading=lazy class=gallery-image data-flex-grow=242 data-flex-basis=582px></p><p>这里简单介绍执行 ipvsadm - L -t vip:port 返回的信息中几个字段的含义：</p><ol><li>Weight 流量转发给某个后端实例所占的权重，当该值为 0 时新连接就不会转发到对应的后端 ip 上；</li><li>ActionConn 是活动连接数，也就是tcp连接状态的 ESTABLISHED；</li><li>InActConn 是指除了ESTABLISHED以外的,所有的其它状态的tcp连接；</li></ol><p>我们在压测开始 server cpu 利用能够跑满，随着 InActConn 数量的增长 server 的 cpu 利用率也开始下滑，最后当 InActConn 维持到 32000 多时，http server 的 cpu 利用率只有 3%，InActConn 数数字几乎没有变化。显然，大部分请求没有到应用层。</p><p><img src=/blog/p/ipvs-caused-problem/img/ipvs-analyze-4.png width=2182 height=1778 srcset="/blog/p/ipvs-caused-problem/img/ipvs-analyze-4_hud9f2fb6970bb36d62dcb40deb5656e8d_1387653_480x0_resize_box_3.png 480w, /blog/p/ipvs-caused-problem/img/ipvs-analyze-4_hud9f2fb6970bb36d62dcb40deb5656e8d_1387653_1024x0_resize_box_3.png 1024w" loading=lazy class=gallery-image data-flex-grow=122 data-flex-basis=294px></p><p>之前在 Kubernetes 社区看到过一个关于 ipvs issue <a class=link href=https://github.com/kubernetes/kubernetes/issues/81775 target=_blank rel=noopener>#81775</a>，主要是讲一个单个客户端向某个 vip 发送请求时，容器销毁过程中会出现大量的请求错误。其中提到了一个关于 ipvs 内核参数 net.ipv4.vs.conn_reuse_mode，该参数用来开启对 ipvs connect 端口重用：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>conn_reuse_mode - INTEGER
</span></span><span class=line><span class=cl>    1 - default
</span></span><span class=line><span class=cl> 
</span></span><span class=line><span class=cl>    Controls how ipvs will deal with connections that are detected
</span></span><span class=line><span class=cl>    port reuse. It is a bitmap, with the values being:
</span></span><span class=line><span class=cl> 
</span></span><span class=line><span class=cl>    0: disable any special handling on port reuse. The new
</span></span><span class=line><span class=cl>    connection will be delivered to the same real server that was
</span></span><span class=line><span class=cl>    servicing the previous connection. This will effectively
</span></span><span class=line><span class=cl>    disable expire_nodest_conn.
</span></span><span class=line><span class=cl> 
</span></span><span class=line><span class=cl>    bit 1: enable rescheduling of new connections when it is safe.
</span></span><span class=line><span class=cl>    That is, whenever expire_nodest_conn and for TCP sockets, when
</span></span><span class=line><span class=cl>    the connection is in TIME_WAIT state (which is only possible if
</span></span><span class=line><span class=cl>    you use NAT mode).
</span></span><span class=line><span class=cl> 
</span></span><span class=line><span class=cl>    bit 2: it is bit 1 plus, for TCP connections, when connections
</span></span><span class=line><span class=cl>    are in FIN_WAIT state, as this is the last state seen by load
</span></span><span class=line><span class=cl>    balancer in Direct Routing mode. This bit helps on adding new
</span></span><span class=line><span class=cl>    real servers to a very busy cluster.
</span></span></code></pre></td></tr></table></div></div><ul><li>net.ipv4.vs.conn_reuse_mode=0时，ipvs不会对新连接进行重新负载，而是复用之前的负载结果，将新连接转发到原来的rs上；</li><li>net.ipv4.vs.conn_reuse_mode=1时，ipvs则会对新连接进行重新调度。</li></ul><p>查看压测的节点内核参数，发现 net.ipv4.vs.conn_reuse_mode 值为 1。然后，我们修改内核参数： net.ipv4.vs.conn_reuse_mode=0，再进行压测，QPS 稳定到了 2000 左右。说明问题就是和 ipvs 这个参数有关。</p><p>相关的，还有一个内核参数<code>net.ipv4.vs.expire_nodest_conn</code>，用于控制连接的rs不可用时的处理。在开启时，如果后端rs不可用，会立即结束掉该连接，使客户端重新发起新的连接请求；否则将数据包<strong>silently drop</strong>，也就是DROP掉数据包但不结束连接，等待客户端的重试。内核中关于<strong>destination 不可用</strong>的判断，是在ipvs执行删除<code>vs</code>（在<code>__ip_vs_del_service()</code>中实现）或删除<code>rs</code>（在<code>ip_vs_del_dest()</code>中实现）时，会调用<code>__ip_vs_unlink_dest()</code>方法，将相应的destination置为不可用。</p><h2 id=进一步深挖>进一步深挖</h2><p>虽然，修改内核后 QPS 由 120+ 提升到 2000 已经满足业务方的要求。但是还有一些疑惑没有解决：</p><ol><li>为什么内核参数设置为 net.ipv4.vs.conn_reuse_mode=1 时，导致 QPS 降低到了 200？</li><li>内核参数修改为 net.ipv4.vs.conn_reuse_mode=0 时，会导致哪些问题？</li><li>为什么 docker run 运行的容器 QPS 能达到 4000 ？</li></ol><p>带着这些疑惑，我们做进一步研究。</p><h3 id=netipv4vsconn_reuse_mode-开启和关闭影响>net.ipv4.vs.conn_reuse_mode 开启和关闭影响</h3><p>ipvs 会将请求 vs 的请求转发到 rs 需要使用 conntrack 表记录每一个连接的四元组信息。在我们压测过程中也可以看到每一条连接都会对应一条记录：</p><p><img src=/blog/p/ipvs-caused-problem/img/ipvs-analyze-5.png width=2878 height=320 srcset="/blog/p/ipvs-caused-problem/img/ipvs-analyze-5_hu9d9917c040d32514e300702a2d20da9f_155101_480x0_resize_box_3.png 480w, /blog/p/ipvs-caused-problem/img/ipvs-analyze-5_hu9d9917c040d32514e300702a2d20da9f_155101_1024x0_resize_box_3.png 1024w" loading=lazy alt=img class=gallery-image data-flex-grow=899 data-flex-basis=2158px></p><p>第一个问题，社区 2018 年在 <a class=link href=https://github.com/kubernetes/kubernetes/issues/70747 target=_blank rel=noopener>#70747</a> issue 中进行了讨论和修复。其中 comment 中有提到一个 <a class=link href="https://marc.info/?l=linux-virtual-server&m=151706660530133&w=2" target=_blank rel=noopener>linux kernel 的讨论</a>。在未开启端口复用时，如果匹配到新请求四元组已经存在于 conntrack 表中，会直接将包丢弃（NF_DROP）：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span><span class=lnt>25
</span><span class=lnt>26
</span><span class=lnt>27
</span><span class=lnt>28
</span><span class=lnt>29
</span><span class=lnt>30
</span><span class=lnt>31
</span><span class=lnt>32
</span><span class=lnt>33
</span><span class=lnt>34
</span><span class=lnt>35
</span><span class=lnt>36
</span><span class=lnt>37
</span><span class=lnt>38
</span><span class=lnt>39
</span><span class=lnt>40
</span><span class=lnt>41
</span><span class=lnt>42
</span><span class=lnt>43
</span><span class=lnt>44
</span><span class=lnt>45
</span><span class=lnt>46
</span><span class=lnt>47
</span><span class=lnt>48
</span><span class=lnt>49
</span><span class=lnt>50
</span><span class=lnt>51
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>// net/netfilter/ipvs/ip_vs_core.c
</span></span><span class=line><span class=cl>static unsigned int
</span></span><span class=line><span class=cl>ip_vs_in(struct netns_ipvs *ipvs, unsigned int hooknum, struct sk_buff *skb, int af)
</span></span><span class=line><span class=cl>{
</span></span><span class=line><span class=cl>... ....
</span></span><span class=line><span class=cl>/*
</span></span><span class=line><span class=cl>* Check if the packet belongs to an existing connection entry
</span></span><span class=line><span class=cl>*/
</span></span><span class=line><span class=cl>cp = pp-&gt;conn_in_get(ipvs, af, skb, &amp;iph);  //判断是否属于某个已有的connection
</span></span><span class=line><span class=cl>conn_reuse_mode = sysctl_conn_reuse_mode(ipvs);
</span></span><span class=line><span class=cl> 
</span></span><span class=line><span class=cl>//当conn_reuse_mode开启，同时出现端口复用（例如收到TCP的SYN包，并且也属于已有的 connection），进行处理
</span></span><span class=line><span class=cl>if (conn_reuse_mode &amp;&amp; !iph.fragoffs &amp;&amp; is_new_conn(skb, &amp;iph) &amp;&amp; cp) {
</span></span><span class=line><span class=cl>    bool uses_ct = false, resched = false;
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    //如果开启了expire_nodest_conn、目标rs的weight为0
</span></span><span class=line><span class=cl>    if (unlikely(sysctl_expire_nodest_conn(ipvs)) &amp;&amp; cp-&gt;dest &amp;&amp;
</span></span><span class=line><span class=cl>       unlikely(!atomic_read(&amp;cp-&gt;dest-&gt;weight))) {
</span></span><span class=line><span class=cl>        resched = true;
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        //查询是否用到了conntrack
</span></span><span class=line><span class=cl>        uses_ct = ip_vs_conn_uses_conntrack(cp, skb);
</span></span><span class=line><span class=cl>    } else if (is_new_conn_expected(cp, conn_reuse_mode)) {
</span></span><span class=line><span class=cl>        //连接是 expected 的情况，比如 FTP
</span></span><span class=line><span class=cl>        uses_ct = ip_vs_conn_uses_conntrack(cp, skb);
</span></span><span class=line><span class=cl>        if (!atomic_read(&amp;cp-&gt;n_control)) {
</span></span><span class=line><span class=cl>        		resched = true;
</span></span><span class=line><span class=cl>        } else {
</span></span><span class=line><span class=cl>            /* Do not reschedule controlling connection
</span></span><span class=line><span class=cl>            * that uses conntrack while it is still
</span></span><span class=line><span class=cl>            * referenced by controlled connection(s).
</span></span><span class=line><span class=cl>            */
</span></span><span class=line><span class=cl>            resched = !uses_ct;
</span></span><span class=line><span class=cl>        }
</span></span><span class=line><span class=cl>    }
</span></span><span class=line><span class=cl> 
</span></span><span class=line><span class=cl>    //如果expire_nodest_conn未开启，并且也非期望连接，实际上直接跳出了
</span></span><span class=line><span class=cl>    if (resched) {
</span></span><span class=line><span class=cl>        if (!atomic_read(&amp;cp-&gt;n_control))
</span></span><span class=line><span class=cl>            ip_vs_conn_expire_now(cp);
</span></span><span class=line><span class=cl>            __ip_vs_conn_put(cp);
</span></span><span class=line><span class=cl>            //当开启了net.ipv4.vs.conntrack，SYN数据包会直接丢弃，等待客户端重新发送SYN
</span></span><span class=line><span class=cl>            if (uses_ct)
</span></span><span class=line><span class=cl>            		return NF_DROP;
</span></span><span class=line><span class=cl>            //未开启conntrack时，会进入下面ip_vs_try_to_schedule的流程
</span></span><span class=line><span class=cl>            cp = NULL;
</span></span><span class=line><span class=cl>        }
</span></span><span class=line><span class=cl>    }
</span></span><span class=line><span class=cl> 
</span></span><span class=line><span class=cl>.... ...
</span></span><span class=line><span class=cl>}
</span></span></code></pre></td></tr></table></div></div><p>TCP 请求发送第一个 SYN 包被丢弃后，需要等待一个 MSL （60s），客户端会重新发送 SYN。在高并发情况，由于会大量新建连接，会导致出现较多的端口重用情况，就导致连接等待 >=1s 进行重新发送握手包。所以，在上面的压测图中可以看到，connect 阶段最大会达到几秒钟。kubernetes 在1.13 版本开始，对该问题进行了优化，kube-proxy 默认会修改内核参数 net.ipv4.vs.conn_reuse_mode=0 。</p><p><img src=/blog/p/ipvs-caused-problem/img/ipvs-analyze-6.png width=1422 height=278 srcset="/blog/p/ipvs-caused-problem/img/ipvs-analyze-6_hu3c12d5b6d02bbbce91d9b22ae95f064a_48097_480x0_resize_box_3.png 480w, /blog/p/ipvs-caused-problem/img/ipvs-analyze-6_hu3c12d5b6d02bbbce91d9b22ae95f064a_48097_1024x0_resize_box_3.png 1024w" loading=lazy alt=img class=gallery-image data-flex-grow=511 data-flex-basis=1227px></p><p>既然从 1.13 开始修改了默认参数，为什么我们的测试环境为 net.ipv4.vs.conn_reuse_mode=1 呢？先看一下，我们出现问题的环境 kubernetes 版本为 1.19.12，os 3.10.0。然后，我们对 1.19.12 k8s 代码进行 review 发现：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-go data-lang=go><span class=line><span class=cl><span class=nx>connReuseMinSupportedKernelVersion</span> <span class=p>=</span> <span class=s>&#34;4.1&#34;</span>
</span></span><span class=line><span class=cl><span class=o>...</span> <span class=o>...</span>
</span></span><span class=line><span class=cl><span class=k>if</span> <span class=nx>kernelVersion</span><span class=p>.</span><span class=nf>LessThan</span><span class=p>(</span><span class=nx>version</span><span class=p>.</span><span class=nf>MustParseGeneric</span><span class=p>(</span><span class=nx>connReuseMinSupportedKernelVersion</span><span class=p>))</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=nx>klog</span><span class=p>.</span><span class=nf>Errorf</span><span class=p>(</span><span class=s>&#34;can&#39;t set sysctl %s, kernel version must be at least %s&#34;</span><span class=p>,</span> <span class=nx>sysctlConnReuse</span><span class=p>,</span> <span class=nx>connReuseMinSupportedKernelVersion</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=p>}</span> <span class=k>else</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=c1>// Set the connection reuse mode
</span></span></span><span class=line><span class=cl><span class=c1></span>    <span class=k>if</span> <span class=nx>err</span> <span class=o>:=</span> <span class=nx>utilproxy</span><span class=p>.</span><span class=nf>EnsureSysctl</span><span class=p>(</span><span class=nx>sysctl</span><span class=p>,</span> <span class=nx>sysctlConnReuse</span><span class=p>,</span> <span class=mi>0</span><span class=p>);</span> <span class=nx>err</span> <span class=o>!=</span> <span class=kc>nil</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=kc>nil</span><span class=p>,</span> <span class=nx>err</span>
</span></span><span class=line><span class=cl>    <span class=p>}</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span><span class=line><span class=cl><span class=o>...</span> <span class=o>...</span>
</span></span></code></pre></td></tr></table></div></div><p>在 <a class=link href=https://github.com/kubernetes/kubernetes/pull/82066 target=_blank rel=noopener>pr#82066</a> 中提到，由于出现 3.10 版本内核中部署 kube-proxy 开启 ipvs 模式后无法启动。社区增加了内核版本的 check，1.19.0 开始如果内核版本 &lt;4.1 则不会修改 net.ipv4.vs.conn_reuse_mode 内核参数。</p><p>另外，上面 issue 中也提到了，主要影响是在大量短连接时会出现端口重用的情况。那如果我们将业务架构改成长连接是否就可以达到一样的效果呢？</p><p></p><p>我们后端部署一个简单的 http server 进行压测可以发现，使用长连接的服务即便关闭端口复用 QPS 明显好与优化内核的场景。</p><p>第二个问题，开启 net.ipv4.vs.conn_reuse_mode 参数后，端口重用导致的问题。我们在上文中提到过社区的一个 issue <a class=link href=https://github.com/kubernetes/kubernetes/issues/81775 target=_blank rel=noopener>#81775</a>。当开启端口重用，单个客户端使用 vip 发送大量请求，如果某个 pod 销毁会出现 no route to host 报错。</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>08:50:10 E http_client.go:558&gt; Unable to connect to 10.86.6.96:80 : dial tcp 10.86.6.96:80: connect: no route to host 08:50:11 E http_client.go:558&gt; Unable to connect to 10.86.6.96:80 : dial tcp 10.86.6.96:80: connect: no route to host 08:50:12 E http_client.go:558&gt; Unable to connect to 10.86.6.96:80 : dial tcp 10.86.6.96:80: connect: no route to host 08:50:13 E http_client.go:558&gt; Unable to connect to 10.86.6.96:80 : dial tcp 10.86.6.96:80: connect: no route to host
</span></span></code></pre></td></tr></table></div></div><p>issue 中进行了大量讨论，我在这里只简单分析一下出现该问题的原因。</p><p>在 Kubernetes 1.13 之前，kube-proxy ipvs 模式并不支持优雅删除，当 Endpoint 被删除时，kube-proxy 会直接移除掉 ipvs 中对应的 rs，这样会导致后续的数据包被丢掉。</p><p>在 1.13 版本后，Kubernetes 添加了<strong>IPVS 优雅删除</strong>的逻辑：</p><ul><li>当 Pod 被删除时，kube-proxy 会先将 rs 的<code>weight</code>置为 0，以防止新连接的请求发送到此 rs，由于不再直接删除 rs，旧连接仍能与 rs 正常通信；</li><li>当 rs 的<code>ActiveConn </code>数量为 0（现在已改为<code>ActiveConn+InactiveConn==0</code>)，即不再有连接转发到此 rs 时，此 rs 才会真正被移除。</li></ul><p>上面有提过 InactiveConn 是处于 TIME_WAIT 的连接，那每个处于 InactiveConn 的连接多久会过期呢，默认是120s，通过 ipvsadm -L &ndash;timeout 可以看到默认值：</p><p><img src=/blog/p/ipvs-caused-problem/img/ipvs-analyze-9.png width=2662 height=496 srcset="/blog/p/ipvs-caused-problem/img/ipvs-analyze-9_hu674ae24dcc1265d427fa4e80126db3d8_437372_480x0_resize_box_3.png 480w, /blog/p/ipvs-caused-problem/img/ipvs-analyze-9_hu674ae24dcc1265d427fa4e80126db3d8_437372_1024x0_resize_box_3.png 1024w" loading=lazy class=gallery-image data-flex-grow=536 data-flex-basis=1288px></p><p>正常情况 120s 就将连接在 conntrack 表中删除。但当开启端口重用后，权重修改为 0 的 rs 如果再次被复用，对于端口复用的连接，ipvs 不会主动进行新的调度（调用<code>ip_vs_try_to_schedule</code>方法）；同时，只是将<code>weight</code>置为 0，也并不会触发由<code>expire_nodest_conn </code>控制的结束连接或 DROP 操作，就这样，新连接的数据包当做什么都没发生一样，发送给了正在删除的 Pod。而这样的一个连接被 ipvs 认为是新的请求，会重置 ipvs timer，也就是说对应的这一个连接需要重新等待 120s 才会被删除。上面提到过，kube-proxy 在 ActiveConn+InactiveConn==0 时才会删除 rs，这样一来，只要不断的有端口复用的连接请求发来，rs 就不会被 kube-proxy 删除，上面提到的优雅删除是无法实现。</p><p>当后端应用进程退出后，后面端口复用的请求，会发送到已经被完全删除的容器 ip 上，就会出现上面的 connect: no route to host 报错。并且这个报错根据 ipvs 另一个参数配置有关：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span><span class=lnt>25
</span><span class=lnt>26
</span><span class=lnt>27
</span><span class=lnt>28
</span><span class=lnt>29
</span><span class=lnt>30
</span><span class=lnt>31
</span><span class=lnt>32
</span><span class=lnt>33
</span><span class=lnt>34
</span><span class=lnt>35
</span><span class=lnt>36
</span><span class=lnt>37
</span><span class=lnt>38
</span><span class=lnt>39
</span><span class=lnt>40
</span><span class=lnt>41
</span><span class=lnt>42
</span><span class=lnt>43
</span><span class=lnt>44
</span><span class=lnt>45
</span><span class=lnt>46
</span><span class=lnt>47
</span><span class=lnt>48
</span><span class=lnt>49
</span><span class=lnt>50
</span><span class=lnt>51
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-c data-lang=c><span class=line><span class=cl><span class=k>static</span> <span class=kt>unsigned</span> <span class=kt>int</span>
</span></span><span class=line><span class=cl><span class=nf>ip_vs_in</span><span class=p>(</span><span class=k>struct</span> <span class=n>netns_ipvs</span> <span class=o>*</span><span class=n>ipvs</span><span class=p>,</span> <span class=kt>unsigned</span> <span class=kt>int</span> <span class=n>hooknum</span><span class=p>,</span> <span class=k>struct</span> <span class=n>sk_buff</span> <span class=o>*</span><span class=n>skb</span><span class=p>,</span> <span class=kt>int</span> <span class=n>af</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=p>{</span>
</span></span><span class=line><span class=cl><span class=p>...</span> <span class=p>....</span>
</span></span><span class=line><span class=cl><span class=cm>/*
</span></span></span><span class=line><span class=cl><span class=cm> * Check if the packet belongs to an existing connection entry
</span></span></span><span class=line><span class=cl><span class=cm> */</span>
</span></span><span class=line><span class=cl><span class=n>cp</span> <span class=o>=</span> <span class=n>pp</span><span class=o>-&gt;</span><span class=nf>conn_in_get</span><span class=p>(</span><span class=n>ipvs</span><span class=p>,</span> <span class=n>af</span><span class=p>,</span> <span class=n>skb</span><span class=p>,</span> <span class=o>&amp;</span><span class=n>iph</span><span class=p>);</span>  <span class=c1>//判断是否属于某个已有的connection
</span></span></span><span class=line><span class=cl><span class=c1></span> 
</span></span><span class=line><span class=cl><span class=n>conn_reuse_mode</span> <span class=o>=</span> <span class=nf>sysctl_conn_reuse_mode</span><span class=p>(</span><span class=n>ipvs</span><span class=p>);</span>
</span></span><span class=line><span class=cl> 
</span></span><span class=line><span class=cl><span class=c1>//当conn_reuse_mode开启，同时出现端口复用（例如收到TCP的SYN包，并且也属于已有的 connection），进行处理
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=k>if</span> <span class=p>(</span><span class=n>conn_reuse_mode</span> <span class=o>&amp;&amp;</span> <span class=o>!</span><span class=n>iph</span><span class=p>.</span><span class=n>fragoffs</span> <span class=o>&amp;&amp;</span> <span class=nf>is_new_conn</span><span class=p>(</span><span class=n>skb</span><span class=p>,</span> <span class=o>&amp;</span><span class=n>iph</span><span class=p>)</span> <span class=o>&amp;&amp;</span> <span class=n>cp</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl> <span class=kt>bool</span> <span class=n>uses_ct</span> <span class=o>=</span> <span class=nb>false</span><span class=p>,</span> <span class=n>resched</span> <span class=o>=</span> <span class=nb>false</span><span class=p>;</span>
</span></span><span class=line><span class=cl> 
</span></span><span class=line><span class=cl> <span class=c1>//如果开启了expire_nodest_conn、目标rs的weight为0
</span></span></span><span class=line><span class=cl><span class=c1></span> <span class=k>if</span> <span class=p>(</span><span class=nf>unlikely</span><span class=p>(</span><span class=nf>sysctl_expire_nodest_conn</span><span class=p>(</span><span class=n>ipvs</span><span class=p>))</span> <span class=o>&amp;&amp;</span> <span class=n>cp</span><span class=o>-&gt;</span><span class=n>dest</span> <span class=o>&amp;&amp;</span>
</span></span><span class=line><span class=cl>     <span class=nf>unlikely</span><span class=p>(</span><span class=o>!</span><span class=nf>atomic_read</span><span class=p>(</span><span class=o>&amp;</span><span class=n>cp</span><span class=o>-&gt;</span><span class=n>dest</span><span class=o>-&gt;</span><span class=n>weight</span><span class=p>)))</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>  <span class=n>resched</span> <span class=o>=</span> <span class=nb>true</span><span class=p>;</span>
</span></span><span class=line><span class=cl>   
</span></span><span class=line><span class=cl>  <span class=c1>//查询是否用到了conntrack
</span></span></span><span class=line><span class=cl><span class=c1></span>  <span class=n>uses_ct</span> <span class=o>=</span> <span class=nf>ip_vs_conn_uses_conntrack</span><span class=p>(</span><span class=n>cp</span><span class=p>,</span> <span class=n>skb</span><span class=p>);</span>
</span></span><span class=line><span class=cl> <span class=p>}</span> <span class=k>else</span> <span class=k>if</span> <span class=p>(</span><span class=nf>is_new_conn_expected</span><span class=p>(</span><span class=n>cp</span><span class=p>,</span> <span class=n>conn_reuse_mode</span><span class=p>))</span> <span class=p>{</span>
</span></span><span class=line><span class=cl> <span class=c1>//连接是expected的情况，比如FTP
</span></span></span><span class=line><span class=cl><span class=c1></span>  <span class=n>uses_ct</span> <span class=o>=</span> <span class=nf>ip_vs_conn_uses_conntrack</span><span class=p>(</span><span class=n>cp</span><span class=p>,</span> <span class=n>skb</span><span class=p>);</span>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=p>(</span><span class=o>!</span><span class=nf>atomic_read</span><span class=p>(</span><span class=o>&amp;</span><span class=n>cp</span><span class=o>-&gt;</span><span class=n>n_control</span><span class=p>))</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>   <span class=n>resched</span> <span class=o>=</span> <span class=nb>true</span><span class=p>;</span>
</span></span><span class=line><span class=cl>  <span class=p>}</span> <span class=k>else</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>   <span class=cm>/* Do not reschedule controlling connection
</span></span></span><span class=line><span class=cl><span class=cm>    * that uses conntrack while it is still
</span></span></span><span class=line><span class=cl><span class=cm>    * referenced by controlled connection(s).
</span></span></span><span class=line><span class=cl><span class=cm>    */</span>
</span></span><span class=line><span class=cl>   <span class=n>resched</span> <span class=o>=</span> <span class=o>!</span><span class=n>uses_ct</span><span class=p>;</span>
</span></span><span class=line><span class=cl>  <span class=p>}</span>
</span></span><span class=line><span class=cl> <span class=p>}</span>
</span></span><span class=line><span class=cl> 
</span></span><span class=line><span class=cl> <span class=c1>//如果expire_nodest_conn未开启，并且也非期望连接，实际上直接跳出了
</span></span></span><span class=line><span class=cl><span class=c1></span> <span class=k>if</span> <span class=p>(</span><span class=n>resched</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>  <span class=k>if</span> <span class=p>(</span><span class=o>!</span><span class=nf>atomic_read</span><span class=p>(</span><span class=o>&amp;</span><span class=n>cp</span><span class=o>-&gt;</span><span class=n>n_control</span><span class=p>))</span>
</span></span><span class=line><span class=cl>   <span class=nf>ip_vs_conn_expire_now</span><span class=p>(</span><span class=n>cp</span><span class=p>);</span>
</span></span><span class=line><span class=cl>  <span class=nf>__ip_vs_conn_put</span><span class=p>(</span><span class=n>cp</span><span class=p>);</span>
</span></span><span class=line><span class=cl>  <span class=c1>//当开启了net.ipv4.vs.conntrack，SYN数据包会直接丢弃，等待客户端重新发送SYN
</span></span></span><span class=line><span class=cl><span class=c1></span>  <span class=k>if</span> <span class=p>(</span><span class=n>uses_ct</span><span class=p>)</span>
</span></span><span class=line><span class=cl>   <span class=k>return</span> <span class=n>NF_DROP</span><span class=p>;</span>
</span></span><span class=line><span class=cl>  <span class=c1>//未开启conntrack时，会进入下面ip_vs_try_to_schedule的流程
</span></span></span><span class=line><span class=cl><span class=c1></span>  <span class=n>cp</span> <span class=o>=</span> <span class=nb>NULL</span><span class=p>;</span>
</span></span><span class=line><span class=cl> <span class=p>}</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span><span class=line><span class=cl> 
</span></span><span class=line><span class=cl><span class=p>....</span> <span class=p>...</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></td></tr></table></div></div><ul><li><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>expire_nodest_conn=0 时，当后端 rs 不可达时，ipvs 会直接将数据包丢弃；
</span></span></code></pre></td></tr></table></div></div></li><li><p>expire_nodest_conn=1 时，当后端 rs 不可达，立即会返回一个报错给客户端。</p></li></ul><p>针对这个问题，只在 kubernetes 上并不能完美的解决，例如：kube-router 中增加了优雅下线，会等待 Pod 配置的 TerminationGracePeriodSeconds 后进行删除 rs，这样只能够在一定程度上避免该问题。</p><h3 id=单核性能更优>单核性能更优？</h3><p>第三个问题，为什么使用 docker run 在线推理业务可以达到 4000 QPS，而 Kubernetes 容器通过内核优化后只能达到 2000？</p><p>针对这个问题，首先和业务方同学进行确认启动 docker 的参数，经过确认发现业务方同学误将 &ndash;cpuset-cpus 当作限制 cpu 使用了，其启动参数命令为：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>docker run --cpuset-cpus 4 xxxx
</span></span></code></pre></td></tr></table></div></div><p>这个启动命令，最终创建的容器其实只使用了一个核，并且将应用进程绑定到了第 4 个 cpu 上。然后，通过修改命令改成非绑核，使用下面命令启动：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>docker run --cpu-quota 400000 xxxx
</span></span></code></pre></td></tr></table></div></div><p>再进行压测 QPS 降低到了 2500，已经和 kubernetes 创建的容器非常接近。由于使用 vip 会经过 ipvs 进行数据包的处理，会有一定的性能损耗，这个结果也比较合理。</p><p>那为什么绑核情况分配1个 cpu 的应用性能会比没有绑核 cpu 会好一倍呢？</p><p>猜测和业务服务逻辑有关，后面还要再进行验证。</p><p>沟通下来确认这个服务业务功能是：首先做少量的数学运算，然后再与通过 grpc 调用后端服务拿到的结果进行计算，将最终的结果返回给客户端。这样的话，这个业务应该算是一个 io 密集型应用，并不需要较高的 cpu 的， 绑核后能够减少 cpu 之间频繁的上下文切换，从而带来更好的效果。</p><h2 id=总结>总结</h2><p>通过修改 ipvs 内核参数，协助联邦同学解决了遇到的吞吐率问题，将 QPS 从 200 提升到了 2000+。</p><p>然后，我们并不是止步于解决问题，对问题过程中遇到的疑惑进一步研究，帮助我们能够对 kubernetes 系统有全面的把控。</p><ul><li>在 1.19.0 版本开始 kubernetes 对 ipvs 默认内核参数进行了改进，当内核版本 &lt;4.1 时，kube-proxy 不会修改 ipvs 内核参数 net.ipv4.vs.conn_reuse_mode。</li><li>通过修改内核参数提高了吞吐率，但同时带来了优雅下线的问题，在 5.9 开始 linux 内核层面已有解决。另外，我们针对可以对业务架构优化的场景使用长连接方式进行压测，能够显著的解决吞吐率降低的问题。</li><li>对于 docker run 场景 QPS 是 Kubernetes 容器的两倍问题，我们发现业务同学使用 docker 运行时使用了 &ndash;cpuset-cpus 参数，也就是将应用绑定到某一个 cpu 核上。这说明对于某些应用并不是分配的应用 cpu 越多，性能越好。</li></ul><p>目前，发现问题的主要场景是在使用官方默认网络组件 kube-proxy 带来的问题。而我们 IDC 内部使用 kube-router 使用了和 Pod TerminationGracePeriodSeconds 一致的等待时间来优雅删除 rs。</p><p>另外，针对 ipvs 性能社区也有一些使用 eBPF 来实现的解决方案，例如：Cillium、腾讯 ipvs-ebpf 等。</p><h2 id=参考>参考</h2><ol><li><a class=link href="https://marc.info/?l=linux-virtual-server&m=151683112005533&w=2" target=_blank rel=noopener>https://marc.info/?l=linux-virtual-server&m=151683112005533&w=2</a></li><li><a class=link href=https://github.com/kubernetes/kubernetes/issues/70747 target=_blank rel=noopener>IPVS low throughput</a></li><li><a class=link href=http://patchwork.ozlabs.org/project/netfilter-devel/patch/20200701151719.4751-1-ja@ssi.bg/ target=_blank rel=noopener>解决关闭端口复用出现 1s 延迟的 patch</a></li><li><a class=link href=http://patchwork.ozlabs.org/project/netfilter-devel/patch/20200708161638.13584-1-kim.andrewsy@gmail.com/ target=_blank rel=noopener>开启端口复用后 rs 下线导致后端不可用问题</a></li><li><a class=link href=https://cloud.tencent.com/developer/article/1687922 target=_blank rel=noopener>绕过conntrack，使用eBPF增强 IPVS优化K8s网络性能</a></li></ol></section><footer class=article-footer><section class=article-copyright><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-copyright" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><path d="M14.5 9a3.5 4 0 100 6"/></svg><span>Licensed under CC BY-NC-SA 4.0</span></section></footer></article><aside class=related-content--wrapper><h2 class=section-title>相关文章</h2><div class=related-content><div class="flex article-list--tile"><article class=has-image><a href=/blog/p/how-endpoint-flush-ipvs/><div class=article-image><img src=/blog/p/how-endpoint-flush-ipvs/_huf941de4769045cdfa8c9ee7036519a2a_35369_36687b530fa140121781e790cfd060e7.jpg width=250 height=150 loading=lazy alt="Featured image of post endpoint 更新后 vip 转发实现探究" data-key=how-endpoint-flush-ipvs data-hash="md5-YBFrHM/IYy6aZffVHfPvwg=="></div><div class=article-details><h2 class=article-title>endpoint 更新后 vip 转发实现探究</h2></div></a></article><article class=has-image><a href=/blog/p/kubectl-exec-deepin/><div class=article-image><img src=/blog/p/kubectl-exec-deepin/_hu45a5e3ad5e058da6a00650ed8fd40bea_15530_27040ddd7399aaf289c0ce70ee6592d9.jpg width=250 height=150 loading=lazy alt="Featured image of post 从 kubectl exec 异常问题开始" data-key=kubectl-exec-deepin data-hash="md5-RvYejqai34/Fvx0koXxEpg=="></div><div class=article-details><h2 class=article-title>从 kubectl exec 异常问题开始</h2></div></a></article><article class=has-image><a href=/blog/p/service-cause-failure/><div class=article-image><img src=/blog/p/service-cause-failure/_hu0a3f1163de68d0b9471979ebf0ecf11e_32400_a354a9a1f5102742bceb4ec236f4ce2f.jpg width=250 height=150 loading=lazy alt="Featured image of post pod 销毁过程引起短时服务不可用" data-key=service-cause-failure data-hash="md5-rC+UhC7sa7h6Y66uIugvQQ=="></div><div class=article-details><h2 class=article-title>pod 销毁过程引起短时服务不可用</h2></div></a></article><article class=has-image><a href=/blog/p/pod-pending-with-enough-resources/><div class=article-image><img src=/blog/p/pod-pending-with-enough-resources/sleep-boy.ba540b47bff96318db80025530254292_hud07293a026e0ecb8feb93a2c733978af_122481_250x150_fill_q75_box_smart1.jpg width=250 height=150 loading=lazy alt="Featured image of post 集群资源充足情况 Pod 出现 schedulfailed" data-key=pod-pending-with-enough-resources data-hash="md5-ulQLR7/5YxjbgAJVMCVCkg=="></div><div class=article-details><h2 class=article-title>集群资源充足情况 Pod 出现 schedulfailed</h2></div></a></article><article><a href=/blog/p/gpu-start-failure/><div class=article-details><h2 class=article-title>kubelet 开启 static 引发 gpu 容器部署异常</h2></div></a></article></div></div></aside><div class=disqus-container><div id=disqus_thread></div><script type=application/javascript>window.disqus_config=function(){},function(){if(["localhost","127.0.0.1"].indexOf(window.location.hostname)!=-1){document.getElementById("disqus_thread").innerHTML="Disqus comments not available by default when the website is previewed locally.";return}var t=document,e=t.createElement("script");e.async=!0,e.src="//hugo-theme-stack.disqus.com/embed.js",e.setAttribute("data-timestamp",+new Date),(t.head||t.body).appendChild(e)}()</script><noscript>Please enable JavaScript to view the <a href=https://disqus.com/?ref_noscript>comments powered by Disqus.</a></noscript><a href=https://disqus.com class=dsq-brlink>comments powered by <span class=logo-disqus>Disqus</span></a></div><style>.disqus-container{background-color:var(--card-background);border-radius:var(--card-border-radius);box-shadow:var(--shadow-l1);padding:var(--card-padding)}</style><script>window.addEventListener("onColorSchemeChange",e=>{typeof DISQUS=="object"&&DISQUS.reset({reload:!0})})</script><footer class=site-footer><section class=copyright>&copy;
2020 -
2023 jason's 博客</section><section class=powerby>Built with <a href=https://gohugo.io/ target=_blank rel=noopener>Hugo</a><br>主题 <b><a href=https://github.com/CaiJimmy/hugo-theme-stack target=_blank rel=noopener data-version=3.16.0>Stack</a></b> 由 <a href=https://jimmycai.com target=_blank rel=noopener>Jimmy</a> 设计</section></footer><div class=pswp tabindex=-1 role=dialog aria-hidden=true><div class=pswp__bg></div><div class=pswp__scroll-wrap><div class=pswp__container><div class=pswp__item></div><div class=pswp__item></div><div class=pswp__item></div></div><div class="pswp__ui pswp__ui--hidden"><div class=pswp__top-bar><div class=pswp__counter></div><button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
<button class="pswp__button pswp__button--share" title=Share></button>
<button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
<button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button><div class=pswp__preloader><div class=pswp__preloader__icn><div class=pswp__preloader__cut><div class=pswp__preloader__donut></div></div></div></div></div><div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap"><div class=pswp__share-tooltip></div></div><button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)"></button>
<button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)"></button><div class=pswp__caption><div class=pswp__caption__center></div></div></div></div></div><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js integrity="sha256-ePwmChbbvXbsO02lbM3HoHbSHTHFAeChekF1xKJdleo=" crossorigin=anonymous defer></script><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js integrity="sha256-UKkzOn/w1mBxRmLLGrSeyB4e1xbrp4xylgAWb3M42pU=" crossorigin=anonymous defer></script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.min.css crossorigin=anonymous><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.css crossorigin=anonymous></main></div><script src=https://cdn.jsdelivr.net/npm/node-vibrant@3.1.6/dist/vibrant.min.js integrity="sha256-awcR2jno4kI5X0zL8ex0vi2z+KMkF24hUW8WePSA9HM=" crossorigin=anonymous></script><script type=text/javascript src=/blog/ts/main.js defer></script>
<script>(function(){const e=document.createElement("link");e.href="https://fonts.googleapis.com/css2?family=Lato:wght@300;400;700&display=swap",e.type="text/css",e.rel="stylesheet",document.head.appendChild(e)})()</script></body></html>