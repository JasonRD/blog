[{"content":"问题现象 最近，在工作想要使用 kubectl exec 进入容器排查问题，结果返回下面异常：\n排查过程 我们知道 kubectl exec 的执行链路是 client -\u0026gt; kube-apiserver -\u0026gt; kubelet -\u0026gt; docker。\n登录 Kubelet 宿主机查看 kubelet 错误日志，发现有相同的报错日志，这说明是 kubelet 和 docker 之间链路又问题。通过 kubelet 日志中不能定为到问题具体原因。然后，我们试图通过抓包，希望在数据包中能发现一些线索。\n在抓包数据结果中我们发现关键字为 exec 的请求，该会话的目的地址为 A:20880, http header 中 Host 为 B:10250 （也就是是物理机上 kubelet 的 httpserver 地址）。我们查询 A 这个IP，发现是业务应用的容器 IP。\n这就比较奇怪了，正常 apiserver 发送 exec 请求为什么转发到了容器的 20880 端口。并且数据包中包含 kubectl (\u0026ldquo;User-Agent: kubectl\u0026rdquo;) http header。难道 kubectl exec 请求发送到 docker 的请求（xxxx/exec/token）被转发到了容器。通过再次尝试执行 kubectl exec 并抓包，发现执行命令和发送到 20880 端口请求匹配，这验证了我们的猜测。\n到此就把问题范围缩小到宿主机网络上，我们知道 kube-proxy 会通过 ipvs 或 iptables 对创建的 nodeport 或 service vip 的请求进行拦截和转发。我们查看 conntrack 请求记录：\n发现一条请求 127.0.0.1:33589 的记录，并且转发到的地址 A:20880 也和我们抓包的结果匹配。然后查看 33589 端口，发现该端口就是被 kubelet 占用。然后，我们查询 service，发现 33589 端口同时是 B 容器的应用 service 的 nodeport。到此问题根本原因定位到了，nodeport 端口和 kubelet 启动的转发端口冲突了，导致发送 exec 请求转发到了应用容器的 20880 端口（dubbo端口）。\n继续深挖 事情到此并没有结束，上面我们只是定位到了具体问题原因。其实还存在两个问题：\n对 kubectl exec 的执行过程还没没挖透； 如何避免该问题？ kubectl exec 的执行过程 问题没有快速定位，主要原因还是对 kubectl exec 执行流程不熟。下面来了解一下 kubectl 是怎么执行的。\n本文基于 1.14.6 源码进行研究。\n首先，简单了解一下 kubelet 架构：\nkubelet 中有上面几个部分：container manager、dockershim、http server、streaming server。kubelet 早期直接调用 docker api 管理容器，后来为了适配更多的 runtime 抽象出了一个接入层 cri。同时，为了兼容 docker 的 API，kubelet 代码中实现了这个叫 dockershim 的部分。这样就对上层屏蔽了底层 runtime。http server 通常使用 10250 对外提供 API 服务。streaming server 是需要和容器进行交互时的一个代理服务。\n在默认情况下，用户执行 kubectl exec 简化流程如下：\nterminal 中键入 kubectl exec xxx 指令，kubectl 发送请求到 apiserver https://apiserver/api/v1/namespaces/{ns}/pods/{pod}/exec?command=bash\u0026amp;container=dragon-claw\u0026amp;stdin=true\u0026amp;stdout=true\u0026amp;tty=true； apiserver 接到请求后，将请求转发到 kubelet， node:10250/api/v1/exec/{ns}/{podid}/{container}。kubelet httpserver 接收到请求后： 首先，向 dockershim 发起 getExec 请求，返回一个流地址 url （exec/{token}）； 然后，kubelet 请求 exec/xxxx url 到 streaming server，streaming server 接收到请求后，response upgrade 将连接升级成为 spdy 或 ws 连接； kubelet 收到 upgrade reponse 后，将该 reponse 直接返回给 apiserver，到此 apiserver -\u0026gt; kubelet -\u0026gt; streaming server -\u0026gt; docker 之间整个通道建立完成； 到此，用户可以在 terminal 中键入命令在容器中执行； 其中， streaming server 是 kubelet 和 docker 之间的一个桥梁，他负责将请求转发给 docker（或者其他 runtime）。kubelet 访问 streaming server 的地址就是 127.0.0.1:{streaming sever port}。\n而我们遇到问题中端口冲突，就是 streaming server 端口和 nodeport 冲突。kubelet 拿到 exec url 后，命中本地 iptables 规则，然后请求被转发到了 nodeport 关联的容器，返回上述错误。\n在源码研究过程中，参数 \u0026ndash;redirect-container-streaming 引起了我们的注意：\n1 2 3 4 5 6 7 8 9 10 11 12 13 func (s *Server) getExec(request *restful.Request, response *restful.Response) { .... 省略若干代码.... url, err := s.host.GetExec(podFullName, params.podUID, params.containerName, params.cmd, *streamOpts) if err != nil { streaming.WriteError(err, response.ResponseWriter) return } if s.redirectContainerStreaming { http.Redirect(response.ResponseWriter, request.Request, url.String(), http.StatusFound) return } proxyStream(response.ResponseWriter, request.Request, url) } 该参数开启后会并不会进行 proxyStream。而是直接向 apiserver 发送 302 跳转，流程变为如下：\n该参数说明，如下：\n1 --redirect-container-streaming Enables container streaming redirect. If false, kubelet will proxy container streaming data between apiserver and container runtime; if true, kubelet will return an http redirect to apiserver, and apiserver will access container runtime directly. The proxy approach is more secure, but introduces some overhead. The redirect approach is more performant, but less secure because the connection between apiserver and container runtime may not be authenticated. 另外，在 1.18 版本中我们发现该参数即将废弃，社区中已经在 kep Cleaning up container streaming requests中详细说明了后续下线计划（1.18 进行下线提示、1.20版本参数失效、1.22 参数被删除）。后续 apiserver 无法直接和 dockershim 通信。\n如何避免端口冲突 经过源码阅读，我们了解了执行 kube exec 的流程，通过关闭 streaming server 可以避免 streaming server 端口和 nodeport 冲突。但是该方案只能在 1.20 版本前的集群中使用。\n另外，进一步思考，如果其他进程使用了一个随机端口是否也会出现该问题呢？\n还是有一定冲突概率的，在 #85418 issue 中就有人提出了该问题，从相关讨论中推荐解决方法是通过宿主机预留端口（net.ipv4.ip_local_port_range）解决。k8s apiserver 默认的 nodeport 端口范围为 30000-32767 （通过 \u0026ndash;service-node-port-range 参数配置），一般宿主机 net.ipv4.ip_local_port_range 默认范围为 32768-60999。而我们出现冲突，因为使用的某云 k8s 集群修改了 apiserver 参数为 30000-50000，导致出现端口冲突问题。\n其实，kube-proxy 为了避免端口冲突的问题，运行过程会监听所有的 nodeport 端口。但是，这存在一个鸡生蛋的问题。如果某个 nodeport 分配前已经被其他应用占用，或者 kube-proxy 重启，还是会存在端口冲突的问题。在 #100643 issue 中也进行了相关讨论，希望后续能有完美的解决方案。\n综上，目前解决方案下面几种：\n1.20 前版本可以通过 \u0026ndash;redirect-container-streaming 关闭 steaming server，避免 kubelet 和 nodeport 端口冲突； 修改系统参数和 apiserver 端口范围，保证和宿主机随机端口范围不重合； 其他技术，例如 #100643 issue 中提出的 ebpf。 总结 通过一个生产 kubectl exec异常问题，我们了解了执行 exec 命令后，整个底层转发逻辑：\napiserver 查询到 pod 所在 node ip，通过 nodeip:10250 端口向 kubelet 发起请求； kubelet 接收到请求后，向本地 runtime 获取 exec url。然后，1.20 之前会基于参数 \u0026ndash;redirect-container-streaming 有两种处理流程： 开启参数，通过 302 跳转方式，将 apiserver 请求重定向到 exec url； 关闭参数，会先直接和 runtime 建立 exec 通道，然后将 apiserver 请求升级为 spdy 或 ws 连接； 后续 apisever 和 runtime 通道建立完成，client 就可以在 terminal 上执行命令了。 ","date":"2022-03-01T00:00:00Z","image":"https://jasonrd.github.io/blog/p/kubectl-exec-deepin/helena-hertz-wWZzXlDpMog-unsplash_hu45a5e3ad5e058da6a00650ed8fd40bea_15530_120x120_fill_q75_box_smart1.jpg","permalink":"https://jasonrd.github.io/blog/p/kubectl-exec-deepin/","title":"从 kubectl exec 异常问题开始"},{"content":"背景 近日，收到业务同学反馈，在进行在线推理业务时发现：部署到 Kubernetes 的服务，压测试性能出现数量级下降问题（只能达到 200 QPS），业务性能将难以满足客户实际需求。\n我们对该问题进行了详细分析，通过对内核参数优化，最终容器环境性能可以达到 2000 QPS。\n下面，我们将会分享整个问题分析和优化的过程，并且对其中涉及到的部分关键性问题进行剖析。另外，为何 kubernetes 优化后最高达到 2000 qps，而 docker run 环境能够达到 4000 qps？下文中也会给出答案。\n分析过程 根据业务同学反馈，压测端（10.58.14.13）使用 kubernetes service 和 nodeport 两种方式，都会出现 QPS 急剧降低的问题。\n观察 QPS 降低为 200 时 ab 压测输出结果，可以看出 rt 大部分消耗在 connect 阶段（最大达到1s），也就是压测机（10.58.14.13）和 10.58.14.15 上的 Kubernetes 容器建立连接的过程：\n出现这种现象，可以从下面两个方面进行分析：\n14.13 -\u0026gt; 14.15 之间网络存在问题； 14.15 系统层面问题； 为了排除网络方面的问题，我们在 14.15 主机上使用 Pod IP 进行压测，压测 QPS 为 2000 左右。然后，同样在 14.15 主机上使用 service ip 进行压测，我们发现在大约 30000 请求后，和之前业务同学描述一致：QPS 由 2000 降低到不到 120。压测时观察系统负载和业务容器 cpu 都非常低，这说明问题和 Kubernetes 网络架构有关。先看一下使用 Kubernetes service 请求和 Pod ip 两种方式有哪些不同：\nService ip 是 Kubernetes 在 IP 池中选取的一个 VIP，每个 VIP 会关联多个 POD 实例。为了能够通过 VIP 请求到具体的容器，Kubernetes 网络插件会在每个节点上做一些处理，目前常用的两种模式是 iptables 或 ipvs。我们本次出问题场景使用的是 ipvs 模式。在 ipvs 模式下，当客户端使用 VIP 请求时，会经过内核 ipvs 模块进行数据处理，才将流量转发到具体的容器实例。\n通过对比发现，我们本次出现问题应该就是 ipvs 模块上。\n为了方便排查问题，我们在 Kubernetes 中部署了一个简单的 http server Pod，然后在 Pod 所在主机上进行压测来进行问题分析。\n在 linux 系统中有很多工具可以方便我们来查看 ipvs 管理的连接，在压测过程中使用 ipvsadm 观察看到 vip 关联的 rs 后端连接数的变化：\n这里简单介绍执行 ipvsadm - L -t vip:port 返回的信息中几个字段的含义：\nWeight 流量转发给某个后端实例所占的权重，当该值为 0 时新连接就不会转发到对应的后端 ip 上； ActionConn 是活动连接数，也就是tcp连接状态的 ESTABLISHED； InActConn 是指除了ESTABLISHED以外的,所有的其它状态的tcp连接； 我们在压测开始 server cpu 利用能够跑满，随着 InActConn 数量的增长 server 的 cpu 利用率也开始下滑，最后当 InActConn 维持到 32000 多时，http server 的 cpu 利用率只有 3%，InActConn 数数字几乎没有变化。显然，大部分请求没有到应用层。\n之前在 Kubernetes 社区看到过一个关于 ipvs issue #81775，主要是讲一个单个客户端向某个 vip 发送请求时，容器销毁过程中会出现大量的请求错误。其中提到了一个关于 ipvs 内核参数 net.ipv4.vs.conn_reuse_mode，该参数用来开启对 ipvs connect 端口重用：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 conn_reuse_mode - INTEGER 1 - default Controls how ipvs will deal with connections that are detected port reuse. It is a bitmap, with the values being: 0: disable any special handling on port reuse. The new connection will be delivered to the same real server that was servicing the previous connection. This will effectively disable expire_nodest_conn. bit 1: enable rescheduling of new connections when it is safe. That is, whenever expire_nodest_conn and for TCP sockets, when the connection is in TIME_WAIT state (which is only possible if you use NAT mode). bit 2: it is bit 1 plus, for TCP connections, when connections are in FIN_WAIT state, as this is the last state seen by load balancer in Direct Routing mode. This bit helps on adding new real servers to a very busy cluster. net.ipv4.vs.conn_reuse_mode=0时，ipvs不会对新连接进行重新负载，而是复用之前的负载结果，将新连接转发到原来的rs上； net.ipv4.vs.conn_reuse_mode=1时，ipvs则会对新连接进行重新调度。 查看压测的节点内核参数，发现 net.ipv4.vs.conn_reuse_mode 值为 1。然后，我们修改内核参数： net.ipv4.vs.conn_reuse_mode=0，再进行压测，QPS 稳定到了 2000 左右。说明问题就是和 ipvs 这个参数有关。\n相关的，还有一个内核参数net.ipv4.vs.expire_nodest_conn，用于控制连接的rs不可用时的处理。在开启时，如果后端rs不可用，会立即结束掉该连接，使客户端重新发起新的连接请求；否则将数据包silently drop，也就是DROP掉数据包但不结束连接，等待客户端的重试。内核中关于destination 不可用的判断，是在ipvs执行删除vs（在__ip_vs_del_service()中实现）或删除rs（在ip_vs_del_dest()中实现）时，会调用__ip_vs_unlink_dest()方法，将相应的destination置为不可用。\n进一步深挖 虽然，修改内核后 QPS 由 120+ 提升到 2000 已经满足业务方的要求。但是还有一些疑惑没有解决：\n为什么内核参数设置为 net.ipv4.vs.conn_reuse_mode=1 时，导致 QPS 降低到了 200？ 内核参数修改为 net.ipv4.vs.conn_reuse_mode=0 时，会导致哪些问题？ 为什么 docker run 运行的容器 QPS 能达到 4000 ？ 带着这些疑惑，我们做进一步研究。\nnet.ipv4.vs.conn_reuse_mode 开启和关闭影响 ipvs 会将请求 vs 的请求转发到 rs 需要使用 conntrack 表记录每一个连接的四元组信息。在我们压测过程中也可以看到每一条连接都会对应一条记录：\n第一个问题，社区 2018 年在 #70747 issue 中进行了讨论和修复。其中 comment 中有提到一个 linux kernel 的讨论。在未开启端口复用时，如果匹配到新请求四元组已经存在于 conntrack 表中，会直接将包丢弃（NF_DROP）：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 // net/netfilter/ipvs/ip_vs_core.c static unsigned int ip_vs_in(struct netns_ipvs *ipvs, unsigned int hooknum, struct sk_buff *skb, int af) { ... .... /* * Check if the packet belongs to an existing connection entry */ cp = pp-\u0026gt;conn_in_get(ipvs, af, skb, \u0026amp;iph); //判断是否属于某个已有的connection conn_reuse_mode = sysctl_conn_reuse_mode(ipvs); //当conn_reuse_mode开启，同时出现端口复用（例如收到TCP的SYN包，并且也属于已有的 connection），进行处理 if (conn_reuse_mode \u0026amp;\u0026amp; !iph.fragoffs \u0026amp;\u0026amp; is_new_conn(skb, \u0026amp;iph) \u0026amp;\u0026amp; cp) { bool uses_ct = false, resched = false; //如果开启了expire_nodest_conn、目标rs的weight为0 if (unlikely(sysctl_expire_nodest_conn(ipvs)) \u0026amp;\u0026amp; cp-\u0026gt;dest \u0026amp;\u0026amp; unlikely(!atomic_read(\u0026amp;cp-\u0026gt;dest-\u0026gt;weight))) { resched = true; //查询是否用到了conntrack uses_ct = ip_vs_conn_uses_conntrack(cp, skb); } else if (is_new_conn_expected(cp, conn_reuse_mode)) { //连接是 expected 的情况，比如 FTP uses_ct = ip_vs_conn_uses_conntrack(cp, skb); if (!atomic_read(\u0026amp;cp-\u0026gt;n_control)) { resched = true; } else { /* Do not reschedule controlling connection * that uses conntrack while it is still * referenced by controlled connection(s). */ resched = !uses_ct; } } //如果expire_nodest_conn未开启，并且也非期望连接，实际上直接跳出了 if (resched) { if (!atomic_read(\u0026amp;cp-\u0026gt;n_control)) ip_vs_conn_expire_now(cp); __ip_vs_conn_put(cp); //当开启了net.ipv4.vs.conntrack，SYN数据包会直接丢弃，等待客户端重新发送SYN if (uses_ct) return NF_DROP; //未开启conntrack时，会进入下面ip_vs_try_to_schedule的流程 cp = NULL; } } .... ... } TCP 请求发送第一个 SYN 包被丢弃后，需要等待一个 MSL （60s），客户端会重新发送 SYN。在高并发情况，由于会大量新建连接，会导致出现较多的端口重用情况，就导致连接等待 \u0026gt;=1s 进行重新发送握手包。所以，在上面的压测图中可以看到，connect 阶段最大会达到几秒钟。kubernetes 在1.13 版本开始，对该问题进行了优化，kube-proxy 默认会修改内核参数 net.ipv4.vs.conn_reuse_mode=0 。\n既然从 1.13 开始修改了默认参数，为什么我们的测试环境为 net.ipv4.vs.conn_reuse_mode=1 呢？先看一下，我们出现问题的环境 kubernetes 版本为 1.19.12，os 3.10.0。然后，我们对 1.19.12 k8s 代码进行 review 发现：\n1 2 3 4 5 6 7 8 9 10 11 connReuseMinSupportedKernelVersion = \u0026#34;4.1\u0026#34; ... ... if kernelVersion.LessThan(version.MustParseGeneric(connReuseMinSupportedKernelVersion)) { klog.Errorf(\u0026#34;can\u0026#39;t set sysctl %s, kernel version must be at least %s\u0026#34;, sysctlConnReuse, connReuseMinSupportedKernelVersion) } else { // Set the connection reuse mode if err := utilproxy.EnsureSysctl(sysctl, sysctlConnReuse, 0); err != nil { return nil, err } } ... ... 在 pr#82066 中提到，由于出现 3.10 版本内核中部署 kube-proxy 开启 ipvs 模式后无法启动。社区增加了内核版本的 check，1.19.0 开始如果内核版本 \u0026lt;4.1 则不会修改 net.ipv4.vs.conn_reuse_mode 内核参数。\n另外，上面 issue 中也提到了，主要影响是在大量短连接时会出现端口重用的情况。那如果我们将业务架构改成长连接是否就可以达到一样的效果呢？\n我们后端部署一个简单的 http server 进行压测可以发现，使用长连接的服务即便关闭端口复用 QPS 明显好与优化内核的场景。\n第二个问题，开启 net.ipv4.vs.conn_reuse_mode 参数后，端口重用导致的问题。我们在上文中提到过社区的一个 issue #81775。当开启端口重用，单个客户端使用 vip 发送大量请求，如果某个 pod 销毁会出现 no route to host 报错。\n1 08:50:10 E http_client.go:558\u0026gt; Unable to connect to 10.86.6.96:80 : dial tcp 10.86.6.96:80: connect: no route to host 08:50:11 E http_client.go:558\u0026gt; Unable to connect to 10.86.6.96:80 : dial tcp 10.86.6.96:80: connect: no route to host 08:50:12 E http_client.go:558\u0026gt; Unable to connect to 10.86.6.96:80 : dial tcp 10.86.6.96:80: connect: no route to host 08:50:13 E http_client.go:558\u0026gt; Unable to connect to 10.86.6.96:80 : dial tcp 10.86.6.96:80: connect: no route to host issue 中进行了大量讨论，我在这里只简单分析一下出现该问题的原因。\n在 Kubernetes 1.13 之前，kube-proxy ipvs 模式并不支持优雅删除，当 Endpoint 被删除时，kube-proxy 会直接移除掉 ipvs 中对应的 rs，这样会导致后续的数据包被丢掉。\n在 1.13 版本后，Kubernetes 添加了IPVS 优雅删除的逻辑：\n当 Pod 被删除时，kube-proxy 会先将 rs 的weight置为 0，以防止新连接的请求发送到此 rs，由于不再直接删除 rs，旧连接仍能与 rs 正常通信； 当 rs 的ActiveConn 数量为 0（现在已改为ActiveConn+InactiveConn==0)，即不再有连接转发到此 rs 时，此 rs 才会真正被移除。 上面有提过 InactiveConn 是处于 TIME_WAIT 的连接，那每个处于 InactiveConn 的连接多久会过期呢，默认是120s，通过 ipvsadm -L \u0026ndash;timeout 可以看到默认值：\n正常情况 120s 就将连接在 conntrack 表中删除。但当开启端口重用后，权重修改为 0 的 rs 如果再次被复用，对于端口复用的连接，ipvs 不会主动进行新的调度（调用ip_vs_try_to_schedule方法）；同时，只是将weight置为 0，也并不会触发由expire_nodest_conn 控制的结束连接或 DROP 操作，就这样，新连接的数据包当做什么都没发生一样，发送给了正在删除的 Pod。而这样的一个连接被 ipvs 认为是新的请求，会重置 ipvs timer，也就是说对应的这一个连接需要重新等待 120s 才会被删除。上面提到过，kube-proxy 在 ActiveConn+InactiveConn==0 时才会删除 rs，这样一来，只要不断的有端口复用的连接请求发来，rs 就不会被 kube-proxy 删除，上面提到的优雅删除是无法实现。\n当后端应用进程退出后，后面端口复用的请求，会发送到已经被完全删除的容器 ip 上，就会出现上面的 connect: no route to host 报错。并且这个报错根据 ipvs 另一个参数配置有关：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 static unsigned int ip_vs_in(struct netns_ipvs *ipvs, unsigned int hooknum, struct sk_buff *skb, int af) { ... .... /* * Check if the packet belongs to an existing connection entry */ cp = pp-\u0026gt;conn_in_get(ipvs, af, skb, \u0026amp;iph); //判断是否属于某个已有的connection conn_reuse_mode = sysctl_conn_reuse_mode(ipvs); //当conn_reuse_mode开启，同时出现端口复用（例如收到TCP的SYN包，并且也属于已有的 connection），进行处理 if (conn_reuse_mode \u0026amp;\u0026amp; !iph.fragoffs \u0026amp;\u0026amp; is_new_conn(skb, \u0026amp;iph) \u0026amp;\u0026amp; cp) { bool uses_ct = false, resched = false; //如果开启了expire_nodest_conn、目标rs的weight为0 if (unlikely(sysctl_expire_nodest_conn(ipvs)) \u0026amp;\u0026amp; cp-\u0026gt;dest \u0026amp;\u0026amp; unlikely(!atomic_read(\u0026amp;cp-\u0026gt;dest-\u0026gt;weight))) { resched = true; //查询是否用到了conntrack uses_ct = ip_vs_conn_uses_conntrack(cp, skb); } else if (is_new_conn_expected(cp, conn_reuse_mode)) { //连接是expected的情况，比如FTP uses_ct = ip_vs_conn_uses_conntrack(cp, skb); if (!atomic_read(\u0026amp;cp-\u0026gt;n_control)) { resched = true; } else { /* Do not reschedule controlling connection * that uses conntrack while it is still * referenced by controlled connection(s). */ resched = !uses_ct; } } //如果expire_nodest_conn未开启，并且也非期望连接，实际上直接跳出了 if (resched) { if (!atomic_read(\u0026amp;cp-\u0026gt;n_control)) ip_vs_conn_expire_now(cp); __ip_vs_conn_put(cp); //当开启了net.ipv4.vs.conntrack，SYN数据包会直接丢弃，等待客户端重新发送SYN if (uses_ct) return NF_DROP; //未开启conntrack时，会进入下面ip_vs_try_to_schedule的流程 cp = NULL; } } .... ... } 1 expire_nodest_conn=0 时，当后端 rs 不可达时，ipvs 会直接将数据包丢弃； expire_nodest_conn=1 时，当后端 rs 不可达，立即会返回一个报错给客户端。\n针对这个问题，只在 kubernetes 上并不能完美的解决，例如：kube-router 中增加了优雅下线，会等待 Pod 配置的 TerminationGracePeriodSeconds 后进行删除 rs，这样只能够在一定程度上避免该问题。\n单核性能更优？ 第三个问题，为什么使用 docker run 在线推理业务可以达到 4000 QPS，而 Kubernetes 容器通过内核优化后只能达到 2000？\n针对这个问题，首先和业务方同学进行确认启动 docker 的参数，经过确认发现业务方同学误将 \u0026ndash;cpuset-cpus 当作限制 cpu 使用了，其启动参数命令为：\n1 docker run --cpuset-cpus 4 xxxx 这个启动命令，最终创建的容器其实只使用了一个核，并且将应用进程绑定到了第 4 个 cpu 上。然后，通过修改命令改成非绑核，使用下面命令启动：\n1 docker run --cpu-quota 400000 xxxx 再进行压测 QPS 降低到了 2500，已经和 kubernetes 创建的容器非常接近。由于使用 vip 会经过 ipvs 进行数据包的处理，会有一定的性能损耗，这个结果也比较合理。\n那为什么绑核情况分配1个 cpu 的应用性能会比没有绑核 cpu 会好一倍呢？\n猜测和业务服务逻辑有关，后面还要再进行验证。\n沟通下来确认这个服务业务功能是：首先做少量的数学运算，然后再与通过 grpc 调用后端服务拿到的结果进行计算，将最终的结果返回给客户端。这样的话，这个业务应该算是一个 io 密集型应用，并不需要较高的 cpu 的， 绑核后能够减少 cpu 之间频繁的上下文切换，从而带来更好的效果。\n总结 通过修改 ipvs 内核参数，协助联邦同学解决了遇到的吞吐率问题，将 QPS 从 200 提升到了 2000+。\n然后，我们并不是止步于解决问题，对问题过程中遇到的疑惑进一步研究，帮助我们能够对 kubernetes 系统有全面的把控。\n在 1.19.0 版本开始 kubernetes 对 ipvs 默认内核参数进行了改进，当内核版本 \u0026lt;4.1 时，kube-proxy 不会修改 ipvs 内核参数 net.ipv4.vs.conn_reuse_mode。 通过修改内核参数提高了吞吐率，但同时带来了优雅下线的问题，在 5.9 开始 linux 内核层面已有解决。另外，我们针对可以对业务架构优化的场景使用长连接方式进行压测，能够显著的解决吞吐率降低的问题。 对于 docker run 场景 QPS 是 Kubernetes 容器的两倍问题，我们发现业务同学使用 docker 运行时使用了 \u0026ndash;cpuset-cpus 参数，也就是将应用绑定到某一个 cpu 核上。这说明对于某些应用并不是分配的应用 cpu 越多，性能越好。 目前，发现问题的主要场景是在使用官方默认网络组件 kube-proxy 带来的问题。而我们 IDC 内部使用 kube-router 使用了和 Pod TerminationGracePeriodSeconds 一致的等待时间来优雅删除 rs。\n另外，针对 ipvs 性能社区也有一些使用 eBPF 来实现的解决方案，例如：Cillium、腾讯 ipvs-ebpf 等。\n参考 https://marc.info/?l=linux-virtual-server\u0026m=151683112005533\u0026w=2 IPVS low throughput 解决关闭端口复用出现 1s 延迟的 patch 开启端口复用后 rs 下线导致后端不可用问题 绕过conntrack，使用eBPF增强 IPVS优化K8s网络性能 ","date":"2022-01-05T00:00:00Z","image":"https://jasonrd.github.io/blog/p/ipvs-caused-problem/matt-le-SJSpo9hQf7s-unsplash_hu958d513eeefe5556a31d065479ecc5ac_14205_120x120_fill_q75_box_smart1.jpg","permalink":"https://jasonrd.github.io/blog/p/ipvs-caused-problem/","title":"k8s 环境中 ipvs 带来的问题"},{"content":"背景 最近业务应用使用 Service ip 进行压测时，当容器销毁时，部分请求会出现 connect refused 错误。按照文档 云上 pod 下线引起短时服务不可用 进行优雅下线优化后有一定改善，但是仍然存在 connect refused 异常。本文通过分析 kube-router 实现 vip 的逻辑，进行定位问题根因，并举一反三对 kubernetes 默认组件 kube-proxy service IP 实现进行研究和分析。\nIPVS VIP 实现 ipvs (IP Virtual Server) 是工作在内核态的4层负载均衡，也就是我们常说的4层LAN交换，作为 Linux 内核的一部分。ipvs运行在主机上，在真实服务器集群前充当负载均衡器。ipvs可以将基于TCP和UDP的服务请求转发到真实服务器上，从而达到通过单个 VIP 代理多个后端真实服务的目的。IPVS 和 iptables 一样都是基于内核底层 netfilter 实现，netfilter 主要通过各个链的钩子实现包处理和转发。\nipvs 作为内核中负载均衡，有多种负载策略：rr（轮询）、wrr（加权轮询）、sh（源地址哈希）等，默认使用 rr 模式。vip 后面关联多个 pod ip， 通过 VIP 请求后 ipvs 会根据配置的均衡策略选取其中一个 pod ip 进行流量转发。\n如上图，使用 ipvsadm 工具查看 10.59.38.148:8001 转发的 RS（Real Server） 有两个：10.60.10.7、10.60.14.8，转发策略为 RR，其中 Weight 标识每个 RS 的权重。\n当 Weight=0 时，新连接不会转发到该 RS；但是，已建立的连接仍会保持，直到连接释放。ActiveConn 是活动连接数，也就是 tcp 连接状态的 ESTABLISHED；InActConn 是指除了 ESTABLISHED 以外的，所有的其它状态的 tcp 连接。\n常用开源组件实现逻辑 kube-router 先已公司使用 cni 插件为 kube-router 作为研究对象，通过使用部署一个简单的 go http server 两副本应用，观察删除一个 Pod 后，会发生哪些变化：\n上面两张图，分别对应 http server 在是否处理 SIGTERM 信号场景下对 ipvs 更新的影响。\n上图可以，看出当删除 Pod 时，在 20s+ 以后 kube-router 才会将被删除的 pod IP 在 ipvs 中摘掉。而摘掉流量方式是直接在 ipvs 中删除对应的 RS (10.60.10.7)：\n==》\n这样，在两种场景下会出现异常：\npod 容器已完全删除，但是 ipvs 还会转发流量到被删除的 Pod IP 上。也就容器完全删除早于 ipvs 中 RS 的删除动作； Ipvs 摘掉pod 流量时，存在未释放的连接。也就是 ipvs 中 RS 删除早于 pod 销毁，并且存在持久化的会话； 第一种问题还是未做好优雅下线导致的，可以通过 prestop 来增加优雅下线，提前将连接释放掉。但是，由于删除后 20s 仍然又新连接进来，虽然解决了第一个场景问题，但是第二个场景问题还是会存在的。\n在上述业务压测出现问题的场景中：\n业务容器已经做了优雅下线（等待15s+业务层开始优雅下线逻辑），整个优雅下线时间\u0026gt;=15s； 修改业务容器 terminatedGracePeriod 时长为 120s 后，我们抓包发现没有新流量进入，但仍然存在 connect refused 异常； 出现问题场景，属于第二种。那第二种问题如何解决呢？回答这个问题前，先要弄清楚下面疑问：\n首先，为什么 ipvs 在 20s 后才会将 pod ip 摘掉； 之前有了解过 ipvs 中 RS weight=0 时，新流量不会转发到该 RS，那 kube-router 有没有实现这个逻辑呢？ 带着上面两个问题，我们来看一下 kube-router 源码。\n源码分析 kube-router 内部会通过 list/watch 来监听 endpoint 和 service 更新，当 endpoint 发生变更（pod 删除）\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 // proxy/network_services_controller.go func (nsc *NetworkServicesController) OnEndpointsUpdate(ep *api.Endpoints) { ... ... // build new service and endpoints map to reflect the change newServiceMap := nsc.buildServicesInfo() newEndpointsMap := nsc.buildEndpointsInfo() if len(newEndpointsMap) != len(nsc.endpointsMap) || !reflect.DeepEqual(newEndpointsMap, nsc.endpointsMap) { nsc.endpointsMap = newEndpointsMap nsc.serviceMap = newServiceMap glog.V(1).Infof(\u0026#34;Syncing IPVS services sync for update to endpoint: %s/%s\u0026#34;, ep.Namespace, ep.Name) nsc.sync(synctypeIpvs) } else { glog.V(1).Infof(\u0026#34;Skipping IPVS services sync on endpoint: %s/%s update as nothing changed\u0026#34;, ep.Namespace, ep.Name) } } // OnServiceUpdate handle change in service update from the API server func (nsc *NetworkServicesController) OnServiceUpdate(svc *api.Service) { ... ... // build new service and endpoints map to reflect the change newServiceMap := nsc.buildServicesInfo() newEndpointsMap := nsc.buildEndpointsInfo() if len(newServiceMap) != len(nsc.serviceMap) || !reflect.DeepEqual(newServiceMap, nsc.serviceMap) { nsc.endpointsMap = newEndpointsMap nsc.serviceMap = newServiceMap glog.V(1).Infof(\u0026#34;Syncing IPVS services sync on update to service: %s/%s\u0026#34;, svc.Namespace, svc.Name) nsc.sync(synctypeIpvs) } else { glog.V(1).Infof(\u0026#34;Skipping syncing IPVS services for update to service: %s/%s as nothing changed\u0026#34;, svc.Namespace, svc.Name) } } 可以看到 endpoint 和 service 更新，都是先更新 nsc 的 endpointMap 和 serviceMap，然后执行 nsc.sync 函数。nsc.sync 函数将变更类型同步到 syncChan：\n1 2 3 4 5 6 7 8 //proxy/network_services_controller.go func (nsc *NetworkServicesController) sync(syncType int) { select { case nsc.syncChan \u0026lt;- syncType: default: glog.V(2).Infof(\u0026#34;Already pending sync, dropping request for type %d\u0026#34;, syncType) } } syncChan 在 NetworkServiceController 的 Run 函数中进行监听：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 // Run periodically sync ipvs configuration to reflect desired state of services and endpoints func (nsc *NetworkServicesController) Run(healthChan chan\u0026lt;- *healthcheck.ControllerHeartbeat, stopCh \u0026lt;-chan struct{}, wg *sync.WaitGroup) { ...... select { case \u0026lt;-stopCh: glog.Info(\u0026#34;Shutting down network services controller\u0026#34;) return default: // kube-router 默认启动时，先更新一遍 ipvs err := nsc.doSync() if err != nil { glog.Fatalf(\u0026#34;Failed to perform initial full sync %s\u0026#34;, err.Error()) } nsc.readyForUpdates = true } // loop forever until notified to stop on stopCh for { select { case \u0026lt;-stopCh: nsc.mu.Lock() nsc.readyForUpdates = false nsc.mu.Unlock() glog.Info(\u0026#34;Shutting down network services controller\u0026#34;) return case \u0026lt;-gracefulTicker.C: if nsc.readyForUpdates \u0026amp;\u0026amp; nsc.gracefulTermination { glog.V(3).Info(\u0026#34;Performing periodic graceful destination cleanup\u0026#34;) nsc.gracefulSync() } // 从 syncChan 唤醒协程执行 ipvs 的更新/同步逻辑 case perform := \u0026lt;-nsc.syncChan: healthcheck.SendHeartBeat(healthChan, \u0026#34;NSC\u0026#34;) switch perform { case synctypeAll: glog.V(1).Info(\u0026#34;Performing requested full sync of services\u0026#34;) err := nsc.doSync() if err != nil { glog.Errorf(\u0026#34;Error during full sync in network service controller. Error: \u0026#34; + err.Error()) } case synctypeIpvs: glog.V(1).Info(\u0026#34;Performing requested sync of ipvs services\u0026#34;) nsc.mu.Lock() // ipvs 的更新/同步逻辑 err := nsc.syncIpvsServices(nsc.serviceMap, nsc.endpointsMap) nsc.mu.Unlock() if err != nil { glog.Errorf(\u0026#34;Error during ipvs sync in network service controller. Error: \u0026#34; + err.Error()) } } if err == nil { healthcheck.SendHeartBeat(healthChan, \u0026#34;NSC\u0026#34;) } case \u0026lt;-t.C: glog.V(1).Info(\u0026#34;Performing periodic sync of ipvs services\u0026#34;) healthcheck.SendHeartBeat(healthChan, \u0026#34;NSC\u0026#34;) err := nsc.doSync() if err != nil { glog.Errorf(\u0026#34;Error during periodic ipvs sync in network service controller. Error: \u0026#34; + err.Error()) glog.Errorf(\u0026#34;Skipping sending heartbeat from network service controller as periodic sync failed.\u0026#34;) } else { healthcheck.SendHeartBeat(healthChan, \u0026#34;NSC\u0026#34;) } } } } 更新 ipvs 函数 nsc.syncIpvsServices ：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 // proxy/service_endpoints_sync.go // sync the ipvs service and server details configured to reflect the desired state of Kubernetes services // and endpoints as learned from services and endpoints information from the api server func (nsc *NetworkServicesController) syncIpvsServices(serviceInfoMap serviceInfoMap, endpointsInfoMap endpointsInfoMap) error { start := time.Now() defer func() { endTime := time.Since(start) if nsc.MetricsEnabled { metrics.ControllerIpvsServicesSyncTime.Observe(endTime.Seconds()) } glog.V(1).Infof(\u0026#34;sync ipvs services took %v\u0026#34;, endTime) }() var err error var syncErrors bool // map to track all active IPVS services and servers that are setup during sync of // cluster IP, nodeport and external IP services activeServiceEndpointMap := make(map[string][]string) // 配置 VIP ipvs err = nsc.setupClusterIPServices(serviceInfoMap, endpointsInfoMap, activeServiceEndpointMap) if err != nil { syncErrors = true glog.Errorf(\u0026#34;Error setting up IPVS services for service cluster IP\u0026#39;s: %s\u0026#34;, err.Error()) } // 配置 nodeport ipvs err = nsc.setupNodePortServices(serviceInfoMap, endpointsInfoMap, activeServiceEndpointMap) if err != nil { syncErrors = true glog.Errorf(\u0026#34;Error setting up IPVS services for service nodeport\u0026#39;s: %s\u0026#34;, err.Error()) } err = nsc.setupExternalIPServices(serviceInfoMap, endpointsInfoMap, activeServiceEndpointMap) if err != nil { syncErrors = true glog.Errorf(\u0026#34;Error setting up IPVS services for service external IP\u0026#39;s and load balancer IP\u0026#39;s: %s\u0026#34;, err.Error()) } // 清理过期 vip err = nsc.cleanupStaleVIPs(activeServiceEndpointMap) if err != nil { syncErrors = true glog.Errorf(\u0026#34;Error cleaning up stale VIP\u0026#39;s configured on the dummy interface: %s\u0026#34;, err.Error()) } // 清理过期 RS IP err = nsc.cleanupStaleIPVSConfig(activeServiceEndpointMap) if err != nil { syncErrors = true glog.Errorf(\u0026#34;Error cleaning up stale IPVS services and servers: %s\u0026#34;, err.Error()) } err = nsc.syncIpvsFirewall() if err != nil { syncErrors = true glog.Errorf(\u0026#34;Error syncing ipvs svc iptables rules to permit traffic to service VIP\u0026#39;s: %s\u0026#34;, err.Error()) } err = nsc.setupForDSR(serviceInfoMap) if err != nil { syncErrors = true glog.Errorf(\u0026#34;Error setting up necessary policy based routing configuration needed for direct server return: %s\u0026#34;, err.Error()) } if syncErrors { glog.V(1).Info(\u0026#34;One or more errors encountered during sync of IPVS services and servers to desired state\u0026#34;) } else { glog.V(1).Info(\u0026#34;IPVS servers and services are synced to desired state\u0026#34;) } return nil } 每次有 endpoint/service 发生变化，nsc.syncIpvsServices 都会将所有ipvs 更新一遍，整个函数执行时间根据 service 数量不同执行时间不同。在我们线下环境有 3200 个 service，函数执行时间 30s+。\n这就解答了「为什么 ipvs 在 20s 后才会将 pod ip 摘掉」。\n那上面说的设置 ipvs rs weight 来停止新连接的转发，在 kube-router 中有没有实现呢？我们在阅读源码时，发现下面这一部分代码：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 // Run periodically sync ipvs configuration to reflect desired state of services and endpoints func (nsc *NetworkServicesController) Run(healthChan chan\u0026lt;- *healthcheck.ControllerHeartbeat, stopCh \u0026lt;-chan struct{}, wg *sync.WaitGroup) { ...... // loop forever until notified to stop on stopCh for { select { case \u0026lt;-stopCh: nsc.mu.Lock() nsc.readyForUpdates = false nsc.mu.Unlock() glog.Info(\u0026#34;Shutting down network services controller\u0026#34;) return case \u0026lt;-gracefulTicker.C: if nsc.readyForUpdates \u0026amp;\u0026amp; nsc.gracefulTermination { glog.V(3).Info(\u0026#34;Performing periodic graceful destination cleanup\u0026#34;) nsc.gracefulSync() } ...... } NSC 中存在一个 graceful 定时器，触发时会执行 nsc.gracefulSync 函数：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 // proxy/network_service_graceful.go func (nsc *NetworkServicesController) gracefulSync() { nsc.gracefulQueue.mu.Lock() defer nsc.gracefulQueue.mu.Unlock() var newQueue []gracefulRequest // Itterate over our queued destination removals one by one, and don\u0026#39;t add them back to the queue if they were processed for _, job := range nsc.gracefulQueue.queue { if removed := nsc.gracefulDeleteIpvsDestination(job); removed { continue } newQueue = append(newQueue, job) } nsc.gracefulQueue.queue = newQueue } func (nsc *NetworkServicesController) gracefulDeleteIpvsDestination(req gracefulRequest) bool { var deleteDestination bool // Get active and inactive connections for the destination aConn, iConn, err := nsc.getIpvsDestinationConnStats(req.ipvsSvc, req.ipvsDst) if err != nil { glog.V(1).Infof(\u0026#34;Could not get connection stats for destination: %s\u0026#34;, err.Error()) } else { // Do we have active or inactive connections to this destination // if we don\u0026#39;t, proceed and delete the destination ahead of graceful period if aConn == 0 \u0026amp;\u0026amp; iConn == 0 { deleteDestination = true } } // Check if our destinations graceful termination period has passed if time.Since(req.deletionTime) \u0026gt; req.gracefulTerminationPeriod { deleteDestination = true } //Destination has has one or more conditions for deletion if deleteDestination { glog.V(2).Infof(\u0026#34;Deleting IPVS destination: %s\u0026#34;, ipvsDestinationString(req.ipvsDst)) if err := nsc.ln.ipvsDelDestination(req.ipvsSvc, req.ipvsDst); err != nil { glog.Errorf(\u0026#34;Failed to delete IPVS destination: %s, %s\u0026#34;, ipvsDestinationString(req.ipvsDst), err.Error()) } } return deleteDestination } nsc.gracefulSync 函数主要逻辑是对 ipvs 中 RS 进行删除，判断逻辑如下：\nInActConn 和 ActConn 都为0，则说明没有连接存在，可以直接删除； 如果 InActConn 或 ActConn 不为零，但是 pod 删除等待时长已经超过 pod.gracefulTerminationPeriod，则可以直接删除； 否则，等待下一次进行； 那 nsc.gracefulQueue 队列是在哪里写入的呢？我们之间检索 gracefulQueue 字段，发现在函数 nsc.ipvsDeleteDestination 中写入\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 // proxy/network_service_graceful.go func (nsc *NetworkServicesController) ipvsDeleteDestination(svc *ipvs.Service, dst *ipvs.Destination) error { // If we have enabled graceful termination set the weight of the destination to 0 // then add it to the queue for graceful termination if nsc.gracefulTermination { req := gracefulRequest{ ipvsSvc: svc, ipvsDst: dst, deletionTime: time.Now(), } dst.Weight = 0 err := nsc.ln.ipvsUpdateDestination(svc, dst) if err != nil { return err } nsc.addToGracefulQueue(\u0026amp;req) } else { err := nsc.ln.ipvsDelDestination(svc, dst) if err != nil { return err } } // flush conntrack when Destination for a UDP service changes if svc.Protocol == syscall.IPPROTO_UDP { if err := nsc.flushConntrackUDP(svc); err != nil { glog.Errorf(\u0026#34;Failed to flush conntrack: %s\u0026#34;, err.Error()) } } return nil } nsc.ipvsDeleteDestination 判断 nsc.gracefulTermination 如果开启，则不会立即删除。而是，执行下面操作：\n更新 ipvs 配置，设置 RS weight=0； 加入 gracefulQueue 队列中，等待删除； 综上，可以看出 nsc.gracefulTermination 就是开启 ipvs 优雅下线的开关，而这个是通过参数 \u0026ndash;ipvs-graceful-termination 来控制的。然后，我们开启 ipvs-graceful-termination 进行测试。\n我们对测试应用 http-server 增加优雅下线逻辑：\n捕获 SIGTERM 信号； 捕获到 SIGTERM 信号后，在 http header 中增加 connection: close，也就是在容器下线阶段使用短连接； 然后再进行压测：\n=\u0026gt;=\u0026gt;\n=\u0026gt;\n上面分别对应，销毁 Pod 前、销毁 Pod 过程中1、销毁 Pod 过程中2、Pod 被完全销毁。\n通过开启 ipvs graceful terminated，并且容器销毁后应用捕获 SIGTERM 信号进行连接的优雅下线，测试中未出现请求错误的问题。\n总结 对以上源码研究，进行总结如下：\nkube-router 每次更新都是全量更新，service 数量不同 ipvs RS 新增、更新、删除的延迟不同； 具有优雅删除 RS 的能力，并且结合了 Pod terminatedGracefulPeriod 进行 RS 的完全删除； 为开启 \u0026ndash;ipvs-graceful-termination，会立即删除 RS，但是由于全量更新的延迟，表现上是有一定延迟（线下 30s+、线上15s+）； kube-proxy 实现逻辑 根据 1.22 版本 kube-proxy 代码，pod 删除过程：\n销毁 Pod，Endpoint 中移除销毁中的 Pod IP； kube-proxy watch 到 endpoint 发生变化，进入 ipvs 删除判断逻辑： 协议不是 UDP 或 SFTP； ActConn 和 InActConn 都为0； 不满足删除判断，则更新 ipvs 中 RS weight=0; 然后，将 RS 加入到本地队列，每隔 1min 进行删除判断； 源码 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 // proxy/ipvs/graceful_termination.go // GracefulDeleteRS to update rs weight to 0, and add rs to graceful terminate list func (m *GracefulTerminationManager) GracefulDeleteRS(vs *utilipvs.VirtualServer, rs *utilipvs.RealServer) error { // Try to delete rs before add it to graceful delete list ele := \u0026amp;listItem{ VirtualServer: vs, RealServer: rs, } deleted, err := m.deleteRsFunc(ele) if err != nil { klog.Errorf(\u0026#34;Delete rs %q err: %v\u0026#34;, ele.String(), err) } if deleted { return nil } rs.Weight = 0 err = m.ipvs.UpdateRealServer(vs, rs) if err != nil { return err } klog.V(5).Infof(\u0026#34;Adding an element to graceful delete rsList: %+v\u0026#34;, ele) m.rsList.add(ele) return nil } func (m *GracefulTerminationManager) deleteRsFunc(rsToDelete *listItem) (bool, error) { klog.V(5).Infof(\u0026#34;Trying to delete rs: %s\u0026#34;, rsToDelete.String()) rss, err := m.ipvs.GetRealServers(rsToDelete.VirtualServer) if err != nil { return false, err } for _, rs := range rss { if rsToDelete.RealServer.Equal(rs) { // For UDP and SCTP traffic, no graceful termination, we immediately delete the RS // (existing connections will be deleted on the next packet because sysctlExpireNoDestConn=1) // For other protocols, don\u0026#39;t delete until all connections have expired) if utilipvs.IsRsGracefulTerminationNeeded(rsToDelete.VirtualServer.Protocol) \u0026amp;\u0026amp; rs.ActiveConn+rs.InactiveConn != 0 { klog.V(5).Infof(\u0026#34;Not deleting, RS %v: %v ActiveConn, %v InactiveConn\u0026#34;, rsToDelete.String(), rs.ActiveConn, rs.InactiveConn) return false, nil } klog.V(5).Infof(\u0026#34;Deleting rs: %s\u0026#34;, rsToDelete.String()) err := m.ipvs.DeleteRealServer(rsToDelete.VirtualServer, rs) if err != nil { return false, fmt.Errorf(\u0026#34;Delete destination %q err: %v\u0026#34;, rs.String(), err) } return true, nil } } return true, fmt.Errorf(\u0026#34;Failed to delete rs %q, can\u0026#39;t find the real server\u0026#34;, rsToDelete.String()) } kube-proxy GracefulDeleteRS 函数会 deleteRsFunc 进行 RS 删除，删除 RS 条件：\n协议不是 UDP 或 SFTP； ActConn 和 InActConn 都为0； 否则，更新 ipvs 将 RS weight 置为 0，不接收新流量进入。然后会加入到带删除列表中，进行定期（间隔1min）清理：\n1 2 3 4 5 6 7 8 9 10 func (m *GracefulTerminationManager) tryDeleteRs() { if !m.rsList.flushList(m.deleteRsFunc) { klog.Errorf(\u0026#34;Try flush graceful termination list err\u0026#34;) } } // Run start a goroutine to try to delete rs in the graceful delete rsList with an interval 1 minute func (m *GracefulTerminationManager) Run() { go wait.Until(m.tryDeleteRs, rsCheckDeleteInterval, wait.NeverStop) } 优化方案 优点 缺点 方案一 kube-router 开启 \u0026ndash;ipvs-graceful-termination 参数 方案二 开发内网负载均衡组件 参考 http://www.dockone.io/article/9441 https://wsgzao.github.io/post/lvs-nat/ Issue 572 - Graceful termination kube-proxy ipvs support graceful termination ","date":"2021-12-19T00:00:00Z","image":"https://jasonrd.github.io/blog/p/how-endpoint-flush-ipvs/the-creative-exchange-d2zvqp3fpro-unsplash_huf941de4769045cdfa8c9ee7036519a2a_35369_120x120_fill_q75_box_smart1.jpg","permalink":"https://jasonrd.github.io/blog/p/how-endpoint-flush-ipvs/","title":"endpoint 更新后 vip 转发实现探究"},{"content":"背景 最近看到了一个文章[1]，其中提到了 sync.map 的一个bug，key 如果使用连接，会导致连接泄漏。文中提到了具体原因是 sync.map 对 key 删除是软删除，只是将 可以对应的 value 置为 nil，key 还会继续存在。\n根据文章中描述，发现自己对 sync.Map 了解不够透彻，所以就打算进行一番了解。sync.Map 是 go 标准库中实现的线程安全的 Map，主要适用的场景：\nThe Map type is optimized for two common use cases: (1) when the entry for a given key is only ever written once but read many times, as in caches that only grow, or (2) when multiple goroutines read, write, and overwrite entries for disjoint sets of keys. In these two cases, use of a Map may significantly reduce lock contention compared to a Go map paired with a separate Mutex or RWMutex.\n同一个 key 读多写少； 并发读、写、覆盖不同 key； 其中，sync.Map 包中提到，在这两种场景中性能要优于使用 Map + Mutex。接下来我们看一下如何实现的，为什么在这两种场景下性能有优势。\n源码分析 数据结构 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 type Map struct { mu Mutex // 只读区，保存部分健值对 // 修改时进行原子性替换 read atomic.Value // 新写入的健值对，先保存到 dirty 中 // 更新或者被删除的，dirty 中不会存储 expunged 健值对 dirty map[interface{}]*entry // 记录 Load 出现多少次在 dirty 中读取健值对情况。 misses int } // 存储 value 的指针 // key 被删除时通过 CAS 修改为 nil 或 expunged type entry struct { p unsafe.Pointer // *interface{} } type readOnly struct { // 存储的部分健值对，只会 m 中元素 // 不会存在并发读写情况。 m map[interface{}]*entry // dirty 中是否有 read.m 中不存在的健值对 amended bool // true if the dirty map contains some key not in m. } sync.Map 中使用了两个 map 对象来尽量避免锁竞争，相当于增加一个缓冲。其中 read map 中记录部分健值对，dirty 中保存新写入的 value 和 read.m 中未被标记为 expunged 或 nil 的 entry。\n接口分析 Store 逻辑： 判断 read.m 中是否存在key。 如果存在进行尝试更新，会起一个循环不停尝试 CAS 更新，更新成功则返回。 如果read.m 中不存在 key，或者更新过程中发现 key 对应 entry 已被标记为 expunged，则进行第二步； 可以看出如果更新某个 key (已经在 read.m 中，并且未被删除) ，更新并不会加锁 加锁，再次判断 read.m 中是否存在 如果存在，然后判断之前 entry 是否为 expunged 状态，如果是（说明 dirty 中不存在 key），则把 key-entry 写入 dirty map 中；然后，原子更新 entry.p 指针。 否则进行第三步 dirty map 中是否存在 key，如果存在，则直接原子更新 entry.p 指针；否则，进行第四步； read 和 dirty 中都不存在目标 key，则添加到 dirty 中。添加会做一些数据同步操作： 如果 read.m 不需要进行修正（ read.amended = false），则同步 read.m 中为被删除（enty.p!=nil or expunged）健值对到 dirty map 中。并通过 CAS 操作更新read.m 中被删除的 entry（enty.p = nil）标记为 expunged。 否则，直接添加到 dirty 中。 Load 逻辑 read.m 中是否存在，如果存在，判断是否已被删除，并返回相应状态；否则，进行下一步； 判断 read.amended 状态： 如果需要修正，进行加锁，然后再一次判断 read.m 和 read.amended，避免并发过程中 read 出现更新； 获取 dirty 中 key，并更新 miss 次数；如果 miss 次数达到 dirty.length，则更新read，并置 dirty=nil; 释放锁； 判断是否存在 key，或者是否被删除，进行相应返回 Delete 逻辑 删除和 Load key 逻辑类似。\n首先，判断 read 中是否存在，如果存在，则标记 entry.p 为nil；如果不存在，继续判断 amended ； 加锁，进行二次判断，然后调用 delete 删除 dirty map 中 key；释放锁； 总结 read.m 中存在的 key 并非读写不会加锁，进行原子更新；所以，对于同一个 key 进行读写，具有较优的性能； 写入时，优先写入 dirty map，对于其他已经存在于 read.m 中的 key 读写并没有性能影响； 通过上面写入、删除、获取逻辑，可以看出：\nkey 大部分是软删除，即标记 entry.p 为 nil 或 expunged。只有在删除-\u0026gt;新增-\u0026gt;读取-\u0026gt;(missed \u0026gt; dirty.length)-\u0026gt;修正read.m情况会通过 GC 自动删除 key。 read.amended 标示存在某些 key 只存在 dirty 中； read.m 更新是在 missed 次数达到阈值（dirty.length）时，直接进行原子覆盖。 为什么不 miss 一次进行一次更新？ dirty 为 nil 情况：未被使用的 map；miss 超过阈值时； 持有锁情况：修改 dirty、修正 read； 看代码时对 store 中判断，entry.unexpungeLocked 时对 dirty 才写入 key。对这个逻辑有个疑问，为什么 entry 为 expunged 状态说明 dirty 不为 nil ，并且其中不存在该 key 呢？ 通过下图能够看到，enrty 只有 read.m 与 dirty 同步逻辑中才会修改为 expunged; 并且同步过程不会同步被删除的 key；所以，expunged 状态的 key 不会存在于 dirty 中。 如5，dirty map 为 nil 只有两种情况，所以只有修正read 情况。因为 read.m 中包含 key，所以并没有进行原子更新 read。这样，就会出现 dirty =nil 情况； dirty 中不会包含 expunged 状态的 entry，并且包含所有有效 key； reference sync.Map 引起的 bug 如何实现一个线程安全的map ","date":"2021-12-17T00:00:00Z","permalink":"https://jasonrd.github.io/blog/p/golang-sync-map/","title":"GO sync.map 探究"},{"content":"读写锁引入 有下面一段程序，面试官问这段程序有什么问题？\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 type Store struct { a string b string sync.RWMutex } func (s *Store) GetA() string { fmt.Println(\u0026#34;get a\u0026#34;) s.RLock() fmt.Println(\u0026#34;get a2\u0026#34;) defer s.RUnlock() return s.a } func (s *Store) GetAB() (string, string) { fmt.Println(\u0026#34;get ab\u0026#34;) s.RLock() fmt.Println(\u0026#34;get ab2\u0026#34;) defer s.RUnlock() return s.GetA(), s.b } func (s *Store) Write(a, b string) { fmt.Println(\u0026#34;write\u0026#34;) s.Lock() defer s.Unlock() fmt.Println(\u0026#34;write2\u0026#34;) s.a = a s.b = b } 看到这段程序程序，首先想到的是读写锁的问题； 其次，看 Store 这个结构体，各个函数都定义的是指针函数。那就说明：不存在读写锁的 copy 过程； GetAB 方法中通过调用 GetA 方法，在 s.RUnlock 前通过调用 s.GetA，又做了一次读写锁上锁 s.RLock，但是读锁可以多次上锁，所以单看这里没什么问题； 然后，想到会不会 Write 和 GetAB 并发调用的时候会存在问题呢？思考了一会，觉得没问题，就放弃了。 以上，是面试时整个思路。\n回头，越想越觉得这里哪里有问题，就在夜读群里求教了一下，群里大神发了一篇读写锁优先级的文章，然后给了一段测试样例，瞬间豁然开朗。\nmain函数逻辑如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 func main() { store := Store{} wg := sync.WaitGroup{} wg.Add(2) go func() { defer wg.Done() for i := 1; i \u0026lt; 10000; i += 1 { fmt.Println(\u0026#34;main write \u0026#34;, i) store.Write(\u0026#34;111\u0026#34;, \u0026#34;1111\u0026#34;) } }() go func() { defer wg.Done() for i := 1; i \u0026lt; 10000; i += 1 { fmt.Println(\u0026#34;main get ab\u0026#34;, i) store.GetAB() } }() wg.Wait() } 执行结果\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 main get ab 12 //main函数读取ab get ab //进入 s.GetAB 函数 main write 1 //main 函数写数据 write //进入 s.Write 函数 write2 //获取写锁 main write 2 .... //写锁一直抢占 .... main write 13 //main 函数写数据 write //进入 s.Write 函数 write2 //获取写锁 main write 14 write write2 get ab2 //之前 get ab 12 才获得读锁 get a //进入 GetA get a2 //获取读锁 main get ab 13 //main函数 get ab get ab //进入 s.GetAB 函数 get ab2 //获取读锁 main write 15 //注意⚠️ 这个时候写数据开始了 write //进入 Write 函数，后面尝试获取写锁 get a //这个时候 GetAB 进入了 GetA，尝试获取读锁 fatal error: all goroutines are asleep - deadlock! //出现了死锁 分析：\n1 2 3 4 5 6 GetAB | GetA | Write | | r0 占用读锁 | | | | w0 尝试获取写锁 等待r0释放读锁 | r1 尝试获取读锁，排在w0后面 | 由于读写锁的优先级，读锁和写锁同时竞争时，读锁要排在写锁后面，导致了 r1 竞争 w0的锁，w0竞争r0，r0执行不下去，最后死锁。\n读写锁底层 读写锁前置条件：\n读写互斥，但是读读不互斥； 读、写锁都不会出现饥饿； 保证读上锁数量与解锁数量一致； 可以思考下，如果让你设计一个这样的锁，你会怎么设计？\ngo中读写锁的结构，如下：\n1 2 3 4 5 6 7 type RWMutex struct { w Mutex // 用来保证同一时间只有一个写锁能够抢到锁 writerSem uint32 // 写锁信号量，在读锁全部解锁时通知阻塞的写锁 readerSem uint32 // 读锁信号量，在写锁解锁时通知阻塞的读操作 readerCount int32 // 等待、已上锁的读锁数量 readerWait int32 // 写锁获得锁前，已经上锁的读锁数量 } 读锁逻辑 首先，看一下读上锁逻辑：\n1 2 3 4 5 6 7 8 func (rw *RWMutex) RLock() { ... if atomic.AddInt32(\u0026amp;rw.readerCount, 1) \u0026lt; 0 { // A writer is pending, wait for it. runtime_SemacquireMutex(\u0026amp;rw.readerSem, false) } ... } 上面，上读锁逻辑获试图获取读锁数量原子性加一： atomic.AddInt32(\u0026amp;rw.readerCount, 1)。自增操作返回值如果小于0，则阻塞等待信号量 readerSem 唤醒。\n疑问：\n什么情况下 readerCount 小于0； runtime_SemacquireMutex 不会造成读读互斥么？ 如何保证读、写互斥？ 再来看一下，读解锁逻辑：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 func (rw *RWMutex) RUnlock() { ... if r := atomic.AddInt32(\u0026amp;rw.readerCount, -1); r \u0026lt; 0 { if r+1 == 0 || r+1 == -rwmutexMaxReaders { race.Enable() throw(\u0026#34;sync: RUnlock of unlocked RWMutex\u0026#34;) } // A writer is pending. if atomic.AddInt32(\u0026amp;rw.readerWait, -1) == 0 { // The last reader unblocks the writer. runtime_Semrelease(\u0026amp;rw.writerSem, false) } } ... } 解锁逻辑：先对 atomic.AddInt32(\u0026amp;rw.readerCount, -1) 进行原子性减一操作。\nr大于 0 ：直接释放锁完成； r小于 0 ：进行读锁数量一致性判断，atomic.AddInt32(\u0026amp;rw.readerWait, -1) 针对 readerWait 原子性减一后判断是否为 0，为 0 则唤起写锁信号量； 与读加锁类似，同样有 atomic.AddInt32(\u0026amp;rw.readerCount, -1) 小于 0 判断。可以有结论 rw.readerCount 小于 0，为写锁上锁的充要条件，后面分析写锁时进行验证。\n解决了的问题：\n释放读锁，读锁全部释放后唤起写锁； 上锁与解锁数量一致性保证； 疑问：\nreaderCount 修改成一个负数？如何保证这个负数足够小呢？ 写锁逻辑 先上代码：\n1 2 3 4 5 6 7 8 9 10 11 12 func (rw *RWMutex) Lock() { ... // First, resolve competition with other writers. rw.w.Lock() // Announce to readers there is a pending writer. r := atomic.AddInt32(\u0026amp;rw.readerCount, -rwmutexMaxReaders) + rwmutexMaxReaders // Wait for active readers. if r != 0 \u0026amp;\u0026amp; atomic.AddInt32(\u0026amp;rw.readerWait, r) != 0 { runtime_SemacquireMutex(\u0026amp;rw.writerSem, false) } ... } 写上锁逻辑：\n首先，互斥量上锁，保证只有一个写锁加锁成功。 然后，令 readerCount 原子性减去 rwmutexMaxReaders（这是个常量，具体定义 const rwmutexMaxReaders = 1 \u0026lt;\u0026lt; 30）。这里可以验证之前猜想，rw.readerCount 小于0，是持有锁的充要条件。 atomic.AddInt32(\u0026amp;rw.readerCount, -rwmutexMaxReaders) + rwmutexMaxReaders 返回结果是在写锁获取前，已持有读锁的数量 r。 r=0，说明没有读锁； r\u0026lt;0，只有在读解锁数量\u0026gt;读加锁数量，或写锁多次时发生；第一个情况，读解锁会 check；第二种情况，mutex 保证同时只有一个写锁； r\u0026gt;0，存在读锁； 再进行 r!=0 判断（即存在读锁）。原子性操作 atomic.AddInt32(\u0026amp;rw.readerWait, r)，记录需要等待的读锁数量，然后等待writerSem唤醒。 最终，保证：1. 写锁唯一性；2. 等待读锁完全释放；3. 阻塞后面读锁的获取；\n再来看一下，写锁解锁逻辑：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 func (rw *RWMutex) Unlock() { ... // Announce to readers there is no active writer. r := atomic.AddInt32(\u0026amp;rw.readerCount, rwmutexMaxReaders) if r \u0026gt;= rwmutexMaxReaders { race.Enable() throw(\u0026#34;sync: Unlock of unlocked RWMutex\u0026#34;) } // Unblock blocked readers, if any. for i := 0; i \u0026lt; int(r); i++ { runtime_Semrelease(\u0026amp;rw.readerSem, false) } // Allow other writers to proceed. rw.w.Unlock() ... } 解锁逻辑：\n原子性操作 atomic.AddInt32(\u0026amp;rw.readerCount, rwmutexMaxReaders)。这里，能够看到两个隐含的点： 原子操作结束后，如果有其他读锁试图获取读锁，不需要阻塞； 这个时候其他线程还是不能够获取写锁； 即：写锁释放锁时，读锁要比写锁优先级高； 原子操作返回值，是当前读锁数量。包括在写锁前读锁（写锁未完全获得情况下写锁解锁），和写锁后阻塞读锁；然后 runtime_Semrelease 唤起阻塞着的读锁。 runtime_Semrelease \u0026gt; runtime_SemacquireMutex 会不会存在问题？验证过不会。 然后写锁释放； 总结 通过分析，可以得出结论：\n写锁释放过程中，读锁优先级要高于写锁； 读锁加锁后，写锁可以进入加锁过程，但是要等待之前读锁释放；即，并不少写锁优先级高于写锁，而是在读锁已经上锁，或没有持有读写锁的协程条件下，读写锁都有机会获取锁； 所以，针对之前的面试题，读锁嵌套读锁，在有写锁的时候，依据结论2会发生死锁。\n通过上面分析，存在待验证问题：\n一个协程个已获取读锁，另个协程试图获取写锁，还有一个协程在完全获取写锁前调用Unlock，再一个协程释放读锁，按顺序进行流程。会发生死锁具体可以自己分析（写锁信号量永远阻塞）；\n一个协程已上写锁锁，一个协程试图获取读锁，然后另一个协程释放读锁，最后一个协程释放写锁，同样会发生死锁（读信号量永远阻塞）；\n在以后用锁的时候不管有没有优先级，都要时刻记住死锁的四个必要条件：\n互斥条件：一个资源每次只能被一个进程使用。 锁的不可抢占：进程已获得的资源，在末使用完之前，不能强行剥夺。 占有且等待：一个进程因请求资源而阻塞时，对已获得的资源保持不放。 循环等待条件: 若干进程之间形成一种头尾相接的循环等待资源关系。 参考 [sync.RWMutex]https://medium.com/golangspec/sync-rwmutex-ca6c6c3208a0 ","date":"2021-11-09T00:00:00Z","image":"https://jasonrd.github.io/blog/p/golang-rwmutex-deadlock/daedlock_hu26d071d1a9c611a8342c73b77f908c12_41977_120x120_fill_q75_box_smart1.jpeg","permalink":"https://jasonrd.github.io/blog/p/golang-rwmutex-deadlock/","title":"GO RWMutex 中隐藏的死锁问题"},{"content":"背景 近日，接连收到多个云上站点业务出现 502 问题反馈：\n和业务负责人沟通后，应用确认加入了优雅下线逻辑。\n排查过程 首先，查看网关日志：\n两次日志，都是请求 LB IP 出现 503 错误码后，然后网关将 LB IP 摘掉。\n分析为什么出现 503 错误码前，先了解一下容器下线逻辑：容器进行下线时，会调用 prestop 脚本执行下线前的操作。\n在 prestop 脚本中，首先 sleep 15s （不要问我为什么），然后调用 http://127.0.0.1:${APP_PORT}/ok.htm?down=true 接口通知 java 进程进行优雅下线。该接口调用成功后，再请求应用 ok 页面，进入下面逻辑：\n也就是返回 halting 数据和503状态码。\nk8s 中在将 Pod 进行下线（标记为 Terminating 状态）时，k8s endpoint controller 就将该 Pod ip 从 lb 或 service 后端列表中摘除。既然 lb/svc 已经将在该 pod IP 摘除，为什么仍然请求到 halting Pod 呢？\n在进入应用容器中进行抓包，并和应用负责人确认后，网关 -\u0026gt; lb -\u0026gt; pod 是使用 http 长连接方式。\n在 Pod 处于 terminating 状态时，通过 svc 请求时新建立的连接将不会转发到该 pod，但是已经建立的连接在 Pod 完全删除前仍可继续通信。所以，虽然 service 将 Pod IP 摘除，但是为了保证容器的优雅下线，已经建立的连接仍然可以继续处理业务，直到容器彻底被删除。\n我们用 python 简单写了一个使用 http 长连接客户端，进行一下测试：\n1 2 3 4 5 6 7 8 9 10 import requests import time client=requests.session() headers = {\u0026#39;Content-Type\u0026#39;: \u0026#39;application/json\u0026#39;, \u0026#39;Connection\u0026#39;: \u0026#39;keep-alive\u0026#39;} while True: r=client.get(\u0026#34;http://xx.xx.16.137:8088/ok.htm\u0026#34;, headers=headers) # xx.xx.16.137 为应用 lb IP print r.status_code print r.content time.sleep(1) python client 请求应用容器：\n上面可以看到，在 15:30:06 将 pod 进行 kill 后，通过长连接仍然可以将请求转发到处于 terminating 的应用容器。直到 15s 后调用下线接口，请求返回 503。请求处于 halting 状态的 pod，server 端会主动 close 请求：\n另外，我们测试应用只有单个实例，也就是实例被删除后，lb 后端实例为 0，请求 lb 的新连接无法建立。所以，测试脚本会出现 connect refused 报错。\n解决方法 综上可知，问题原因是 kill pod 后仍然会有流量进入到 terminating 状态的 pod，然后 15s 后 prestop 脚本通知进程进行下线逻辑（也就是 ok 页面返回 halting 和 503 状态码），当网关继续请求到该 pod 就会认为 lb 出现异常，将唯一的 lb 标记为不健康，从而出现 502 异常。\n其他站点未出现该问题原因是：网关直接转发到 pod ip，摘掉的是出现异常的 pod ip 。而有问题的站点对接的只是一个公有云 LB ip。\n具体优化逻辑如下(如上图，针对单个pod):\n应用增加 connection filter，在应用进入优雅下线（被调用 http://127.0.0.1:${APP_PORT}/ok.htm?down=true）后，所有请求的 http 响应头中增加 connection:close（也就是使用短连接）； 因为 pod 处于 terminating 时，新连接不会进入该 pod； 参考代码如下： 健康检查接口，修改为去掉 503 异常，避免网关检测到 503 异常时，直接摘掉 lb ip。 参考代码如下： ","date":"2021-11-01T00:00:00Z","image":"https://jasonrd.github.io/blog/p/service-cause-failure/luca-bravo-alS7ewQ41M8-unsplash_hu0a3f1163de68d0b9471979ebf0ecf11e_32400_120x120_fill_q75_box_smart1.jpg","permalink":"https://jasonrd.github.io/blog/p/service-cause-failure/","title":"pod 销毁过程引起短时服务不可用"},{"content":"使用套路 informer 使用套路（其中略去一些细节，具体参考informers demo）:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 # 1. 创建 k8s client 对象： cfg, err = clientcmd.BuildConfigFromFlags(masterURL, kubeconfig) kube_client, err = kubernetes.NewForConfig(cfg) # 2. 创建资源 informer，以 POD 为例： factory = kubeinformers.NewSharedInformerFactory(client, 0) pod_informer = ctrl.factory.Core().V1().Pods() # 3. 注册回调函数，用来处理 Add、Update、Delete 事件： pod_informer.Informer().AddEventHandler(handler) handler 实现了 OnAdd、OnUpdate、OnDelete 三个接口的: type handler struct { queue: workqueue.NewRateLimitingQueue(workqueue.DefaultControllerRateLimiter()) OnAdd: func (obj interface{}) { ...... queue.Add(cache.MetaNamespaceKeyFunc(obj)) ...... }, OnUpdate: func(obj, obj interface{}) { ...... queue.Add(cache.MetaNamespaceKeyFunc(obj)) ...... }, OnDelete: func(obj interface{}) { ...... cache.DeletionHandlingMetaNamespaceKeyFunc(obj) ...... }, Task: func() { key, quit := c.queue.Get() defer c.queue.Done(key) ...... } } # 4. 启动 informer factory.Start(stop) informers 包中提供了工厂类，通过调用接口factory.Core().V1().Pods()创建 k8s pod informer 对象，其他 k8s 内置资源类同。informer 封装都在 k8s.io/client-go/pkg/informers 包中。\n执行逻辑 eventhandler 回调函数注册和执行过程 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 type sharedIndexInformer struct { // 带索引资源 cache indexer Indexer // 资源控制器，负责: // 1. 启动 reflector list\u0026amp;watch; // 2. Add、Update、Delete 事件发生时，通知 processor 执行订阅任务； // 3. 以及cache缓存更新，处理逻辑在 sharedIndexInformer.HandleDeltas； // controller 对象在执行 sharedIndexInformer.Run 函数时初始化 controller Controller // 负责事件触发时，执行订阅者的 OnAdd、OnUpdate、OnDelete 回调逻辑 processor *sharedProcessor // list\u0026amp;watch 资源变化，watch 通过 chunk 实现资源发生变更时进行推送 listerWatcher ListerWatcher // 关注的资源类型 objectType runtime.Object ....... } sharedIndexInformer 对象被创建后，执行 Run 函数启动事件监听：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 func (s *sharedIndexInformer) Run(stopCh \u0026lt;-chan struct{}) { defer utilruntime.HandleCrash() // 先入先出队列 fifo := NewDeltaFIFO(MetaNamespaceKeyFunc, s.indexer) cfg := \u0026amp;Config{ Queue: fifo, ListerWatcher: s.listerWatcher, ObjectType: s.objectType, FullResyncPeriod: s.resyncCheckPeriod, RetryOnError: false, ShouldResync: s.processor.shouldResync, // 消费 fifo 队列，在 controller.processLoop 函数中执行 Process: s.HandleDeltas, } // 减小锁粒度 func() { s.startedLock.Lock() defer s.startedLock.Unlock() // 初始化 controller s.controller = New(cfg) s.controller.(*controller).clock = s.clock s.started = true }() ...... // 首先启动事件处理器，监听事件通知 wg.StartWithChannel(processorStopCh, s.processor.run) ...... // 启动 controller s.controller.Run(stopCh) } 在了解 processor.run 函数逻辑前，先了解一下我们关注的事件回调函数是如何注册的。在调用 sharedIndexInformer.AddEventHandler(handler) 实际上就是创建一个 processListner 事件监听器，然后注册到 processor 中进行事件监听：\n1 2 3 4 5 6 7 8 9 func (s *sharedIndexInformer) AddEventHandlerWithResyncPeriod(handler ResourceEventHandler, resyncPeriod time.Duration) { ...... listener := newProcessListener(handler, resyncPeriod, determineResyncPeriod(resyncPeriod, s.resyncCheckPeriod), s.clock.Now(), initialBufferSize) ...... // 注册到 processor listeners 数组中 s.processor.addListener(listener) ...... } 明白了回调函数如何注册的，接下来看一下 processor.run 函数，了解一下什么时候执行回调函数：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 func (p *sharedProcessor) run(stopCh \u0026lt;-chan struct{}) { // 细粒度锁 func() { p.listenersLock.RLock() defer p.listenersLock.RUnlock() // 异步启动所有监听器，完成事件的消费 for _, listener := range p.listeners { p.wg.Start(listener.run) p.wg.Start(listener.pop) } p.listenersStarted = true }() \u0026lt;-stopCh p.listenersLock.RLock() defer p.listenersLock.RUnlock() for _, listener := range p.listeners { close(listener.addCh) // Tell .pop() to stop. .pop() will tell .run() to stop } p.wg.Wait() // Wait for all .pop() and .run() to stop } 上面函数中 listener.run 和 listener.pop 函数，两个函数配合完成收到事件，并执行回调：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 // 消费 processor 广播的事件，并通过 channel 内部转发 // 这个函数挺有意思，通过两个 channel 完成事件的缓存和通知。通过 addCh 无缓存 channel 通知。 // 首次函数启动或通知事件消费完成，nextCh 为 nil，第一个 case 会一直阻塞到新事件过来 // 如果事件通知速度大于消费速度，会将事件缓存在 pendingNotifications 先入先出队列，然后异步消费 func (p *processorListener) pop() { defer utilruntime.HandleCrash() defer close(p.nextCh) // Tell .run() to stop var nextCh chan\u0026lt;- interface{} var notification interface{} for { select { // 1 listener 内部广播事件 case nextCh \u0026lt;- notification: // Notification dispatched var ok bool // 2 消费缓存事件 notification, ok = p.pendingNotifications.ReadOne() if !ok { // Nothing to pop nextCh = nil // Disable this select case } // 3 接收 processor 通知事件 case notificationToAdd, ok := \u0026lt;-p.addCh: if !ok { return } // 4 notification 为 nil 说明没有未消费的事件，之间内部广播 if notification == nil { notification = notificationToAdd nextCh = p.nextCh // 5 来不及消费，先暂存 } else { p.pendingNotifications.WriteOne(notificationToAdd) } } } } 下面使用了 wait.Until 函数，在闭包执行完成后，间隔1分钟再次执行：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 func (p *processorListener) run() { stopCh := make(chan struct{}) wait.Until(func() { // this gives us a few quick retries before a long pause and then a few more quick retries err := wait.ExponentialBackoff(retry.DefaultRetry, func() (bool, error) { // 我们关注回调真正的执行地方 // p.nextCh 无缓冲 channel 由上面 pop 函数传入事件 for next := range p.nextCh { switch notification := next.(type) { case updateNotification: p.handler.OnUpdate(notification.oldObj, notification.newObj) case addNotification: p.handler.OnAdd(notification.newObj) case deleteNotification: p.handler.OnDelete(notification.oldObj) default: utilruntime.HandleError(fmt.Errorf(\u0026#34;unrecognized notification: %#v\u0026#34;, next)) } } // the only way to get here is if the p.nextCh is empty and closed return true, nil }) // the only way to get here is if the p.nextCh is empty and closed if err == nil { close(stopCh) } }, 1*time.Minute, stopCh) } 更新 indexer 缓存和 deltas evnet 事件处理 HandleDeltas 函数实现了 fifo 队列消费逻辑，分别对各事件类型分别操作 indexer 索引缓存和通知 processor 向订阅者分发事件。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 // 处理 fifo 弹出事件 func (s *sharedIndexInformer) HandleDeltas(obj interface{}) error { ....... for _, d := range obj.(Deltas) { switch d.Type { case Sync, Added, Updated: ...... if old, exists, err := s.indexer.Get(d.Object); err == nil \u0026amp;\u0026amp; exists { if err := s.indexer.Update(d.Object); err != nil { return err } // 通知 processor 向订阅者广播事件 s.processor.distribute(updateNotification{oldObj: old, newObj: d.Object}, isSync) } else { if err := s.indexer.Add(d.Object); err != nil { return err } s.processor.distribute(addNotification{newObj: d.Object}, isSync) } case Deleted: if err := s.indexer.Delete(d.Object); err != nil { return err } s.processor.distribute(deleteNotification{oldObj: d.Object}, false) } } return nil } HandleDeltas 函数的执行时在 controller 启动时，每秒钟调用 controller.processLoop 消费 DeltaFIFO 中事件。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 func (c *controller) Run(stopCh \u0026lt;-chan struct{}) { ...... r := NewReflector( c.config.ListerWatcher, c.config.ObjectType, c.config.Queue, c.config.FullResyncPeriod, ) ........ wg.StartWithChannel(stopCh, r.Run) wait.Until(c.processLoop, time.Second, stopCh) } 消费 listwatch 写入 DeltaFIFO 的事件。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 func (c *controller) processLoop() { for { // 弹出第一个事件，并消费 // c.config.Process = sharedIndexInformer.HandleDeltas obj, err := c.config.Queue.Pop(PopProcessFunc(c.config.Process)) if err != nil { if err == FIFOClosedError { return } if c.config.RetryOnError { // This is the safe way to re-enqueue. c.config.Queue.AddIfNotPresent(obj) } } } } list watch 资源 Event 事件和 deltas event 生产 1 2 3 4 5 6 7 8 9 10 // Run starts a watch and handles watch events. Will restart the watch if it is closed. // Run will exit when stopCh is closed. func (r *Reflector) Run(stopCh \u0026lt;-chan struct{}) { klog.V(3).Infof(\u0026#34;Starting reflector %v (%s) from %s\u0026#34;, r.expectedType, r.resyncPeriod, r.name) wait.Until(func() { if err := r.ListAndWatch(stopCh); err != nil { utilruntime.HandleError(err) } }, r.period, stopCh) } controller 启动 reflector，间隔 r.period 时间，通过 reflector.ListAndWatch 函数获取最新事件。\nreflector.ListAndWatch 代码较长，就不在此列出，简单讲一下执行逻辑：\n首先通过 List 方法，一次性拉取所有资源，并获取最新 resourceVersion； 根据需要，启动定时异步 List 协程； watch 资源直到 informer 退出。调用 watchHandler 获取资源变更（watch 使用 http 协议 chunk 机制完成）。对 response 反序列化成内部资源对象 event.Object，根据 event.Type 执行 r.store 更新、删除、新增（其中，r.store 为 DeltaFIFO）； 主要对象数据结构 Controller 1 2 3 4 5 6 7 // Controller is a generic controller framework. type controller struct { config Config reflector *Reflector reflectorMutex sync.RWMutex clock clock.Clock } reflector 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 // Reflector watches a specified resource and causes all changes to be reflected in the given store. type Reflector struct { // name identifies this reflector. By default it will be a file:line if possible. name string // metrics tracks basic metric information about the reflector metrics *reflectorMetrics // The type of object we expect to place in the store. expectedType reflect.Type // The destination to sync up with the watch source store Store // listerWatcher is used to perform lists and watches. listerWatcher ListerWatcher // period controls timing between one watch ending and // the beginning of the next one. period time.Duration resyncPeriod time.Duration ShouldResync func() bool // clock allows tests to manipulate time clock clock.Clock // lastSyncResourceVersion is the resource version token last // observed when doing a sync with the underlying store // it is thread safe, but not synchronized with the underlying store lastSyncResourceVersion string // lastSyncResourceVersionMutex guards read/write access to lastSyncResourceVersion lastSyncResourceVersionMutex sync.RWMutex } sharedProcessor 1 2 3 4 5 6 7 8 type sharedProcessor struct { listenersStarted bool listenersLock sync.RWMutex listeners []*processorListener syncingListeners []*processorListener clock clock.Clock wg wait.Group } processListner 1 2 3 4 5 6 7 8 9 10 11 12 13 14 type processorListener struct { nextCh chan interface{} addCh chan interface{} handler ResourceEventHandler // pendingNotifications is an unbounded ring buffer that holds all notifications not yet distributed. // There is one per listener, but a failing/stalled listener will have infinite pendingNotifications // added until we OOM. // TODO: This is no worse than before, since reflectors were backed by unbounded DeltaFIFOs, but // we should try to do something better. // RingGrowing 是一个动态增长的循环队列 pendingNotifications buffer.RingGrowing } 疑问 为什么需要 waitForCacheSync 1 2 3 4 5 func (f *DeltaFIFO) HasSynced() bool { f.lock.Lock() defer f.lock.Unlock() return f.populated \u0026amp;\u0026amp; f.initialPopulationCount == 0 } waitForCacheSync 最终回调用 DeltaFIFO.HasSynced 函数来确定当前 cache 缓存全部写入 workqueue 中。 在 调用 DeltaFIFO.Add，以及 reflector list 完成资源，通过调用 replace 写入 deltafifo 时会 f.populated 置为 true。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 func (f *DeltaFIFO) Add(obj interface{}) error { f.lock.Lock() defer f.lock.Unlock() f.populated = true ...... } func (r *Reflector) ListAndWatch(stopCh \u0026lt;-chan struct{}) error { ...... list, err := r.listerWatcher.List(options) ...... items, err := meta.ExtractList(list) ...... if err := r.syncWith(items, resourceVersion); err != nil { return fmt.Errorf(\u0026#34;%s: Unable to sync list result: %v\u0026#34;, r.name, err) } ...... } // syncWith replaces the store\u0026#39;s items with the given list. func (r *Reflector) syncWith(items []runtime.Object, resourceVersion string) error { found := make([]interface{}, 0, len(items)) for _, item := range items { found = append(found, item) } return r.store.Replace(found, resourceVersion) } func (f *FIFO) Replace(list []interface{}, resourceVersion string) error { ...... if !f.populated { f.populated = true f.initialPopulationCount = len(items) } ...... } 而 f.initialPopulationCount 只有在 list 的所有资源都被 pop 后，才会被重新赋值为0：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 func (f *FIFO) Pop(process PopProcessFunc) (interface{}, error) { f.lock.Lock() defer f.lock.Unlock() for { // 并发验证 for len(f.queue) == 0 { // When the queue is empty, invocation of Pop() is blocked until new item is enqueued. // When Close() is called, the f.closed is set and the condition is broadcasted. // Which causes this loop to continue and return from the Pop(). if f.IsClosed() { return nil, FIFOClosedError } f.cond.Wait() } id := f.queue[0] f.queue = f.queue[1:] if f.initialPopulationCount \u0026gt; 0 { f.initialPopulationCount-- } ....... } 所以，根据源码分析，调用 waitForCacheSync 是等待第一次 list 完全部资源后，并且 list 的资源全部写入到 workqueue 后再启动对应的 work，处理事件。这样，降低了 list 大量资源时高并发资源处理资源问题。\n为什么使用 workqueue ？ 在一些 informer demo，以及 operator framework 的代码里面，都能看到 Add、Update、Delete 事件都要先写到 workqueue 中，然后再异步消费。不知道有没有思考过，为什么要加入 workqueue，而不是在事件函数里直接起一个协程来处理事件?\n首先，考虑一个场景 Add -\u0026gt; Deleted -\u0026gt; Add，如果并发处理同一个资源，会不会存在问题？针对顺序依赖问题，最简单方式就是使用队列。那并发执行问题呢？来看一下 worqueue 底层代码是怎么实现的：\nk8s.io/client-go/util/workqueue 包有多个文件：\n1 2 3 4 5 default_rate_limiters.go // 限速器，包含两种，全局限速（BucketRateLimiter）和针对 item 限速（ItemExponentialFailureRateLimiter） delaying_queue.go // 延迟队列，支持延迟添加，队列使用的 queue parallelizer.go // 并发控制器， queue.go // 队列，会对添加的 key 去重，同一个 key 同时只会处理一次 rate_limitting_queue.go // 限速队列包装器，组合延迟队列、 在使用 informer 时，经常看到 workqueue.NewRateLimitingQueue(workqueue.DefaultControllerRateLimiter()) 生成一个队列，缓存事件对象的 metaNamespaceKey。\n1 2 3 4 5 6 func NewRateLimitingQueue(rateLimiter RateLimiter) RateLimitingInterface { return \u0026amp;rateLimitingType{ DelayingInterface: NewDelayingQueue(), rateLimiter: rateLimiter, } } 1 2 3 4 5 6 7 8 9 10 11 12 func DefaultControllerRateLimiter() RateLimiter { return NewMaxOfRateLimiter( // 针对单个 item 限制处理间隔，最小为 5 毫秒，重新入队一次指数增长，最大为 1000 秒延迟 // backoff := float64(r.baseDelay.Nanoseconds()) * math.Pow(2, float64(exp)) exp 是重新入队的次数 // if backoff \u0026gt; math.MaxInt64 { // return r.maxDelay // } NewItemExponentialFailureRateLimiter(5*time.Millisecond, 1000*time.Second), // 使用令牌桶限流算法，限制流速为每秒10个，桶大小为 100 能够应对100的突发 \u0026amp;BucketRateLimiter{Limiter: rate.NewLimiter(rate.Limit(10), 100)}, ) } delayQueue 使用一个定时器，和带缓冲 channel（1000）延迟添加需要重新入队的 item，队列定义在 queue.go 中。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 // Type is a work queue (see the package comment). type Type struct { // 通过 slice 实现有序队列，在 queue 中的 item 必定在 dirty 中也存在 queue []t // Add 队列前需要先在这个集合中添加，确保队列中 item 的唯一性 // processing 中的 item 会临时存在这个里面，调用 Type.Done 才 append 到 queue 中 dirty set // 调用 Get 的 item 保存在这个集合中，同时在 queue 和 dirty 中删除 processing set // 队列使用信号量保证访问安全和通知等待消费的 worker cond *sync.Cond } 下面了解一下，入队和出队的代码：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 func (q *Type) Get() (item interface{}, shutdown bool) { q.cond.L.Lock() defer q.cond.L.Unlock() for len(q.queue) == 0 \u0026amp;\u0026amp; !q.shuttingDown { q.cond.Wait() } if len(q.queue) == 0 { // We must be shutting down. return nil, true } // 代码编译时会有临时寄存器保存中间值 // 类似 temp=a, a=b, b=temp item, q.queue = q.queue[0], q.queue[1:] q.metrics.get(item) q.processing.insert(item) q.dirty.delete(item) return item, false } 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 // Add marks item as needing processing. func (q *Type) Add(item interface{}) { q.cond.L.Lock() defer q.cond.L.Unlock() if q.shuttingDown { return } if q.dirty.has(item) { return } q.metrics.add(item) q.dirty.insert(item) if q.processing.has(item) { return } q.queue = append(q.queue, item) q.cond.Signal() } 所以，综上使用 workqueue 保证了事件资源的唯一性。另外，消费失败的 item 可以通过调用 ratelimit.AddAfter 和 ratelimit.AddRateLimited 避免了 hotloop 的问题。AddRateLimited 最终回调用 ItemFastSlowRateLimiter.When 函数确保 failure item 的延迟指数增长。\n","date":"2021-10-15T00:00:00Z","image":"https://jasonrd.github.io/blog/p/client-go-informer/informer-arch_hu99f14accc96104069d2695ab8dd85549_718091_120x120_fill_box_smart1_3.png","permalink":"https://jasonrd.github.io/blog/p/client-go-informer/","title":"informer 内部实现探究"},{"content":"问题背景 我们在 kubernetes 生产环境上了 GPU 虚拟化后，对原生的 kube-scheduler 进行了扩展，支持 GPU 虚拟化相关调度算法。在 GPU 扩展调度组件上线后，部署推理 业务容器时，出现了 pod 一直处于 Pending 状态。查看 Pending Pod 的 event 日志，提示 pod 资源不足而导致 pod 调度失败。\n查看 event 日志，可以看出是节点 cpu 资源不足导致。但是，我们通过 describe gpu node 返回结果可以看到 GPU、CPU、memory 等资源都非常充足。\n并且，非 gpu 节点并没有发现类似的问题。\n问题定位 为了定位是否是引入 GPU 扩展调度组件导致的，我们修改了 gpu 容器 request 资源，去掉虚拟化 gpu 的配额并且强制调度到 218.27 这台node，容器仍然 Pending。\n到现在仍然不能排除是增加 GPU 扩展调度组件引起的问题，虽然这个组件在上线前在线下做了较久时间的验证都没有出现该问题。我们在 playbox 环境中环境尝试复现该问题，经过多次同样问题出现了。为了定位到具体原因，我们将 kube-scheduler 的日志级别调到最高，希望通过观察日志能够定位到问题的根源。\n我们发现扩容时存在某些 pod 会出现两次调度的情况，pod 虽然最终调度成功并且处于 running 状态，但是仍然出现过调度失败的日志：\n1 I0624 13:11:39.054874 7225 event.go:209] Event(v1.ObjectReference{Kind:\u0026#34;Pod\u0026#34;, Namespace:\u0026#34;gpu\u0026#34;, Name:\u0026#34;gpu-work-85bb88797b-7zf85\u0026#34;, UID:\u0026#34;22f6a277-2812-49af-b12f-6759e47920d4\u0026#34;, APIVersion:\u0026#34;v1\u0026#34;, ResourceVersion:\u0026#34;11045409\u0026#34;, FieldPath:\u0026#34;\u0026#34;}): t ype: \u0026#39;Warning\u0026#39; reason: \u0026#39;FailedScheduling\u0026#39; 0/5 nodes are available: 4 Insufficient xxxx-core, 4 Insufficient xxx-memory. 这给我们带来了以下疑问：\n同一个 pod 为什么会出现两次调度？ 为什么 pod 调度成功，也出现了调度失败的事件呢？ 其他 pod 调度失败是否和 pod 出现两次调度有关系？ 原因排查 一个 pod 为什么会出现两次调度？ 我们只使用了 kube-scheduler 的 predicate extender，pod 在 k8s scheduler 默认的 predicate 流程执行完成后，会回调 GPU 扩展调度组件完成 GPU 辅助调度，最后优选最优的 node 节点。pod 在回调时，GPU 扩展调度组件会将分配 gpu 设备号和调度的 node 等信息 patch 到 pod annotations 中：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 func (gpuFilter *GPUFilter) deviceFilter( ...... annotationMap := make(map[string]string) for k, v := range newPod.Annotations { if strings.Contains(k, util.GPUAssigned) || strings.Contains(k, util.PredicateTimeAnnotation) || strings.Contains(k, util.PredicateGPUIndexPrefix) || strings.Contains(k, util.PredicateNode) { annotationMap[k] = v } } err := gpuFilter.patchPodWithAnnotations(newPod, annotationMap) if err != nil { failedNodesMap[node.Name] = \u0026#34;update pod annotation failed\u0026#34; continue } .... } 然后，kube-scheduler watch 到 pod update 事件后，会将 pod 重新加入到调度队列：\n1 2 3 4 5 6 7 8 9 func (sched *Scheduler) updatePodInSchedulingQueue(oldObj, newObj interface{}) { pod := newObj.(*v1.Pod) if sched.skipPodUpdate(pod) { return } if err := sched.config.SchedulingQueue.Update(oldObj.(*v1.Pod), pod); err != nil { utilruntime.HandleError(fmt.Errorf(\u0026#34;unable to update %T: %v\u0026#34;, newObj, err)) } } 虽然上么有 skipPodUpdate 这个逻辑，但是实际上更新 annotation 后这一步被跳过了，具体后面再讲。\n为什么 pod 调度成功，也出现了调度失败的事件呢？ 更新后的 podA 重新进入调度队列后，相当于 podA 经过了两次调度循环 A 和 B。由于调度队列的优先级算法，在没有优先级更高的 pod 被创建的情况下，A、B 两个循环是相邻的。\n在 node 资源充足的情况下，一个调度循环大概流程：\nscheduling queue pop \u0026mdash;\u0026gt; schedule \u0026mdash;-\u0026gt; predicate \u0026mdash;-\u0026gt; call predicate extender \u0026mdash;\u0026ndash;\u0026gt; priority \u0026mdash;\u0026mdash;\u0026gt; asynchronous binding\n上图为调度的基本流程。调度器在回调 GPU 扩展调度组件时 pod 属性被修改，update 事件进入 kube-scheduler pod 被重新加入调度队列。pod 在第二次调度过程，会再次回调 GPU 扩展调度组件，我们查看 GPU 扩展调度组件日志，在第二次调度时 response 返回的 filteredNode 为空：\nI0628 03:46:20.957317 1 routes.go:81] GPUQuotaPredicate: extenderFilterResult = {\u0026ldquo;Nodes\u0026rdquo;:null,\u0026ldquo;NodeNames\u0026rdquo;:null,\u0026ldquo;FailedNodes\u0026rdquo;:null,\u0026ldquo;Error\u0026rdquo;:\u0026quot;\u0026quot;}\n从而 predicate 过程预选可用 node 集合为空，从而报出 0/5 nodes are available: 4 Insufficient xxx-core, 4 Insufficient xxx-memory 错误日志。我们 GPU 扩展调度组件代码中，如果存在 gpu annotations 时会直接返回一个空列表：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 //deviceFilter will choose one and only one node fullfil the request, //so it should always be the last filter of gpuFilter func (gpuFilter *GPUFilter) deviceFilter( pod *corev1.Pod, nodes []corev1.Node) ([]corev1.Node, schedulerapi.FailedNodesMap, error) { // #lizard forgives var ( filteredNodes = make([]corev1.Node, 0) failedNodesMap = make(schedulerapi.FailedNodesMap) nodeInfoList []*device.NodeInfo success bool sorter = device.NodeInfoSort( device.ByAllocatableCores, device.ByAllocatableMemory, device.ByID) ) for k := range pod.Annotations { if strings.Contains(k, util.GPUAssigned) || strings.Contains(k, util.PredicateTimeAnnotation) || strings.Contains(k, util.PredicateGPUIndexPrefix) { return filteredNodes, failedNodesMap, nil } } ...... } 资源充足调度失败原因 通过上面两小节的分析，我们大致了解了场景触发前提：\nGPU 扩展调度组件在 kube-scheduler 回调时（调度循环期间），会同时 patch pod annotations 属性； pod 第二次调度循环， GPU 扩展调度组件 response 返回空调度队列； 然后，我们对 kube-scheduler 代码进行阅读和调试分析，发现 kube-scheduler 存在一个内存泄漏的bug。\n我们知道，通常情况下 predicate 阶段判断调度失败，kube-scheduler 会进行以下操作：\n将 pod 入队 unschedulableQ 从而在下一个调度循环进行调度； 进入 preempt 流程，如果有可抢占的 node ，则标记 pod nominatedName 为被抢占的 node; 另外，nominatedPodMap 中记录的 pod 只在调度成功后 assumed 阶段会被清理。而 assumed 只有在调度循环成功，进入到 bind 阶段前会执行。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 // scheduler.go func (sched *Scheduler) scheduleOne() { ... ... // assume modifies `assumedPod` by setting NodeName=scheduleResult.SuggestedHost err = sched.assume(assumedPod, scheduleResult.SuggestedHost) if err != nil { klog.Errorf(\u0026#34;error assuming pod: %v\u0026#34;, err) metrics.PodScheduleErrors.Inc() return } // bind the pod to its host asynchronously (we can do this b/c of the assumption step above). go func() { // Bind volumes first before Pod ... ... err := sched.bind(assumedPod, \u0026amp;v1.Binding{ ObjectMeta: metav1.ObjectMeta{Namespace: assumedPod.Namespace, Name: assumedPod.Name, UID: assumedPod.UID}, Target: v1.ObjectReference{ Kind: \u0026#34;Node\u0026#34;, Name: scheduleResult.SuggestedHost, }, }) ... ... } ... ... } 如果，pod 调度失败但是并没有入队 unschedulableQ，则 nominated pod map 就存在资源泄漏的问题。\n我们看一下 unschedulableQ 入队代码：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 ... ... defer runtime.HandleCrash() podID := types.NamespacedName{ Namespace: pod.Namespace, Name: pod.Name, } // When pod priority is enabled, we would like to place an unschedulable // pod in the unschedulable queue. This ensures that if the pod is nominated // to run on a node, scheduler takes the pod into account when running // predicates for the node. if !util.PodPriorityEnabled() { if !backoff.TryBackoffAndWait(podID, stopEverything) { klog.Warningf(\u0026#34;Request for pod %v already in flight, abandoning\u0026#34;, podID) return } } // Get the pod again; it may have changed/been scheduled already. getBackoff := initialGetBackoff for { pod, err := client.CoreV1().Pods(podID.Namespace).Get(podID.Name, metav1.GetOptions{}) if err == nil { if len(pod.Spec.NodeName) == 0 { podQueue.AddUnschedulableIfNotPresent(pod, podSchedulingCycle) } break } if errors.IsNotFound(err) { klog.Warningf(\u0026#34;A pod %v no longer exists\u0026#34;, podID) return } klog.Errorf(\u0026#34;Error getting pod %v for retry: %v; retrying...\u0026#34;, podID, err) if getBackoff = getBackoff * 2; getBackoff \u0026gt; maximalGetBackoff { getBackoff = maximalGetBackoff } time.Sleep(getBackoff) } ... ... 图中 a 和 b 分别表示两次调度循环，\na.3 (asummed) 之前 pod 更新事件进入，触发 pod 的二次调度；\npod 的第二次调度 b predicate 阶段返回 fitsError 错误，从而触发 b.3 调度入队 unschedulableQ，但是这时 pod 在 a 调度循环中完成 node bind 操作，导致 (pod.Spec.NodeName != \u0026quot;\u0026quot;)，从而 b.3 没有入队 unschedulableQ；\nb 调度循环中 在 b.6 中将 pod 加入 nominated pod map，因为 pod 已经绑定了node 所以后续不会再进行调度，pod 会一直在 nominatedPodMap 中；\n接下来，解释以下 nominatedPodMap 的作用。nominatedPodMap 数据结构：\n1 2 3 4 5 6 7 8 9 10 11 12 13 // nominatedPodMap is a structure that stores pods nominated to run on nodes. // It exists because nominatedNodeName of pod objects stored in the structure // may be different than what scheduler has here. We should be able to find pods // by their UID and update/delete them. type nominatedPodMap struct { // nominatedPods is a map keyed by a node name and the value is a list of // pods which are nominated to run on the node. These are pods which can be in // the activeQ or unschedulableQ. nominatedPods map[string][]*v1.Pod // nominatedPodToNode is map keyed by a Pod UID to the node name where it is // nominated. nominatedPodToNode map[ktypes.UID]string } nominatedPodMap 记录了 node 关联的 pod，包括 running 状态和候选 pod。在 pod 调度失败时，会进行 node 抢占，为了避免被抢占的 pod 重新夺回 node 资源，所以，在每次 predicate 阶段会将 nominatedPodMap 中 node 关联的 pod 占用的资源也考虑进去。\n1 2 3 4 5 6 7 8 9 10 11 12 func podFitsOnNode() { ...... for i := 0; i \u0026lt; 2; i++ { metaToUse := meta nodeInfoToUse := info if i == 0 { podsAdded, metaToUse, nodeInfoToUse = addNominatedPods(pod, meta, info, queue) } else if !podsAdded || len(failedPredicates) != 0 { break } ..... } 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 // addNominatedPods adds pods with equal or greater priority which are nominated // to run on the node given in nodeInfo to meta and nodeInfo. It returns 1) whether // any pod was found, 2) augmented meta data, 3) augmented nodeInfo. func addNominatedPods(pod *v1.Pod, meta predicates.PredicateMetadata, nodeInfo *schedulernodeinfo.NodeInfo, queue internalqueue.SchedulingQueue) (bool, predicates.PredicateMetadata, *schedulernodeinfo.NodeInfo) { if queue == nil || nodeInfo == nil || nodeInfo.Node() == nil { // This may happen only in tests. return false, meta, nodeInfo } nominatedPods := queue.NominatedPodsForNode(nodeInfo.Node().Name) if nominatedPods == nil || len(nominatedPods) == 0 { return false, meta, nodeInfo } var metaOut predicates.PredicateMetadata if meta != nil { metaOut = meta.ShallowCopy() } nodeInfoOut := nodeInfo.Clone() for _, p := range nominatedPods { if util.GetPodPriority(p) \u0026gt;= util.GetPodPriority(pod) \u0026amp;\u0026amp; p.UID != pod.UID { nodeInfoOut.AddPod(p) if metaOut != nil { metaOut.AddPod(p, nodeInfoOut) } } } return true, metaOut, nodeInfoOut } 至此，我们也分析清楚了资源泄露的问题。\n查看 kubernetes 代码，在 1.18 版本中已经对次问题进行了优化，在pr #86230 中进行了修复。修复逻辑也比较简单：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 func (sched *Scheduler) scheduleOne(ctx context.Context) { fwk := sched.Framework podInfo := sched.NextPod() // pod could be nil when schedulerQueue is closed if podInfo == nil || podInfo.Pod == nil { return } pod := podInfo.Pod if sched.skipPodSchedule(pod) { return } ... } 在 Pod 进入调度循环前，再次进行 skipPodSchedule 判断，这个函数我们上面提到过，在 scheduler 捕获到 pod Update 事件后，会执行这个函数：\n1 2 3 4 5 6 7 8 9 10 // scheduler/eventhandlers.go func (sched *Scheduler) updatePodInSchedulingQueue(oldObj, newObj interface{}) { pod := newObj.(*v1.Pod) if sched.skipPodUpdate(pod) { return } if err := sched.config.SchedulingQueue.Update(oldObj.(*v1.Pod), pod); err != nil { utilruntime.HandleError(fmt.Errorf(\u0026#34;unable to update %T: %v\u0026#34;, newObj, err)) } } skipPodSchedule函数逻辑:\n1 2 3 4 5 // skipPodUpdate checks whether the specified pod update should be ignored. // This function will return true if // - The pod has already been assumed, AND // - The pod has only its ResourceVersion, Spec.NodeName and/or Annotations // updated. pod 被 assumed 是 pod 成功选择要调度的 Node 后，加入调度器中 assumed cache 中，后面就可以进行异步的 bind 操作。assumed cache 会在 bind 结束 30s 后进行清理。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 // assume signals to the cache that a pod is already in the cache, so that binding can be asynchronous. // assume modifies `assumed`. func (sched *Scheduler) assume(assumed *v1.Pod, host string) error { // Optimistically assume that the binding will succeed and send it to apiserver // in the background. // If the binding fails, scheduler will release resources allocated to assumed pod // immediately. assumed.Spec.NodeName = host if err := sched.config.SchedulerCache.AssumePod(assumed); err != nil { klog.Errorf(\u0026#34;scheduler cache AssumePod failed: %v\u0026#34;, err) ... ... } ... ... } 上面说过我们在 update pod annotations 是在 predicate 阶段，会有一定概率 update 早于 assumed 操作，导致再次加入调度循环。\n修复方案 了解了整个问题的原委，目前有两种修复方案：\nk8s 组件方面： 升级 k8s 到 1.18+ 版本； 1.14 和 1.18 功能特性变化较大存在较多不确定性，升级成本较高 GPU 扩展调度组件层面进行修改： 在第二次调度时，判断已经调度时 GPU 扩展调度组件并不是返回空 node 列表，而是返回 error 错误； 能够避免问题 bug 的触发，但是会存在一定的 schedulefailed event 日志； 最后，经过总和考虑，我们采用“方案2”进行修复。\n相关参考 https://github.com/kubernetes/kubernetes/pull/86230 https://github.com/kubernetes/kubernetes/issues/86942 ","date":"2021-09-08T00:00:00Z","image":"https://jasonrd.github.io/blog/p/pod-pending-with-enough-resources/sleep-boy_hud07293a026e0ecb8feb93a2c733978af_122481_120x120_fill_q75_box_smart1.jpg","permalink":"https://jasonrd.github.io/blog/p/pod-pending-with-enough-resources/","title":"集群资源充足情况 Pod 出现 schedulfailed"},{"content":"问题描述 AI 应用在迁移到 k8s 部署后发现应用在启动阶段耗时长，且非常容易失败，查看日志发现应用包找不到 GPU 设备问题。\n问题分析 排查发现出现问题主要在开启了 kubelet static 节点，频繁的出现业务发布失败的情况，报错信息：\n我们进行了一些研究，发现 kubernetes 社区（参考2）和 nvidia-docker （参考3）有相关 issue 讨论，在讨论中 nvidia 员工 klueska 给出了具体原因：\n究其根本，是 nvidia-plugin 在容器启动前会通过 nividia-docker runtime 将 gpu device mount 到容器内部，这些 mount 信息对 docker 是不可见的。我们把 kubernetes 的内核绑定参数 CPUManager=static 开启后，kubelet CPUManager 会定期 update 容器 cpuset cgroup 配置，这时上面 nvidia mount 的 device 被清除，导致无法分配 gpu 。nvidia-plugin 通过在 kubelet 调用 Allocate 接口时返回 device 列表，修复了该bug。\nsummarize：已定位到原因，nvidia 的 k8s 插件和 k8s 绑核功能不兼容导致，需要升级 nvidia 插件。\n验证 我们在线下搭建了 gpu 环境，分别使用 10.57.33.30、10.57.33.31 两台机器做对比测试。首先，我们替换 Nvidia daemonset 容器镜像为修复后的版本 ps/nvidia/nvidia/k8s-device-plugin:1.0.0-beta6，并且修改 daemonset 的升级策略为 OnDelete，这样删除 33.30 机器上 Nvidia pod ，新建的 pod 使用的镜像为升级后的版本。\n有问题插件验证 我们测试 33.31 上的 ai-face-gen 应用，进入 pod 内部，执行命令 nvidia-smi ，会报 nvm unknown error：\n我们安装参考3中的相关描述，docker inspect 容器，查看一下挂载的 devices 列表：\n会发现列表为空。然后，我们测试升级后应用报错场景。删除 pod af-55666f8bf8-tffxg，容器重建后的 pod 出现启动失败：\n查看应用日志，报错和线上场景相同：\n升级新版插件验证 我们首先将 33.30 机器的 nvidia-plugin 升级到最新版本（其中1.11 版本为内部的版本号，1.0.0-beta6 为社区修复gpu 问题的最新版本）：\n然后和上面同样的操作流程，我们首先进入容器 af-55666f8bf8-7z4ld 内部执行命令 nvidia-smi：\n可以发现命令执行正常。然后 同样使用 docker inspect 命令查看容器挂载 device 列表：\ndevice 列表中挂载了 gpu 显卡和相关驱动。最后，我们删除一下容器 af-55666f8bf8-7z4ld，验证一下容器发布是否会出现失败的情况：\npod 启动正常，执行 nvidia-smi 命令正常：\n参考 Nvidia GPU如何在Kubernetes 里工作 Updating cpu-manager-policy=static causes NVML unknown error NVIDIA NVML Driver/library version mismatch 解决Driver/library version mismatch 升级 nvidia 驱动 https://developer.download.nvidia.cn/compute/cuda/repos/rhel7/x86_64/ https://developer.download.nvidia.cn/compute/cuda/repos/rhel7/x86_64/cuda-rhel7.repo ","date":"2021-06-05T00:00:00Z","image":"https://jasonrd.github.io/blog/p/gpu-start-failure/the-creative-exchange-d2zvqp3fpro-unsplash_huf941de4769045cdfa8c9ee7036519a2a_35369_120x120_fill_q75_box_smart1.jpg","permalink":"https://jasonrd.github.io/blog/p/gpu-start-failure/","title":"kubelet 开启 static 引发 gpu 容器部署异常"}]