[{"content":"问题现象 最近，在工作想要使用 kubectl exec 进入容器排查问题，结果返回下面异常：\n排查过程 我们知道 kubectl exec 的执行链路是 client -\u0026gt; kube-apiserver -\u0026gt; kubelet -\u0026gt; docker。\n登录 Kubelet 宿主机查看 kubelet 错误日志，发现有相同的报错日志，这说明是 kubelet 和 docker 之间链路又问题。通过 kubelet 日志中不能定为到问题具体原因。然后，我们试图通过抓包，希望在数据包中能发现一些线索。\n在抓包数据结果中我们发现关键字为 exec 的请求，该会话的目的地址为 A:20880, http header 中 Host 为 B:10250 （也就是是物理机上 kubelet 的 httpserver 地址）。我们查询 A 这个IP，发现是业务应用的容器 IP。\n这就比较奇怪了，正常 apiserver 发送 exec 请求为什么转发到了容器的 20880 端口。并且数据包中包含 kubectl (\u0026ldquo;User-Agent: kubectl\u0026rdquo;) http header。难道 kubectl exec 请求发送到 docker 的请求（xxxx/exec/token）被转发到了容器。通过再次尝试执行 kubectl exec 并抓包，发现执行命令和发送到 20880 端口请求匹配，这验证了我们的猜测。\n到此就把问题范围缩小到宿主机网络上，我们知道 kube-proxy 会通过 ipvs 或 iptables 对创建的 nodeport 或 service vip 的请求进行拦截和转发。我们查看 conntrack 请求记录：\n发现一条请求 127.0.0.1:33589 的记录，并且转发到的地址 A:20880 也和我们抓包的结果匹配。然后查看 33589 端口，发现该端口就是被 kubelet 占用。然后，我们查询 service，发现 33589 端口同时是 B 容器的应用 service 的 nodeport。到此问题根本原因定位到了，nodeport 端口和 kubelet 启动的转发端口冲突了，导致发送 exec 请求转发到了应用容器的 20880 端口（dubbo端口）。\n继续深挖 事情到此并没有结束，上面我们只是定位到了具体问题原因。其实还存在两个问题：\n对 kubectl exec 的执行过程还没没挖透； 如何避免该问题？ kubectl exec 的执行过程 问题没有快速定位，主要原因还是对 kubectl exec 执行流程不熟。下面来了解一下 kubectl 是怎么执行的。\n本文基于 1.14.6 源码进行研究。\n首先，简单了解一下 kubelet 架构：\nkubelet 中有上面几个部分：container manager、dockershim、http server、streaming server。kubelet 早期直接调用 docker api 管理容器，后来为了适配更多的 runtime 抽象出了一个接入层 cri。同时，为了兼容 docker 的 API，kubelet 代码中实现了这个叫 dockershim 的部分。这样就对上层屏蔽了底层 runtime。http server 通常使用 10250 对外提供 API 服务。streaming server 是需要和容器进行交互时的一个代理服务。\n在默认情况下，用户执行 kubectl exec 简化流程如下：\nterminal 中键入 kubectl exec xxx 指令，kubectl 发送请求到 apiserver https://apiserver/api/v1/namespaces/{ns}/pods/{pod}/exec?command=bash\u0026amp;container=dragon-claw\u0026amp;stdin=true\u0026amp;stdout=true\u0026amp;tty=true； apiserver 接到请求后，将请求转发到 kubelet， node:10250/api/v1/exec/{ns}/{podid}/{container}。kubelet httpserver 接收到请求后： 首先，向 dockershim 发起 getExec 请求，返回一个流地址 url （exec/{token}）； 然后，kubelet 请求 exec/xxxx url 到 streaming server，streaming server 接收到请求后，response upgrade 将连接升级成为 spdy 或 ws 连接； kubelet 收到 upgrade reponse 后，将该 reponse 直接返回给 apiserver，到此 apiserver -\u0026gt; kubelet -\u0026gt; streaming server -\u0026gt; docker 之间整个通道建立完成； 到此，用户可以在 terminal 中键入命令在容器中执行； 其中， streaming server 是 kubelet 和 docker 之间的一个桥梁，他负责将请求转发给 docker（或者其他 runtime）。kubelet 访问 streaming server 的地址就是 127.0.0.1:{streaming sever port}。\n而我们遇到问题中端口冲突，就是 streaming server 端口和 nodeport 冲突。kubelet 拿到 exec url 后，命中本地 iptables 规则，然后请求被转发到了 nodeport 关联的容器，返回上述错误。\n在源码研究过程中，参数 \u0026ndash;redirect-container-streaming 引起了我们的注意：\n1 2 3 4 5 6 7 8 9 10 11 12 13 func (s *Server) getExec(request *restful.Request, response *restful.Response) { .... 省略若干代码.... url, err := s.host.GetExec(podFullName, params.podUID, params.containerName, params.cmd, *streamOpts) if err != nil { streaming.WriteError(err, response.ResponseWriter) return } if s.redirectContainerStreaming { http.Redirect(response.ResponseWriter, request.Request, url.String(), http.StatusFound) return } proxyStream(response.ResponseWriter, request.Request, url) } 该参数开启后会并不会进行 proxyStream。而是直接向 apiserver 发送 302 跳转，流程变为如下：\n该参数说明，如下：\n1 --redirect-container-streaming Enables container streaming redirect. If false, kubelet will proxy container streaming data between apiserver and container runtime; if true, kubelet will return an http redirect to apiserver, and apiserver will access container runtime directly. The proxy approach is more secure, but introduces some overhead. The redirect approach is more performant, but less secure because the connection between apiserver and container runtime may not be authenticated. 另外，在 1.18 版本中我们发现该参数即将废弃，社区中已经在 kep Cleaning up container streaming requests中详细说明了后续下线计划（1.18 进行下线提示、1.20版本参数失效、1.22 参数被删除）。后续 apiserver 无法直接和 dockershim 通信。\n如何避免端口冲突 经过源码阅读，我们了解了执行 kube exec 的流程，通过关闭 streaming server 可以避免 streaming server 端口和 nodeport 冲突。但是该方案只能在 1.20 版本前的集群中使用。\n另外，进一步思考，如果其他进程使用了一个随机端口是否也会出现该问题呢？\n还是有一定冲突概率的，在 #85418 issue 中就有人提出了该问题，从相关讨论中推荐解决方法是通过宿主机预留端口（net.ipv4.ip_local_port_range）解决。k8s apiserver 默认的 nodeport 端口范围为 30000-32767 （通过 \u0026ndash;service-node-port-range 参数配置），一般宿主机 net.ipv4.ip_local_port_range 默认范围为 32768-60999。而我们出现冲突，因为使用的某云 k8s 集群修改了 apiserver 参数为 30000-50000，导致出现端口冲突问题。\n其实，kube-proxy 为了避免端口冲突的问题，运行过程会监听所有的 nodeport 端口。但是，这存在一个鸡生蛋的问题。如果某个 nodeport 分配前已经被其他应用占用，或者 kube-proxy 重启，还是会存在端口冲突的问题。在 #100643 issue 中也进行了相关讨论，希望后续能有完美的解决方案。\n综上，目前解决方案下面几种：\n1.20 前版本可以通过 \u0026ndash;redirect-container-streaming 关闭 steaming server，避免 kubelet 和 nodeport 端口冲突； 修改系统参数和 apiserver 端口范围，保证和宿主机随机端口范围不重合； 其他技术，例如 #100643 issue 中提出的 ebpf。 总结 通过一个生产 kubectl exec异常问题，我们了解了执行 exec 命令后，整个底层转发逻辑：\napiserver 查询到 pod 所在 node ip，通过 nodeip:10250 端口向 kubelet 发起请求； kubelet 接收到请求后，向本地 runtime 获取 exec url。然后，1.20 之前会基于参数 \u0026ndash;redirect-container-streaming 有两种处理流程： 开启参数，通过 302 跳转方式，将 apiserver 请求重定向到 exec url； 关闭参数，会先直接和 runtime 建立 exec 通道，然后将 apiserver 请求升级为 spdy 或 ws 连接； 后续 apisever 和 runtime 通道建立完成，client 就可以在 terminal 上执行命令了。 ","date":"2022-03-01T00:00:00Z","image":"https://jasonrd.github.io/blog/p/kubectl-exec-deepin/helena-hertz-wWZzXlDpMog-unsplash_hu45a5e3ad5e058da6a00650ed8fd40bea_15530_120x120_fill_q75_box_smart1.jpg","permalink":"https://jasonrd.github.io/blog/p/kubectl-exec-deepin/","title":"从 kubectl exec 异常问题开始"},{"content":"背景 最近业务应用使用 Service ip 进行压测时，当容器销毁时，部分请求会出现 connect refused 错误。按照文档 云上 pod 下线引起短时服务不可用 进行优雅下线优化后有一定改善，但是仍然存在 connect refused 异常。本文通过分析 kube-router 实现 vip 的逻辑，进行定位问题根因，并举一反三对 kubernetes 默认组件 kube-proxy service IP 实现进行研究和分析。\nIPVS VIP 实现 ipvs (IP Virtual Server) 是工作在内核态的4层负载均衡，也就是我们常说的4层LAN交换，作为 Linux 内核的一部分。ipvs运行在主机上，在真实服务器集群前充当负载均衡器。ipvs可以将基于TCP和UDP的服务请求转发到真实服务器上，从而达到通过单个 VIP 代理多个后端真实服务的目的。IPVS 和 iptables 一样都是基于内核底层 netfilter 实现，netfilter 主要通过各个链的钩子实现包处理和转发。\nipvs 作为内核中负载均衡，有多种负载策略：rr（轮询）、wrr（加权轮询）、sh（源地址哈希）等，默认使用 rr 模式。vip 后面关联多个 pod ip， 通过 VIP 请求后 ipvs 会根据配置的均衡策略选取其中一个 pod ip 进行流量转发。\n如上图，使用 ipvsadm 工具查看 10.59.38.148:8001 转发的 RS（Real Server） 有两个：10.60.10.7、10.60.14.8，转发策略为 RR，其中 Weight 标识每个 RS 的权重。\n当 Weight=0 时，新连接不会转发到该 RS；但是，已建立的连接仍会保持，直到连接释放。ActiveConn 是活动连接数，也就是 tcp 连接状态的 ESTABLISHED；InActConn 是指除了 ESTABLISHED 以外的，所有的其它状态的 tcp 连接。\n常用开源组件实现逻辑 kube-router 先已公司使用 cni 插件为 kube-router 作为研究对象，通过使用部署一个简单的 go http server 两副本应用，观察删除一个 Pod 后，会发生哪些变化：\n上面两张图，分别对应 http server 在是否处理 SIGTERM 信号场景下对 ipvs 更新的影响。\n上图可以，看出当删除 Pod 时，在 20s+ 以后 kube-router 才会将被删除的 pod IP 在 ipvs 中摘掉。而摘掉流量方式是直接在 ipvs 中删除对应的 RS (10.60.10.7)：\n==》\n这样，在两种场景下会出现异常：\npod 容器已完全删除，但是 ipvs 还会转发流量到被删除的 Pod IP 上。也就容器完全删除早于 ipvs 中 RS 的删除动作； Ipvs 摘掉pod 流量时，存在未释放的连接。也就是 ipvs 中 RS 删除早于 pod 销毁，并且存在持久化的会话； 第一种问题还是未做好优雅下线导致的，可以通过 prestop 来增加优雅下线，提前将连接释放掉。但是，由于删除后 20s 仍然又新连接进来，虽然解决了第一个场景问题，但是第二个场景问题还是会存在的。\n在上述业务压测出现问题的场景中：\n业务容器已经做了优雅下线（等待15s+业务层开始优雅下线逻辑），整个优雅下线时间\u0026gt;=15s； 修改业务容器 terminatedGracePeriod 时长为 120s 后，我们抓包发现没有新流量进入，但仍然存在 connect refused 异常； 出现问题场景，属于第二种。那第二种问题如何解决呢？回答这个问题前，先要弄清楚下面疑问：\n首先，为什么 ipvs 在 20s 后才会将 pod ip 摘掉； 之前有了解过 ipvs 中 RS weight=0 时，新流量不会转发到该 RS，那 kube-router 有没有实现这个逻辑呢？ 带着上面两个问题，我们来看一下 kube-router 源码。\n源码分析 kube-router 内部会通过 list/watch 来监听 endpoint 和 service 更新，当 endpoint 发生变更（pod 删除）\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 // proxy/network_services_controller.go func (nsc *NetworkServicesController) OnEndpointsUpdate(ep *api.Endpoints) { ... ... // build new service and endpoints map to reflect the change newServiceMap := nsc.buildServicesInfo() newEndpointsMap := nsc.buildEndpointsInfo() if len(newEndpointsMap) != len(nsc.endpointsMap) || !reflect.DeepEqual(newEndpointsMap, nsc.endpointsMap) { nsc.endpointsMap = newEndpointsMap nsc.serviceMap = newServiceMap glog.V(1).Infof(\u0026#34;Syncing IPVS services sync for update to endpoint: %s/%s\u0026#34;, ep.Namespace, ep.Name) nsc.sync(synctypeIpvs) } else { glog.V(1).Infof(\u0026#34;Skipping IPVS services sync on endpoint: %s/%s update as nothing changed\u0026#34;, ep.Namespace, ep.Name) } } // OnServiceUpdate handle change in service update from the API server func (nsc *NetworkServicesController) OnServiceUpdate(svc *api.Service) { ... ... // build new service and endpoints map to reflect the change newServiceMap := nsc.buildServicesInfo() newEndpointsMap := nsc.buildEndpointsInfo() if len(newServiceMap) != len(nsc.serviceMap) || !reflect.DeepEqual(newServiceMap, nsc.serviceMap) { nsc.endpointsMap = newEndpointsMap nsc.serviceMap = newServiceMap glog.V(1).Infof(\u0026#34;Syncing IPVS services sync on update to service: %s/%s\u0026#34;, svc.Namespace, svc.Name) nsc.sync(synctypeIpvs) } else { glog.V(1).Infof(\u0026#34;Skipping syncing IPVS services for update to service: %s/%s as nothing changed\u0026#34;, svc.Namespace, svc.Name) } } 可以看到 endpoint 和 service 更新，都是先更新 nsc 的 endpointMap 和 serviceMap，然后执行 nsc.sync 函数。nsc.sync 函数将变更类型同步到 syncChan：\n1 2 3 4 5 6 7 8 //proxy/network_services_controller.go func (nsc *NetworkServicesController) sync(syncType int) { select { case nsc.syncChan \u0026lt;- syncType: default: glog.V(2).Infof(\u0026#34;Already pending sync, dropping request for type %d\u0026#34;, syncType) } } syncChan 在 NetworkServiceController 的 Run 函数中进行监听：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 // Run periodically sync ipvs configuration to reflect desired state of services and endpoints func (nsc *NetworkServicesController) Run(healthChan chan\u0026lt;- *healthcheck.ControllerHeartbeat, stopCh \u0026lt;-chan struct{}, wg *sync.WaitGroup) { ...... select { case \u0026lt;-stopCh: glog.Info(\u0026#34;Shutting down network services controller\u0026#34;) return default: // kube-router 默认启动时，先更新一遍 ipvs err := nsc.doSync() if err != nil { glog.Fatalf(\u0026#34;Failed to perform initial full sync %s\u0026#34;, err.Error()) } nsc.readyForUpdates = true } // loop forever until notified to stop on stopCh for { select { case \u0026lt;-stopCh: nsc.mu.Lock() nsc.readyForUpdates = false nsc.mu.Unlock() glog.Info(\u0026#34;Shutting down network services controller\u0026#34;) return case \u0026lt;-gracefulTicker.C: if nsc.readyForUpdates \u0026amp;\u0026amp; nsc.gracefulTermination { glog.V(3).Info(\u0026#34;Performing periodic graceful destination cleanup\u0026#34;) nsc.gracefulSync() } // 从 syncChan 唤醒协程执行 ipvs 的更新/同步逻辑 case perform := \u0026lt;-nsc.syncChan: healthcheck.SendHeartBeat(healthChan, \u0026#34;NSC\u0026#34;) switch perform { case synctypeAll: glog.V(1).Info(\u0026#34;Performing requested full sync of services\u0026#34;) err := nsc.doSync() if err != nil { glog.Errorf(\u0026#34;Error during full sync in network service controller. Error: \u0026#34; + err.Error()) } case synctypeIpvs: glog.V(1).Info(\u0026#34;Performing requested sync of ipvs services\u0026#34;) nsc.mu.Lock() // ipvs 的更新/同步逻辑 err := nsc.syncIpvsServices(nsc.serviceMap, nsc.endpointsMap) nsc.mu.Unlock() if err != nil { glog.Errorf(\u0026#34;Error during ipvs sync in network service controller. Error: \u0026#34; + err.Error()) } } if err == nil { healthcheck.SendHeartBeat(healthChan, \u0026#34;NSC\u0026#34;) } case \u0026lt;-t.C: glog.V(1).Info(\u0026#34;Performing periodic sync of ipvs services\u0026#34;) healthcheck.SendHeartBeat(healthChan, \u0026#34;NSC\u0026#34;) err := nsc.doSync() if err != nil { glog.Errorf(\u0026#34;Error during periodic ipvs sync in network service controller. Error: \u0026#34; + err.Error()) glog.Errorf(\u0026#34;Skipping sending heartbeat from network service controller as periodic sync failed.\u0026#34;) } else { healthcheck.SendHeartBeat(healthChan, \u0026#34;NSC\u0026#34;) } } } } 更新 ipvs 函数 nsc.syncIpvsServices ：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 // proxy/service_endpoints_sync.go // sync the ipvs service and server details configured to reflect the desired state of Kubernetes services // and endpoints as learned from services and endpoints information from the api server func (nsc *NetworkServicesController) syncIpvsServices(serviceInfoMap serviceInfoMap, endpointsInfoMap endpointsInfoMap) error { start := time.Now() defer func() { endTime := time.Since(start) if nsc.MetricsEnabled { metrics.ControllerIpvsServicesSyncTime.Observe(endTime.Seconds()) } glog.V(1).Infof(\u0026#34;sync ipvs services took %v\u0026#34;, endTime) }() var err error var syncErrors bool // map to track all active IPVS services and servers that are setup during sync of // cluster IP, nodeport and external IP services activeServiceEndpointMap := make(map[string][]string) // 配置 VIP ipvs err = nsc.setupClusterIPServices(serviceInfoMap, endpointsInfoMap, activeServiceEndpointMap) if err != nil { syncErrors = true glog.Errorf(\u0026#34;Error setting up IPVS services for service cluster IP\u0026#39;s: %s\u0026#34;, err.Error()) } // 配置 nodeport ipvs err = nsc.setupNodePortServices(serviceInfoMap, endpointsInfoMap, activeServiceEndpointMap) if err != nil { syncErrors = true glog.Errorf(\u0026#34;Error setting up IPVS services for service nodeport\u0026#39;s: %s\u0026#34;, err.Error()) } err = nsc.setupExternalIPServices(serviceInfoMap, endpointsInfoMap, activeServiceEndpointMap) if err != nil { syncErrors = true glog.Errorf(\u0026#34;Error setting up IPVS services for service external IP\u0026#39;s and load balancer IP\u0026#39;s: %s\u0026#34;, err.Error()) } // 清理过期 vip err = nsc.cleanupStaleVIPs(activeServiceEndpointMap) if err != nil { syncErrors = true glog.Errorf(\u0026#34;Error cleaning up stale VIP\u0026#39;s configured on the dummy interface: %s\u0026#34;, err.Error()) } // 清理过期 RS IP err = nsc.cleanupStaleIPVSConfig(activeServiceEndpointMap) if err != nil { syncErrors = true glog.Errorf(\u0026#34;Error cleaning up stale IPVS services and servers: %s\u0026#34;, err.Error()) } err = nsc.syncIpvsFirewall() if err != nil { syncErrors = true glog.Errorf(\u0026#34;Error syncing ipvs svc iptables rules to permit traffic to service VIP\u0026#39;s: %s\u0026#34;, err.Error()) } err = nsc.setupForDSR(serviceInfoMap) if err != nil { syncErrors = true glog.Errorf(\u0026#34;Error setting up necessary policy based routing configuration needed for direct server return: %s\u0026#34;, err.Error()) } if syncErrors { glog.V(1).Info(\u0026#34;One or more errors encountered during sync of IPVS services and servers to desired state\u0026#34;) } else { glog.V(1).Info(\u0026#34;IPVS servers and services are synced to desired state\u0026#34;) } return nil } 每次有 endpoint/service 发生变化，nsc.syncIpvsServices 都会将所有ipvs 更新一遍，整个函数执行时间根据 service 数量不同执行时间不同。在我们线下环境有 3200 个 service，函数执行时间 30s+。\n这就解答了「为什么 ipvs 在 20s 后才会将 pod ip 摘掉」。\n那上面说的设置 ipvs rs weight 来停止新连接的转发，在 kube-router 中有没有实现呢？我们在阅读源码时，发现下面这一部分代码：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 // Run periodically sync ipvs configuration to reflect desired state of services and endpoints func (nsc *NetworkServicesController) Run(healthChan chan\u0026lt;- *healthcheck.ControllerHeartbeat, stopCh \u0026lt;-chan struct{}, wg *sync.WaitGroup) { ...... // loop forever until notified to stop on stopCh for { select { case \u0026lt;-stopCh: nsc.mu.Lock() nsc.readyForUpdates = false nsc.mu.Unlock() glog.Info(\u0026#34;Shutting down network services controller\u0026#34;) return case \u0026lt;-gracefulTicker.C: if nsc.readyForUpdates \u0026amp;\u0026amp; nsc.gracefulTermination { glog.V(3).Info(\u0026#34;Performing periodic graceful destination cleanup\u0026#34;) nsc.gracefulSync() } ...... } NSC 中存在一个 graceful 定时器，触发时会执行 nsc.gracefulSync 函数：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 // proxy/network_service_graceful.go func (nsc *NetworkServicesController) gracefulSync() { nsc.gracefulQueue.mu.Lock() defer nsc.gracefulQueue.mu.Unlock() var newQueue []gracefulRequest // Itterate over our queued destination removals one by one, and don\u0026#39;t add them back to the queue if they were processed for _, job := range nsc.gracefulQueue.queue { if removed := nsc.gracefulDeleteIpvsDestination(job); removed { continue } newQueue = append(newQueue, job) } nsc.gracefulQueue.queue = newQueue } func (nsc *NetworkServicesController) gracefulDeleteIpvsDestination(req gracefulRequest) bool { var deleteDestination bool // Get active and inactive connections for the destination aConn, iConn, err := nsc.getIpvsDestinationConnStats(req.ipvsSvc, req.ipvsDst) if err != nil { glog.V(1).Infof(\u0026#34;Could not get connection stats for destination: %s\u0026#34;, err.Error()) } else { // Do we have active or inactive connections to this destination // if we don\u0026#39;t, proceed and delete the destination ahead of graceful period if aConn == 0 \u0026amp;\u0026amp; iConn == 0 { deleteDestination = true } } // Check if our destinations graceful termination period has passed if time.Since(req.deletionTime) \u0026gt; req.gracefulTerminationPeriod { deleteDestination = true } //Destination has has one or more conditions for deletion if deleteDestination { glog.V(2).Infof(\u0026#34;Deleting IPVS destination: %s\u0026#34;, ipvsDestinationString(req.ipvsDst)) if err := nsc.ln.ipvsDelDestination(req.ipvsSvc, req.ipvsDst); err != nil { glog.Errorf(\u0026#34;Failed to delete IPVS destination: %s, %s\u0026#34;, ipvsDestinationString(req.ipvsDst), err.Error()) } } return deleteDestination } nsc.gracefulSync 函数主要逻辑是对 ipvs 中 RS 进行删除，判断逻辑如下：\nInActConn 和 ActConn 都为0，则说明没有连接存在，可以直接删除； 如果 InActConn 或 ActConn 不为零，但是 pod 删除等待时长已经超过 pod.gracefulTerminationPeriod，则可以直接删除； 否则，等待下一次进行； 那 nsc.gracefulQueue 队列是在哪里写入的呢？我们之间检索 gracefulQueue 字段，发现在函数 nsc.ipvsDeleteDestination 中写入\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 // proxy/network_service_graceful.go func (nsc *NetworkServicesController) ipvsDeleteDestination(svc *ipvs.Service, dst *ipvs.Destination) error { // If we have enabled graceful termination set the weight of the destination to 0 // then add it to the queue for graceful termination if nsc.gracefulTermination { req := gracefulRequest{ ipvsSvc: svc, ipvsDst: dst, deletionTime: time.Now(), } dst.Weight = 0 err := nsc.ln.ipvsUpdateDestination(svc, dst) if err != nil { return err } nsc.addToGracefulQueue(\u0026amp;req) } else { err := nsc.ln.ipvsDelDestination(svc, dst) if err != nil { return err } } // flush conntrack when Destination for a UDP service changes if svc.Protocol == syscall.IPPROTO_UDP { if err := nsc.flushConntrackUDP(svc); err != nil { glog.Errorf(\u0026#34;Failed to flush conntrack: %s\u0026#34;, err.Error()) } } return nil } nsc.ipvsDeleteDestination 判断 nsc.gracefulTermination 如果开启，则不会立即删除。而是，执行下面操作：\n更新 ipvs 配置，设置 RS weight=0； 加入 gracefulQueue 队列中，等待删除； 综上，可以看出 nsc.gracefulTermination 就是开启 ipvs 优雅下线的开关，而这个是通过参数 \u0026ndash;ipvs-graceful-termination 来控制的。然后，我们开启 ipvs-graceful-termination 进行测试。\n我们对测试应用 http-server 增加优雅下线逻辑：\n捕获 SIGTERM 信号； 捕获到 SIGTERM 信号后，在 http header 中增加 connection: close，也就是在容器下线阶段使用短连接； 然后再进行压测：\n=\u0026gt;=\u0026gt;\n=\u0026gt;\n上面分别对应，销毁 Pod 前、销毁 Pod 过程中1、销毁 Pod 过程中2、Pod 被完全销毁。\n通过开启 ipvs graceful terminated，并且容器销毁后应用捕获 SIGTERM 信号进行连接的优雅下线，测试中未出现请求错误的问题。\n总结 对以上源码研究，进行总结如下：\nkube-router 每次更新都是全量更新，service 数量不同 ipvs RS 新增、更新、删除的延迟不同； 具有优雅删除 RS 的能力，并且结合了 Pod terminatedGracefulPeriod 进行 RS 的完全删除； 为开启 \u0026ndash;ipvs-graceful-termination，会立即删除 RS，但是由于全量更新的延迟，表现上是有一定延迟（线下 30s+、线上15s+）； kube-proxy 实现逻辑 根据 1.22 版本 kube-proxy 代码，pod 删除过程：\n销毁 Pod，Endpoint 中移除销毁中的 Pod IP； kube-proxy watch 到 endpoint 发生变化，进入 ipvs 删除判断逻辑： 协议不是 UDP 或 SFTP； ActConn 和 InActConn 都为0； 不满足删除判断，则更新 ipvs 中 RS weight=0; 然后，将 RS 加入到本地队列，每隔 1min 进行删除判断； 源码 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 // proxy/ipvs/graceful_termination.go // GracefulDeleteRS to update rs weight to 0, and add rs to graceful terminate list func (m *GracefulTerminationManager) GracefulDeleteRS(vs *utilipvs.VirtualServer, rs *utilipvs.RealServer) error { // Try to delete rs before add it to graceful delete list ele := \u0026amp;listItem{ VirtualServer: vs, RealServer: rs, } deleted, err := m.deleteRsFunc(ele) if err != nil { klog.Errorf(\u0026#34;Delete rs %q err: %v\u0026#34;, ele.String(), err) } if deleted { return nil } rs.Weight = 0 err = m.ipvs.UpdateRealServer(vs, rs) if err != nil { return err } klog.V(5).Infof(\u0026#34;Adding an element to graceful delete rsList: %+v\u0026#34;, ele) m.rsList.add(ele) return nil } func (m *GracefulTerminationManager) deleteRsFunc(rsToDelete *listItem) (bool, error) { klog.V(5).Infof(\u0026#34;Trying to delete rs: %s\u0026#34;, rsToDelete.String()) rss, err := m.ipvs.GetRealServers(rsToDelete.VirtualServer) if err != nil { return false, err } for _, rs := range rss { if rsToDelete.RealServer.Equal(rs) { // For UDP and SCTP traffic, no graceful termination, we immediately delete the RS // (existing connections will be deleted on the next packet because sysctlExpireNoDestConn=1) // For other protocols, don\u0026#39;t delete until all connections have expired) if utilipvs.IsRsGracefulTerminationNeeded(rsToDelete.VirtualServer.Protocol) \u0026amp;\u0026amp; rs.ActiveConn+rs.InactiveConn != 0 { klog.V(5).Infof(\u0026#34;Not deleting, RS %v: %v ActiveConn, %v InactiveConn\u0026#34;, rsToDelete.String(), rs.ActiveConn, rs.InactiveConn) return false, nil } klog.V(5).Infof(\u0026#34;Deleting rs: %s\u0026#34;, rsToDelete.String()) err := m.ipvs.DeleteRealServer(rsToDelete.VirtualServer, rs) if err != nil { return false, fmt.Errorf(\u0026#34;Delete destination %q err: %v\u0026#34;, rs.String(), err) } return true, nil } } return true, fmt.Errorf(\u0026#34;Failed to delete rs %q, can\u0026#39;t find the real server\u0026#34;, rsToDelete.String()) } kube-proxy GracefulDeleteRS 函数会 deleteRsFunc 进行 RS 删除，删除 RS 条件：\n协议不是 UDP 或 SFTP； ActConn 和 InActConn 都为0； 否则，更新 ipvs 将 RS weight 置为 0，不接收新流量进入。然后会加入到带删除列表中，进行定期（间隔1min）清理：\n1 2 3 4 5 6 7 8 9 10 func (m *GracefulTerminationManager) tryDeleteRs() { if !m.rsList.flushList(m.deleteRsFunc) { klog.Errorf(\u0026#34;Try flush graceful termination list err\u0026#34;) } } // Run start a goroutine to try to delete rs in the graceful delete rsList with an interval 1 minute func (m *GracefulTerminationManager) Run() { go wait.Until(m.tryDeleteRs, rsCheckDeleteInterval, wait.NeverStop) } 优化方案 优点 缺点 方案一 kube-router 开启 \u0026ndash;ipvs-graceful-termination 参数 方案二 开发内网负载均衡组件 参考 http://www.dockone.io/article/9441 https://wsgzao.github.io/post/lvs-nat/ Issue 572 - Graceful termination kube-proxy ipvs support graceful termination ","date":"2021-12-19T00:00:00Z","image":"https://jasonrd.github.io/blog/p/how-endpoint-flush-ipvs/the-creative-exchange-d2zvqp3fpro-unsplash_huf941de4769045cdfa8c9ee7036519a2a_35369_120x120_fill_q75_box_smart1.jpg","permalink":"https://jasonrd.github.io/blog/p/how-endpoint-flush-ipvs/","title":"endpoint 更新后 vip 转发实现探究"},{"content":"背景 近日，接连收到多个云上站点业务出现 502 问题反馈：\n和业务负责人沟通后，应用确认加入了优雅下线逻辑。\n排查过程 首先，查看网关日志：\n两次日志，都是请求 LB IP 出现 503 错误码后，然后网关将 LB IP 摘掉。\n分析为什么出现 503 错误码前，先了解一下容器下线逻辑：容器进行下线时，会调用 prestop 脚本执行下线前的操作。\n在 prestop 脚本中，首先 sleep 15s （不要问我为什么），然后调用 http://127.0.0.1:${APP_PORT}/ok.htm?down=true 接口通知 java 进程进行优雅下线。该接口调用成功后，再请求应用 ok 页面，进入下面逻辑：\n也就是返回 halting 数据和503状态码。\nk8s 中在将 Pod 进行下线（标记为 Terminating 状态）时，k8s endpoint controller 就将该 Pod ip 从 lb 或 service 后端列表中摘除。既然 lb/svc 已经将在该 pod IP 摘除，为什么仍然请求到 halting Pod 呢？\n在进入应用容器中进行抓包，并和应用负责人确认后，网关 -\u0026gt; lb -\u0026gt; pod 是使用 http 长连接方式。\n在 Pod 处于 terminating 状态时，通过 svc 请求时新建立的连接将不会转发到该 pod，但是已经建立的连接在 Pod 完全删除前仍可继续通信。所以，虽然 service 将 Pod IP 摘除，但是为了保证容器的优雅下线，已经建立的连接仍然可以继续处理业务，直到容器彻底被删除。\n我们用 python 简单写了一个使用 http 长连接客户端，进行一下测试：\n1 2 3 4 5 6 7 8 9 10 import requests import time client=requests.session() headers = {\u0026#39;Content-Type\u0026#39;: \u0026#39;application/json\u0026#39;, \u0026#39;Connection\u0026#39;: \u0026#39;keep-alive\u0026#39;} while True: r=client.get(\u0026#34;http://xx.xx.16.137:8088/ok.htm\u0026#34;, headers=headers) # xx.xx.16.137 为应用 lb IP print r.status_code print r.content time.sleep(1) python client 请求应用容器：\n上面可以看到，在 15:30:06 将 pod 进行 kill 后，通过长连接仍然可以将请求转发到处于 terminating 的应用容器。直到 15s 后调用下线接口，请求返回 503。请求处于 halting 状态的 pod，server 端会主动 close 请求：\n另外，我们测试应用只有单个实例，也就是实例被删除后，lb 后端实例为 0，请求 lb 的新连接无法建立。所以，测试脚本会出现 connect refused 报错。\n解决方法 综上可知，问题原因是 kill pod 后仍然会有流量进入到 terminating 状态的 pod，然后 15s 后 prestop 脚本通知进程进行下线逻辑（也就是 ok 页面返回 halting 和 503 状态码），当网关继续请求到该 pod 就会认为 lb 出现异常，将唯一的 lb 标记为不健康，从而出现 502 异常。\n其他站点未出现该问题原因是：网关直接转发到 pod ip，摘掉的是出现异常的 pod ip 。而有问题的站点对接的只是一个公有云 LB ip。\n具体优化逻辑如下(如上图，针对单个pod):\n应用增加 connection filter，在应用进入优雅下线（被调用 http://127.0.0.1:${APP_PORT}/ok.htm?down=true）后，所有请求的 http 响应头中增加 connection:close（也就是使用短连接）； 因为 pod 处于 terminating 时，新连接不会进入该 pod； 参考代码如下： 健康检查接口，修改为去掉 503 异常，避免网关检测到 503 异常时，直接摘掉 lb ip。 参考代码如下： ","date":"2021-11-01T00:00:00Z","image":"https://jasonrd.github.io/blog/p/service-cause-failure/luca-bravo-alS7ewQ41M8-unsplash_hu0a3f1163de68d0b9471979ebf0ecf11e_32400_120x120_fill_q75_box_smart1.jpg","permalink":"https://jasonrd.github.io/blog/p/service-cause-failure/","title":"云上 pod 下线引起短时服务不可用"}]