[{"content":"问题现象 最近，在工作想要使用 kubectl exec 进入容器排查问题，结果返回下面异常：\n排查过程 我们知道 kubectl exec 的执行链路是 client -\u0026gt; kube-apiserver -\u0026gt; kubelet -\u0026gt; docker。\n登录 Kubelet 宿主机查看 kubelet 错误日志，发现有相同的报错日志，这说明是 kubelet 和 docker 之间链路又问题。通过 kubelet 日志中不能定为到问题具体原因。然后，我们试图通过抓包，希望在数据包中能发现一些线索。\n在抓包数据结果中我们发现关键字为 exec 的请求，该会话的目的地址为 A:20880, http header 中 Host 为 B:10250 （也就是是物理机上 kubelet 的 httpserver 地址）。我们查询 A 这个IP，发现是业务应用的容器 IP。\n这就比较奇怪了，正常 apiserver 发送 exec 请求为什么转发到了容器的 20880 端口。并且数据包中包含 kubectl (\u0026ldquo;User-Agent: kubectl\u0026rdquo;) http header。难道 kubectl exec 请求发送到 docker 的请求（xxxx/exec/token）被转发到了容器。通过再次尝试执行 kubectl exec 并抓包，发现执行命令和发送到 20880 端口请求匹配，这验证了我们的猜测。\n到此就把问题范围缩小到宿主机网络上，我们知道 kube-proxy 会通过 ipvs 或 iptables 对创建的 nodeport 或 service vip 的请求进行拦截和转发。我们查看 conntrack 请求记录：\n发现一条请求 127.0.0.1:33589 的记录，并且转发到的地址 A:20880 也和我们抓包的结果匹配。然后查看 33589 端口，发现该端口就是被 kubelet 占用。然后，我们查询 service，发现 33589 端口同时是 B 容器的应用 service 的 nodeport。到此问题根本原因定位到了，nodeport 端口和 kubelet 启动的转发端口冲突了，导致发送 exec 请求转发到了应用容器的 20880 端口（dubbo端口）。\n继续深挖 事情到此并没有结束，上面我们只是定位到了具体问题原因。其实还存在两个问题：\n对 kubectl exec 的执行过程还没没挖透； 如何避免该问题？ kubectl exec 的执行过程 问题没有快速定位，主要原因还是对 kubectl exec 执行流程不熟。下面来了解一下 kubectl 是怎么执行的。\n本文基于 1.14.6 源码进行研究。\n首先，简单了解一下 kubelet 架构：\nkubelet 中有上面几个部分：container manager、dockershim、http server、streaming server。kubelet 早期直接调用 docker api 管理容器，后来为了适配更多的 runtime 抽象出了一个接入层 cri。同时，为了兼容 docker 的 API，kubelet 代码中实现了这个叫 dockershim 的部分。这样就对上层屏蔽了底层 runtime。http server 通常使用 10250 对外提供 API 服务。streaming server 是需要和容器进行交互时的一个代理服务。\n在默认情况下，用户执行 kubectl exec 简化流程如下：\nterminal 中键入 kubectl exec xxx 指令，kubectl 发送请求到 apiserver https://apiserver/api/v1/namespaces/{ns}/pods/{pod}/exec?command=bash\u0026amp;container=dragon-claw\u0026amp;stdin=true\u0026amp;stdout=true\u0026amp;tty=true； apiserver 接到请求后，将请求转发到 kubelet， node:10250/api/v1/exec/{ns}/{podid}/{container}。kubelet httpserver 接收到请求后： 首先，向 dockershim 发起 getExec 请求，返回一个流地址 url （exec/{token}）； 然后，kubelet 请求 exec/xxxx url 到 streaming server，streaming server 接收到请求后，response upgrade 将连接升级成为 spdy 或 ws 连接； kubelet 收到 upgrade reponse 后，将该 reponse 直接返回给 apiserver，到此 apiserver -\u0026gt; kubelet -\u0026gt; streaming server -\u0026gt; docker 之间整个通道建立完成； 到此，用户可以在 terminal 中键入命令在容器中执行； 其中， streaming server 是 kubelet 和 docker 之间的一个桥梁，他负责将请求转发给 docker（或者其他 runtime）。kubelet 访问 streaming server 的地址就是 127.0.0.1:{streaming sever port}。\n而我们遇到问题中端口冲突，就是 streaming server 端口和 nodeport 冲突。kubelet 拿到 exec url 后，命中本地 iptables 规则，然后请求被转发到了 nodeport 关联的容器，返回上述错误。\n在源码研究过程中，参数 \u0026ndash;redirect-container-streaming 引起了我们的注意：\n1 2 3 4 5 6 7 8 9 10 11 12 13 func (s *Server) getExec(request *restful.Request, response *restful.Response) { .... 省略若干代码.... url, err := s.host.GetExec(podFullName, params.podUID, params.containerName, params.cmd, *streamOpts) if err != nil { streaming.WriteError(err, response.ResponseWriter) return } if s.redirectContainerStreaming { http.Redirect(response.ResponseWriter, request.Request, url.String(), http.StatusFound) return } proxyStream(response.ResponseWriter, request.Request, url) } 该参数开启后会并不会进行 proxyStream。而是直接向 apiserver 发送 302 跳转，流程变为如下：\n该参数说明，如下：\n1 --redirect-container-streaming Enables container streaming redirect. If false, kubelet will proxy container streaming data between apiserver and container runtime; if true, kubelet will return an http redirect to apiserver, and apiserver will access container runtime directly. The proxy approach is more secure, but introduces some overhead. The redirect approach is more performant, but less secure because the connection between apiserver and container runtime may not be authenticated. 另外，在 1.18 版本中我们发现该参数即将废弃，社区中已经在 kep Cleaning up container streaming requests中详细说明了后续下线计划（1.18 进行下线提示、1.20版本参数失效、1.22 参数被删除）。后续 apiserver 无法直接和 dockershim 通信。\n如何避免端口冲突 经过源码阅读，我们了解了执行 kube exec 的流程，通过关闭 streaming server 可以避免 streaming server 端口和 nodeport 冲突。但是该方案只能在 1.20 版本前的集群中使用。\n另外，进一步思考，如果其他进程使用了一个随机端口是否也会出现该问题呢？\n还是有一定冲突概率的，在 #85418 issue 中就有人提出了该问题，从相关讨论中推荐解决方法是通过宿主机预留端口（net.ipv4.ip_local_port_range）解决。k8s apiserver 默认的 nodeport 端口范围为 30000-32767 （通过 \u0026ndash;service-node-port-range 参数配置），一般宿主机 net.ipv4.ip_local_port_range 默认范围为 32768-60999。而我们出现冲突，因为使用的某云 k8s 集群修改了 apiserver 参数为 30000-50000，导致出现端口冲突问题。\n其实，kube-proxy 为了避免端口冲突的问题，运行过程会监听所有的 nodeport 端口。但是，这存在一个鸡生蛋的问题。如果某个 nodeport 分配前已经被其他应用占用，或者 kube-proxy 重启，还是会存在端口冲突的问题。在 #100643 issue 中也进行了相关讨论，希望后续能有完美的解决方案。\n综上，目前解决方案下面几种：\n1.20 前版本可以通过 \u0026ndash;redirect-container-streaming 关闭 steaming server，避免 kubelet 和 nodeport 端口冲突； 修改系统参数和 apiserver 端口范围，保证和宿主机随机端口范围不重合； 其他技术，例如 #100643 issue 中提出的 ebpf。 总结 通过一个生产 kubectl exec异常问题，我们了解了执行 exec 命令后，整个底层转发逻辑：\napiserver 查询到 pod 所在 node ip，通过 nodeip:10250 端口向 kubelet 发起请求； kubelet 接收到请求后，向本地 runtime 获取 exec url。然后，1.20 之前会基于参数 \u0026ndash;redirect-container-streaming 有两种处理流程： 开启参数，通过 302 跳转方式，将 apiserver 请求重定向到 exec url； 关闭参数，会先直接和 runtime 建立 exec 通道，然后将 apiserver 请求升级为 spdy 或 ws 连接； 后续 apisever 和 runtime 通道建立完成，client 就可以在 terminal 上执行命令了。 ","date":"2022-03-01T00:00:00Z","image":"https://jasonrd.github.io/blog/p/kubectl-exec-deepin/helena-hertz-wWZzXlDpMog-unsplash_hu45a5e3ad5e058da6a00650ed8fd40bea_15530_120x120_fill_q75_box_smart1.jpg","permalink":"https://jasonrd.github.io/blog/p/kubectl-exec-deepin/","title":"从 kubectl exec 异常问题开始"},{"content":"背景 近日，收到业务同学反馈，在进行在线推理业务时发现：部署到 Kubernetes 的服务，压测试性能出现数量级下降问题（只能达到 200 QPS），业务性能将难以满足客户实际需求。\n我们对该问题进行了详细分析，通过对内核参数优化，最终容器环境性能可以达到 2000 QPS。\n下面，我们将会分享整个问题分析和优化的过程，并且对其中涉及到的部分关键性问题进行剖析。另外，为何 kubernetes 优化后最高达到 2000 qps，而 docker run 环境能够达到 4000 qps？下文中也会给出答案。\n分析过程 根据业务同学反馈，压测端（10.58.14.13）使用 kubernetes service 和 nodeport 两种方式，都会出现 QPS 急剧降低的问题。\n观察 QPS 降低为 200 时 ab 压测输出结果，可以看出 rt 大部分消耗在 connect 阶段（最大达到1s），也就是压测机（10.58.14.13）和 10.58.14.15 上的 Kubernetes 容器建立连接的过程：\n出现这种现象，可以从下面两个方面进行分析：\n14.13 -\u0026gt; 14.15 之间网络存在问题； 14.15 系统层面问题； 为了排除网络方面的问题，我们在 14.15 主机上使用 Pod IP 进行压测，压测 QPS 为 2000 左右。然后，同样在 14.15 主机上使用 service ip 进行压测，我们发现在大约 30000 请求后，和之前业务同学描述一致：QPS 由 2000 降低到不到 120。压测时观察系统负载和业务容器 cpu 都非常低，这说明问题和 Kubernetes 网络架构有关。先看一下使用 Kubernetes service 请求和 Pod ip 两种方式有哪些不同：\nService ip 是 Kubernetes 在 IP 池中选取的一个 VIP，每个 VIP 会关联多个 POD 实例。为了能够通过 VIP 请求到具体的容器，Kubernetes 网络插件会在每个节点上做一些处理，目前常用的两种模式是 iptables 或 ipvs。我们本次出问题场景使用的是 ipvs 模式。在 ipvs 模式下，当客户端使用 VIP 请求时，会经过内核 ipvs 模块进行数据处理，才将流量转发到具体的容器实例。\n通过对比发现，我们本次出现问题应该就是 ipvs 模块上。\n为了方便排查问题，我们在 Kubernetes 中部署了一个简单的 http server Pod，然后在 Pod 所在主机上进行压测来进行问题分析。\n在 linux 系统中有很多工具可以方便我们来查看 ipvs 管理的连接，在压测过程中使用 ipvsadm 观察看到 vip 关联的 rs 后端连接数的变化：\n这里简单介绍执行 ipvsadm - L -t vip:port 返回的信息中几个字段的含义：\nWeight 流量转发给某个后端实例所占的权重，当该值为 0 时新连接就不会转发到对应的后端 ip 上； ActionConn 是活动连接数，也就是tcp连接状态的 ESTABLISHED； InActConn 是指除了ESTABLISHED以外的,所有的其它状态的tcp连接； 我们在压测开始 server cpu 利用能够跑满，随着 InActConn 数量的增长 server 的 cpu 利用率也开始下滑，最后当 InActConn 维持到 32000 多时，http server 的 cpu 利用率只有 3%，InActConn 数数字几乎没有变化。显然，大部分请求没有到应用层。\n之前在 Kubernetes 社区看到过一个关于 ipvs issue #81775，主要是讲一个单个客户端向某个 vip 发送请求时，容器销毁过程中会出现大量的请求错误。其中提到了一个关于 ipvs 内核参数 net.ipv4.vs.conn_reuse_mode，该参数用来开启对 ipvs connect 端口重用：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 conn_reuse_mode - INTEGER 1 - default Controls how ipvs will deal with connections that are detected port reuse. It is a bitmap, with the values being: 0: disable any special handling on port reuse. The new connection will be delivered to the same real server that was servicing the previous connection. This will effectively disable expire_nodest_conn. bit 1: enable rescheduling of new connections when it is safe. That is, whenever expire_nodest_conn and for TCP sockets, when the connection is in TIME_WAIT state (which is only possible if you use NAT mode). bit 2: it is bit 1 plus, for TCP connections, when connections are in FIN_WAIT state, as this is the last state seen by load balancer in Direct Routing mode. This bit helps on adding new real servers to a very busy cluster. net.ipv4.vs.conn_reuse_mode=0时，ipvs不会对新连接进行重新负载，而是复用之前的负载结果，将新连接转发到原来的rs上； net.ipv4.vs.conn_reuse_mode=1时，ipvs则会对新连接进行重新调度。 查看压测的节点内核参数，发现 net.ipv4.vs.conn_reuse_mode 值为 1。然后，我们修改内核参数： net.ipv4.vs.conn_reuse_mode=0，再进行压测，QPS 稳定到了 2000 左右。说明问题就是和 ipvs 这个参数有关。\n相关的，还有一个内核参数net.ipv4.vs.expire_nodest_conn，用于控制连接的rs不可用时的处理。在开启时，如果后端rs不可用，会立即结束掉该连接，使客户端重新发起新的连接请求；否则将数据包silently drop，也就是DROP掉数据包但不结束连接，等待客户端的重试。内核中关于destination 不可用的判断，是在ipvs执行删除vs（在__ip_vs_del_service()中实现）或删除rs（在ip_vs_del_dest()中实现）时，会调用__ip_vs_unlink_dest()方法，将相应的destination置为不可用。\n进一步深挖 虽然，修改内核后 QPS 由 120+ 提升到 2000 已经满足业务方的要求。但是还有一些疑惑没有解决：\n为什么内核参数设置为 net.ipv4.vs.conn_reuse_mode=1 时，导致 QPS 降低到了 200？ 内核参数修改为 net.ipv4.vs.conn_reuse_mode=0 时，会导致哪些问题？ 为什么 docker run 运行的容器 QPS 能达到 4000 ？ 带着这些疑惑，我们做进一步研究。\nnet.ipv4.vs.conn_reuse_mode 开启和关闭影响 ipvs 会将请求 vs 的请求转发到 rs 需要使用 conntrack 表记录每一个连接的四元组信息。在我们压测过程中也可以看到每一条连接都会对应一条记录：\n第一个问题，社区 2018 年在 #70747 issue 中进行了讨论和修复。其中 comment 中有提到一个 linux kernel 的讨论。在未开启端口复用时，如果匹配到新请求四元组已经存在于 conntrack 表中，会直接将包丢弃（NF_DROP）：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 // net/netfilter/ipvs/ip_vs_core.c static unsigned int ip_vs_in(struct netns_ipvs *ipvs, unsigned int hooknum, struct sk_buff *skb, int af) { ... .... /* * Check if the packet belongs to an existing connection entry */ cp = pp-\u0026gt;conn_in_get(ipvs, af, skb, \u0026amp;iph); //判断是否属于某个已有的connection conn_reuse_mode = sysctl_conn_reuse_mode(ipvs); //当conn_reuse_mode开启，同时出现端口复用（例如收到TCP的SYN包，并且也属于已有的 connection），进行处理 if (conn_reuse_mode \u0026amp;\u0026amp; !iph.fragoffs \u0026amp;\u0026amp; is_new_conn(skb, \u0026amp;iph) \u0026amp;\u0026amp; cp) { bool uses_ct = false, resched = false; //如果开启了expire_nodest_conn、目标rs的weight为0 if (unlikely(sysctl_expire_nodest_conn(ipvs)) \u0026amp;\u0026amp; cp-\u0026gt;dest \u0026amp;\u0026amp; unlikely(!atomic_read(\u0026amp;cp-\u0026gt;dest-\u0026gt;weight))) { resched = true; //查询是否用到了conntrack uses_ct = ip_vs_conn_uses_conntrack(cp, skb); } else if (is_new_conn_expected(cp, conn_reuse_mode)) { //连接是 expected 的情况，比如 FTP uses_ct = ip_vs_conn_uses_conntrack(cp, skb); if (!atomic_read(\u0026amp;cp-\u0026gt;n_control)) { resched = true; } else { /* Do not reschedule controlling connection * that uses conntrack while it is still * referenced by controlled connection(s). */ resched = !uses_ct; } } //如果expire_nodest_conn未开启，并且也非期望连接，实际上直接跳出了 if (resched) { if (!atomic_read(\u0026amp;cp-\u0026gt;n_control)) ip_vs_conn_expire_now(cp); __ip_vs_conn_put(cp); //当开启了net.ipv4.vs.conntrack，SYN数据包会直接丢弃，等待客户端重新发送SYN if (uses_ct) return NF_DROP; //未开启conntrack时，会进入下面ip_vs_try_to_schedule的流程 cp = NULL; } } .... ... } TCP 请求发送第一个 SYN 包被丢弃后，需要等待一个 MSL （60s），客户端会重新发送 SYN。在高并发情况，由于会大量新建连接，会导致出现较多的端口重用情况，就导致连接等待 \u0026gt;=1s 进行重新发送握手包。所以，在上面的压测图中可以看到，connect 阶段最大会达到几秒钟。kubernetes 在1.13 版本开始，对该问题进行了优化，kube-proxy 默认会修改内核参数 net.ipv4.vs.conn_reuse_mode=0 。\n既然从 1.13 开始修改了默认参数，为什么我们的测试环境为 net.ipv4.vs.conn_reuse_mode=1 呢？先看一下，我们出现问题的环境 kubernetes 版本为 1.19.12，os 3.10.0。然后，我们对 1.19.12 k8s 代码进行 review 发现：\n1 2 3 4 5 6 7 8 9 10 11 connReuseMinSupportedKernelVersion = \u0026#34;4.1\u0026#34; ... ... if kernelVersion.LessThan(version.MustParseGeneric(connReuseMinSupportedKernelVersion)) { klog.Errorf(\u0026#34;can\u0026#39;t set sysctl %s, kernel version must be at least %s\u0026#34;, sysctlConnReuse, connReuseMinSupportedKernelVersion) } else { // Set the connection reuse mode if err := utilproxy.EnsureSysctl(sysctl, sysctlConnReuse, 0); err != nil { return nil, err } } ... ... 在 pr#82066 中提到，由于出现 3.10 版本内核中部署 kube-proxy 开启 ipvs 模式后无法启动。社区增加了内核版本的 check，1.19.0 开始如果内核版本 \u0026lt;4.1 则不会修改 net.ipv4.vs.conn_reuse_mode 内核参数。\n另外，上面 issue 中也提到了，主要影响是在大量短连接时会出现端口重用的情况。那如果我们将业务架构改成长连接是否就可以达到一样的效果呢？\n我们后端部署一个简单的 http server 进行压测可以发现，使用长连接的服务即便关闭端口复用 QPS 明显好与优化内核的场景。\n第二个问题，开启 net.ipv4.vs.conn_reuse_mode 参数后，端口重用导致的问题。我们在上文中提到过社区的一个 issue #81775。当开启端口重用，单个客户端使用 vip 发送大量请求，如果某个 pod 销毁会出现 no route to host 报错。\n1 08:50:10 E http_client.go:558\u0026gt; Unable to connect to 10.86.6.96:80 : dial tcp 10.86.6.96:80: connect: no route to host 08:50:11 E http_client.go:558\u0026gt; Unable to connect to 10.86.6.96:80 : dial tcp 10.86.6.96:80: connect: no route to host 08:50:12 E http_client.go:558\u0026gt; Unable to connect to 10.86.6.96:80 : dial tcp 10.86.6.96:80: connect: no route to host 08:50:13 E http_client.go:558\u0026gt; Unable to connect to 10.86.6.96:80 : dial tcp 10.86.6.96:80: connect: no route to host issue 中进行了大量讨论，我在这里只简单分析一下出现该问题的原因。\n在 Kubernetes 1.13 之前，kube-proxy ipvs 模式并不支持优雅删除，当 Endpoint 被删除时，kube-proxy 会直接移除掉 ipvs 中对应的 rs，这样会导致后续的数据包被丢掉。\n在 1.13 版本后，Kubernetes 添加了IPVS 优雅删除的逻辑：\n当 Pod 被删除时，kube-proxy 会先将 rs 的weight置为 0，以防止新连接的请求发送到此 rs，由于不再直接删除 rs，旧连接仍能与 rs 正常通信； 当 rs 的ActiveConn 数量为 0（现在已改为ActiveConn+InactiveConn==0)，即不再有连接转发到此 rs 时，此 rs 才会真正被移除。 上面有提过 InactiveConn 是处于 TIME_WAIT 的连接，那每个处于 InactiveConn 的连接多久会过期呢，默认是120s，通过 ipvsadm -L \u0026ndash;timeout 可以看到默认值：\n正常情况 120s 就将连接在 conntrack 表中删除。但当开启端口重用后，权重修改为 0 的 rs 如果再次被复用，对于端口复用的连接，ipvs 不会主动进行新的调度（调用ip_vs_try_to_schedule方法）；同时，只是将weight置为 0，也并不会触发由expire_nodest_conn 控制的结束连接或 DROP 操作，就这样，新连接的数据包当做什么都没发生一样，发送给了正在删除的 Pod。而这样的一个连接被 ipvs 认为是新的请求，会重置 ipvs timer，也就是说对应的这一个连接需要重新等待 120s 才会被删除。上面提到过，kube-proxy 在 ActiveConn+InactiveConn==0 时才会删除 rs，这样一来，只要不断的有端口复用的连接请求发来，rs 就不会被 kube-proxy 删除，上面提到的优雅删除是无法实现。\n当后端应用进程退出后，后面端口复用的请求，会发送到已经被完全删除的容器 ip 上，就会出现上面的 connect: no route to host 报错。并且这个报错根据 ipvs 另一个参数配置有关：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 static unsigned int ip_vs_in(struct netns_ipvs *ipvs, unsigned int hooknum, struct sk_buff *skb, int af) { ... .... /* * Check if the packet belongs to an existing connection entry */ cp = pp-\u0026gt;conn_in_get(ipvs, af, skb, \u0026amp;iph); //判断是否属于某个已有的connection conn_reuse_mode = sysctl_conn_reuse_mode(ipvs); //当conn_reuse_mode开启，同时出现端口复用（例如收到TCP的SYN包，并且也属于已有的 connection），进行处理 if (conn_reuse_mode \u0026amp;\u0026amp; !iph.fragoffs \u0026amp;\u0026amp; is_new_conn(skb, \u0026amp;iph) \u0026amp;\u0026amp; cp) { bool uses_ct = false, resched = false; //如果开启了expire_nodest_conn、目标rs的weight为0 if (unlikely(sysctl_expire_nodest_conn(ipvs)) \u0026amp;\u0026amp; cp-\u0026gt;dest \u0026amp;\u0026amp; unlikely(!atomic_read(\u0026amp;cp-\u0026gt;dest-\u0026gt;weight))) { resched = true; //查询是否用到了conntrack uses_ct = ip_vs_conn_uses_conntrack(cp, skb); } else if (is_new_conn_expected(cp, conn_reuse_mode)) { //连接是expected的情况，比如FTP uses_ct = ip_vs_conn_uses_conntrack(cp, skb); if (!atomic_read(\u0026amp;cp-\u0026gt;n_control)) { resched = true; } else { /* Do not reschedule controlling connection * that uses conntrack while it is still * referenced by controlled connection(s). */ resched = !uses_ct; } } //如果expire_nodest_conn未开启，并且也非期望连接，实际上直接跳出了 if (resched) { if (!atomic_read(\u0026amp;cp-\u0026gt;n_control)) ip_vs_conn_expire_now(cp); __ip_vs_conn_put(cp); //当开启了net.ipv4.vs.conntrack，SYN数据包会直接丢弃，等待客户端重新发送SYN if (uses_ct) return NF_DROP; //未开启conntrack时，会进入下面ip_vs_try_to_schedule的流程 cp = NULL; } } .... ... } 1 expire_nodest_conn=0 时，当后端 rs 不可达时，ipvs 会直接将数据包丢弃； expire_nodest_conn=1 时，当后端 rs 不可达，立即会返回一个报错给客户端。\n针对这个问题，只在 kubernetes 上并不能完美的解决，例如：kube-router 中增加了优雅下线，会等待 Pod 配置的 TerminationGracePeriodSeconds 后进行删除 rs，这样只能够在一定程度上避免该问题。\n单核性能更优？ 第三个问题，为什么使用 docker run 在线推理业务可以达到 4000 QPS，而 Kubernetes 容器通过内核优化后只能达到 2000？\n针对这个问题，首先和业务方同学进行确认启动 docker 的参数，经过确认发现业务方同学误将 \u0026ndash;cpuset-cpus 当作限制 cpu 使用了，其启动参数命令为：\n1 docker run --cpuset-cpus 4 xxxx 这个启动命令，最终创建的容器其实只使用了一个核，并且将应用进程绑定到了第 4 个 cpu 上。然后，通过修改命令改成非绑核，使用下面命令启动：\n1 docker run --cpu-quota 400000 xxxx 再进行压测 QPS 降低到了 2500，已经和 kubernetes 创建的容器非常接近。由于使用 vip 会经过 ipvs 进行数据包的处理，会有一定的性能损耗，这个结果也比较合理。\n那为什么绑核情况分配1个 cpu 的应用性能会比没有绑核 cpu 会好一倍呢？\n猜测和业务服务逻辑有关，后面还要再进行验证。\n沟通下来确认这个服务业务功能是：首先做少量的数学运算，然后再与通过 grpc 调用后端服务拿到的结果进行计算，将最终的结果返回给客户端。这样的话，这个业务应该算是一个 io 密集型应用，并不需要较高的 cpu 的， 绑核后能够减少 cpu 之间频繁的上下文切换，从而带来更好的效果。\n总结 通过修改 ipvs 内核参数，协助联邦同学解决了遇到的吞吐率问题，将 QPS 从 200 提升到了 2000+。\n然后，我们并不是止步于解决问题，对问题过程中遇到的疑惑进一步研究，帮助我们能够对 kubernetes 系统有全面的把控。\n在 1.19.0 版本开始 kubernetes 对 ipvs 默认内核参数进行了改进，当内核版本 \u0026lt;4.1 时，kube-proxy 不会修改 ipvs 内核参数 net.ipv4.vs.conn_reuse_mode。 通过修改内核参数提高了吞吐率，但同时带来了优雅下线的问题，在 5.9 开始 linux 内核层面已有解决。另外，我们针对可以对业务架构优化的场景使用长连接方式进行压测，能够显著的解决吞吐率降低的问题。 对于 docker run 场景 QPS 是 Kubernetes 容器的两倍问题，我们发现业务同学使用 docker 运行时使用了 \u0026ndash;cpuset-cpus 参数，也就是将应用绑定到某一个 cpu 核上。这说明对于某些应用并不是分配的应用 cpu 越多，性能越好。 目前，发现问题的主要场景是在使用官方默认网络组件 kube-proxy 带来的问题。而我们 IDC 内部使用 kube-router 使用了和 Pod TerminationGracePeriodSeconds 一致的等待时间来优雅删除 rs。\n另外，针对 ipvs 性能社区也有一些使用 eBPF 来实现的解决方案，例如：Cillium、腾讯 ipvs-ebpf 等。\n参考 https://marc.info/?l=linux-virtual-server\u0026m=151683112005533\u0026w=2 IPVS low throughput 解决关闭端口复用出现 1s 延迟的 patch 开启端口复用后 rs 下线导致后端不可用问题 绕过conntrack，使用eBPF增强 IPVS优化K8s网络性能 ","date":"2022-01-05T00:00:00Z","image":"https://jasonrd.github.io/blog/p/ipvs-caused-problem/matt-le-SJSpo9hQf7s-unsplash_hu958d513eeefe5556a31d065479ecc5ac_14205_120x120_fill_q75_box_smart1.jpg","permalink":"https://jasonrd.github.io/blog/p/ipvs-caused-problem/","title":"k8s 环境中 ipvs 带来的问题"},{"content":"背景 最近业务应用使用 Service ip 进行压测时，当容器销毁时，部分请求会出现 connect refused 错误。按照文档 云上 pod 下线引起短时服务不可用 进行优雅下线优化后有一定改善，但是仍然存在 connect refused 异常。本文通过分析 kube-router 实现 vip 的逻辑，进行定位问题根因，并举一反三对 kubernetes 默认组件 kube-proxy service IP 实现进行研究和分析。\nIPVS VIP 实现 ipvs (IP Virtual Server) 是工作在内核态的4层负载均衡，也就是我们常说的4层LAN交换，作为 Linux 内核的一部分。ipvs运行在主机上，在真实服务器集群前充当负载均衡器。ipvs可以将基于TCP和UDP的服务请求转发到真实服务器上，从而达到通过单个 VIP 代理多个后端真实服务的目的。IPVS 和 iptables 一样都是基于内核底层 netfilter 实现，netfilter 主要通过各个链的钩子实现包处理和转发。\nipvs 作为内核中负载均衡，有多种负载策略：rr（轮询）、wrr（加权轮询）、sh（源地址哈希）等，默认使用 rr 模式。vip 后面关联多个 pod ip， 通过 VIP 请求后 ipvs 会根据配置的均衡策略选取其中一个 pod ip 进行流量转发。\n如上图，使用 ipvsadm 工具查看 10.59.38.148:8001 转发的 RS（Real Server） 有两个：10.60.10.7、10.60.14.8，转发策略为 RR，其中 Weight 标识每个 RS 的权重。\n当 Weight=0 时，新连接不会转发到该 RS；但是，已建立的连接仍会保持，直到连接释放。ActiveConn 是活动连接数，也就是 tcp 连接状态的 ESTABLISHED；InActConn 是指除了 ESTABLISHED 以外的，所有的其它状态的 tcp 连接。\n常用开源组件实现逻辑 kube-router 先已公司使用 cni 插件为 kube-router 作为研究对象，通过使用部署一个简单的 go http server 两副本应用，观察删除一个 Pod 后，会发生哪些变化：\n上面两张图，分别对应 http server 在是否处理 SIGTERM 信号场景下对 ipvs 更新的影响。\n上图可以，看出当删除 Pod 时，在 20s+ 以后 kube-router 才会将被删除的 pod IP 在 ipvs 中摘掉。而摘掉流量方式是直接在 ipvs 中删除对应的 RS (10.60.10.7)：\n==》\n这样，在两种场景下会出现异常：\npod 容器已完全删除，但是 ipvs 还会转发流量到被删除的 Pod IP 上。也就容器完全删除早于 ipvs 中 RS 的删除动作； Ipvs 摘掉pod 流量时，存在未释放的连接。也就是 ipvs 中 RS 删除早于 pod 销毁，并且存在持久化的会话； 第一种问题还是未做好优雅下线导致的，可以通过 prestop 来增加优雅下线，提前将连接释放掉。但是，由于删除后 20s 仍然又新连接进来，虽然解决了第一个场景问题，但是第二个场景问题还是会存在的。\n在上述业务压测出现问题的场景中：\n业务容器已经做了优雅下线（等待15s+业务层开始优雅下线逻辑），整个优雅下线时间\u0026gt;=15s； 修改业务容器 terminatedGracePeriod 时长为 120s 后，我们抓包发现没有新流量进入，但仍然存在 connect refused 异常； 出现问题场景，属于第二种。那第二种问题如何解决呢？回答这个问题前，先要弄清楚下面疑问：\n首先，为什么 ipvs 在 20s 后才会将 pod ip 摘掉； 之前有了解过 ipvs 中 RS weight=0 时，新流量不会转发到该 RS，那 kube-router 有没有实现这个逻辑呢？ 带着上面两个问题，我们来看一下 kube-router 源码。\n源码分析 kube-router 内部会通过 list/watch 来监听 endpoint 和 service 更新，当 endpoint 发生变更（pod 删除）\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 // proxy/network_services_controller.go func (nsc *NetworkServicesController) OnEndpointsUpdate(ep *api.Endpoints) { ... ... // build new service and endpoints map to reflect the change newServiceMap := nsc.buildServicesInfo() newEndpointsMap := nsc.buildEndpointsInfo() if len(newEndpointsMap) != len(nsc.endpointsMap) || !reflect.DeepEqual(newEndpointsMap, nsc.endpointsMap) { nsc.endpointsMap = newEndpointsMap nsc.serviceMap = newServiceMap glog.V(1).Infof(\u0026#34;Syncing IPVS services sync for update to endpoint: %s/%s\u0026#34;, ep.Namespace, ep.Name) nsc.sync(synctypeIpvs) } else { glog.V(1).Infof(\u0026#34;Skipping IPVS services sync on endpoint: %s/%s update as nothing changed\u0026#34;, ep.Namespace, ep.Name) } } // OnServiceUpdate handle change in service update from the API server func (nsc *NetworkServicesController) OnServiceUpdate(svc *api.Service) { ... ... // build new service and endpoints map to reflect the change newServiceMap := nsc.buildServicesInfo() newEndpointsMap := nsc.buildEndpointsInfo() if len(newServiceMap) != len(nsc.serviceMap) || !reflect.DeepEqual(newServiceMap, nsc.serviceMap) { nsc.endpointsMap = newEndpointsMap nsc.serviceMap = newServiceMap glog.V(1).Infof(\u0026#34;Syncing IPVS services sync on update to service: %s/%s\u0026#34;, svc.Namespace, svc.Name) nsc.sync(synctypeIpvs) } else { glog.V(1).Infof(\u0026#34;Skipping syncing IPVS services for update to service: %s/%s as nothing changed\u0026#34;, svc.Namespace, svc.Name) } } 可以看到 endpoint 和 service 更新，都是先更新 nsc 的 endpointMap 和 serviceMap，然后执行 nsc.sync 函数。nsc.sync 函数将变更类型同步到 syncChan：\n1 2 3 4 5 6 7 8 //proxy/network_services_controller.go func (nsc *NetworkServicesController) sync(syncType int) { select { case nsc.syncChan \u0026lt;- syncType: default: glog.V(2).Infof(\u0026#34;Already pending sync, dropping request for type %d\u0026#34;, syncType) } } syncChan 在 NetworkServiceController 的 Run 函数中进行监听：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 // Run periodically sync ipvs configuration to reflect desired state of services and endpoints func (nsc *NetworkServicesController) Run(healthChan chan\u0026lt;- *healthcheck.ControllerHeartbeat, stopCh \u0026lt;-chan struct{}, wg *sync.WaitGroup) { ...... select { case \u0026lt;-stopCh: glog.Info(\u0026#34;Shutting down network services controller\u0026#34;) return default: // kube-router 默认启动时，先更新一遍 ipvs err := nsc.doSync() if err != nil { glog.Fatalf(\u0026#34;Failed to perform initial full sync %s\u0026#34;, err.Error()) } nsc.readyForUpdates = true } // loop forever until notified to stop on stopCh for { select { case \u0026lt;-stopCh: nsc.mu.Lock() nsc.readyForUpdates = false nsc.mu.Unlock() glog.Info(\u0026#34;Shutting down network services controller\u0026#34;) return case \u0026lt;-gracefulTicker.C: if nsc.readyForUpdates \u0026amp;\u0026amp; nsc.gracefulTermination { glog.V(3).Info(\u0026#34;Performing periodic graceful destination cleanup\u0026#34;) nsc.gracefulSync() } // 从 syncChan 唤醒协程执行 ipvs 的更新/同步逻辑 case perform := \u0026lt;-nsc.syncChan: healthcheck.SendHeartBeat(healthChan, \u0026#34;NSC\u0026#34;) switch perform { case synctypeAll: glog.V(1).Info(\u0026#34;Performing requested full sync of services\u0026#34;) err := nsc.doSync() if err != nil { glog.Errorf(\u0026#34;Error during full sync in network service controller. Error: \u0026#34; + err.Error()) } case synctypeIpvs: glog.V(1).Info(\u0026#34;Performing requested sync of ipvs services\u0026#34;) nsc.mu.Lock() // ipvs 的更新/同步逻辑 err := nsc.syncIpvsServices(nsc.serviceMap, nsc.endpointsMap) nsc.mu.Unlock() if err != nil { glog.Errorf(\u0026#34;Error during ipvs sync in network service controller. Error: \u0026#34; + err.Error()) } } if err == nil { healthcheck.SendHeartBeat(healthChan, \u0026#34;NSC\u0026#34;) } case \u0026lt;-t.C: glog.V(1).Info(\u0026#34;Performing periodic sync of ipvs services\u0026#34;) healthcheck.SendHeartBeat(healthChan, \u0026#34;NSC\u0026#34;) err := nsc.doSync() if err != nil { glog.Errorf(\u0026#34;Error during periodic ipvs sync in network service controller. Error: \u0026#34; + err.Error()) glog.Errorf(\u0026#34;Skipping sending heartbeat from network service controller as periodic sync failed.\u0026#34;) } else { healthcheck.SendHeartBeat(healthChan, \u0026#34;NSC\u0026#34;) } } } } 更新 ipvs 函数 nsc.syncIpvsServices ：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 // proxy/service_endpoints_sync.go // sync the ipvs service and server details configured to reflect the desired state of Kubernetes services // and endpoints as learned from services and endpoints information from the api server func (nsc *NetworkServicesController) syncIpvsServices(serviceInfoMap serviceInfoMap, endpointsInfoMap endpointsInfoMap) error { start := time.Now() defer func() { endTime := time.Since(start) if nsc.MetricsEnabled { metrics.ControllerIpvsServicesSyncTime.Observe(endTime.Seconds()) } glog.V(1).Infof(\u0026#34;sync ipvs services took %v\u0026#34;, endTime) }() var err error var syncErrors bool // map to track all active IPVS services and servers that are setup during sync of // cluster IP, nodeport and external IP services activeServiceEndpointMap := make(map[string][]string) // 配置 VIP ipvs err = nsc.setupClusterIPServices(serviceInfoMap, endpointsInfoMap, activeServiceEndpointMap) if err != nil { syncErrors = true glog.Errorf(\u0026#34;Error setting up IPVS services for service cluster IP\u0026#39;s: %s\u0026#34;, err.Error()) } // 配置 nodeport ipvs err = nsc.setupNodePortServices(serviceInfoMap, endpointsInfoMap, activeServiceEndpointMap) if err != nil { syncErrors = true glog.Errorf(\u0026#34;Error setting up IPVS services for service nodeport\u0026#39;s: %s\u0026#34;, err.Error()) } err = nsc.setupExternalIPServices(serviceInfoMap, endpointsInfoMap, activeServiceEndpointMap) if err != nil { syncErrors = true glog.Errorf(\u0026#34;Error setting up IPVS services for service external IP\u0026#39;s and load balancer IP\u0026#39;s: %s\u0026#34;, err.Error()) } // 清理过期 vip err = nsc.cleanupStaleVIPs(activeServiceEndpointMap) if err != nil { syncErrors = true glog.Errorf(\u0026#34;Error cleaning up stale VIP\u0026#39;s configured on the dummy interface: %s\u0026#34;, err.Error()) } // 清理过期 RS IP err = nsc.cleanupStaleIPVSConfig(activeServiceEndpointMap) if err != nil { syncErrors = true glog.Errorf(\u0026#34;Error cleaning up stale IPVS services and servers: %s\u0026#34;, err.Error()) } err = nsc.syncIpvsFirewall() if err != nil { syncErrors = true glog.Errorf(\u0026#34;Error syncing ipvs svc iptables rules to permit traffic to service VIP\u0026#39;s: %s\u0026#34;, err.Error()) } err = nsc.setupForDSR(serviceInfoMap) if err != nil { syncErrors = true glog.Errorf(\u0026#34;Error setting up necessary policy based routing configuration needed for direct server return: %s\u0026#34;, err.Error()) } if syncErrors { glog.V(1).Info(\u0026#34;One or more errors encountered during sync of IPVS services and servers to desired state\u0026#34;) } else { glog.V(1).Info(\u0026#34;IPVS servers and services are synced to desired state\u0026#34;) } return nil } 每次有 endpoint/service 发生变化，nsc.syncIpvsServices 都会将所有ipvs 更新一遍，整个函数执行时间根据 service 数量不同执行时间不同。在我们线下环境有 3200 个 service，函数执行时间 30s+。\n这就解答了「为什么 ipvs 在 20s 后才会将 pod ip 摘掉」。\n那上面说的设置 ipvs rs weight 来停止新连接的转发，在 kube-router 中有没有实现呢？我们在阅读源码时，发现下面这一部分代码：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 // Run periodically sync ipvs configuration to reflect desired state of services and endpoints func (nsc *NetworkServicesController) Run(healthChan chan\u0026lt;- *healthcheck.ControllerHeartbeat, stopCh \u0026lt;-chan struct{}, wg *sync.WaitGroup) { ...... // loop forever until notified to stop on stopCh for { select { case \u0026lt;-stopCh: nsc.mu.Lock() nsc.readyForUpdates = false nsc.mu.Unlock() glog.Info(\u0026#34;Shutting down network services controller\u0026#34;) return case \u0026lt;-gracefulTicker.C: if nsc.readyForUpdates \u0026amp;\u0026amp; nsc.gracefulTermination { glog.V(3).Info(\u0026#34;Performing periodic graceful destination cleanup\u0026#34;) nsc.gracefulSync() } ...... } NSC 中存在一个 graceful 定时器，触发时会执行 nsc.gracefulSync 函数：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 // proxy/network_service_graceful.go func (nsc *NetworkServicesController) gracefulSync() { nsc.gracefulQueue.mu.Lock() defer nsc.gracefulQueue.mu.Unlock() var newQueue []gracefulRequest // Itterate over our queued destination removals one by one, and don\u0026#39;t add them back to the queue if they were processed for _, job := range nsc.gracefulQueue.queue { if removed := nsc.gracefulDeleteIpvsDestination(job); removed { continue } newQueue = append(newQueue, job) } nsc.gracefulQueue.queue = newQueue } func (nsc *NetworkServicesController) gracefulDeleteIpvsDestination(req gracefulRequest) bool { var deleteDestination bool // Get active and inactive connections for the destination aConn, iConn, err := nsc.getIpvsDestinationConnStats(req.ipvsSvc, req.ipvsDst) if err != nil { glog.V(1).Infof(\u0026#34;Could not get connection stats for destination: %s\u0026#34;, err.Error()) } else { // Do we have active or inactive connections to this destination // if we don\u0026#39;t, proceed and delete the destination ahead of graceful period if aConn == 0 \u0026amp;\u0026amp; iConn == 0 { deleteDestination = true } } // Check if our destinations graceful termination period has passed if time.Since(req.deletionTime) \u0026gt; req.gracefulTerminationPeriod { deleteDestination = true } //Destination has has one or more conditions for deletion if deleteDestination { glog.V(2).Infof(\u0026#34;Deleting IPVS destination: %s\u0026#34;, ipvsDestinationString(req.ipvsDst)) if err := nsc.ln.ipvsDelDestination(req.ipvsSvc, req.ipvsDst); err != nil { glog.Errorf(\u0026#34;Failed to delete IPVS destination: %s, %s\u0026#34;, ipvsDestinationString(req.ipvsDst), err.Error()) } } return deleteDestination } nsc.gracefulSync 函数主要逻辑是对 ipvs 中 RS 进行删除，判断逻辑如下：\nInActConn 和 ActConn 都为0，则说明没有连接存在，可以直接删除； 如果 InActConn 或 ActConn 不为零，但是 pod 删除等待时长已经超过 pod.gracefulTerminationPeriod，则可以直接删除； 否则，等待下一次进行； 那 nsc.gracefulQueue 队列是在哪里写入的呢？我们之间检索 gracefulQueue 字段，发现在函数 nsc.ipvsDeleteDestination 中写入\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 // proxy/network_service_graceful.go func (nsc *NetworkServicesController) ipvsDeleteDestination(svc *ipvs.Service, dst *ipvs.Destination) error { // If we have enabled graceful termination set the weight of the destination to 0 // then add it to the queue for graceful termination if nsc.gracefulTermination { req := gracefulRequest{ ipvsSvc: svc, ipvsDst: dst, deletionTime: time.Now(), } dst.Weight = 0 err := nsc.ln.ipvsUpdateDestination(svc, dst) if err != nil { return err } nsc.addToGracefulQueue(\u0026amp;req) } else { err := nsc.ln.ipvsDelDestination(svc, dst) if err != nil { return err } } // flush conntrack when Destination for a UDP service changes if svc.Protocol == syscall.IPPROTO_UDP { if err := nsc.flushConntrackUDP(svc); err != nil { glog.Errorf(\u0026#34;Failed to flush conntrack: %s\u0026#34;, err.Error()) } } return nil } nsc.ipvsDeleteDestination 判断 nsc.gracefulTermination 如果开启，则不会立即删除。而是，执行下面操作：\n更新 ipvs 配置，设置 RS weight=0； 加入 gracefulQueue 队列中，等待删除； 综上，可以看出 nsc.gracefulTermination 就是开启 ipvs 优雅下线的开关，而这个是通过参数 \u0026ndash;ipvs-graceful-termination 来控制的。然后，我们开启 ipvs-graceful-termination 进行测试。\n我们对测试应用 http-server 增加优雅下线逻辑：\n捕获 SIGTERM 信号； 捕获到 SIGTERM 信号后，在 http header 中增加 connection: close，也就是在容器下线阶段使用短连接； 然后再进行压测：\n=\u0026gt;=\u0026gt;\n=\u0026gt;\n上面分别对应，销毁 Pod 前、销毁 Pod 过程中1、销毁 Pod 过程中2、Pod 被完全销毁。\n通过开启 ipvs graceful terminated，并且容器销毁后应用捕获 SIGTERM 信号进行连接的优雅下线，测试中未出现请求错误的问题。\n总结 对以上源码研究，进行总结如下：\nkube-router 每次更新都是全量更新，service 数量不同 ipvs RS 新增、更新、删除的延迟不同； 具有优雅删除 RS 的能力，并且结合了 Pod terminatedGracefulPeriod 进行 RS 的完全删除； 为开启 \u0026ndash;ipvs-graceful-termination，会立即删除 RS，但是由于全量更新的延迟，表现上是有一定延迟（线下 30s+、线上15s+）； kube-proxy 实现逻辑 根据 1.22 版本 kube-proxy 代码，pod 删除过程：\n销毁 Pod，Endpoint 中移除销毁中的 Pod IP； kube-proxy watch 到 endpoint 发生变化，进入 ipvs 删除判断逻辑： 协议不是 UDP 或 SFTP； ActConn 和 InActConn 都为0； 不满足删除判断，则更新 ipvs 中 RS weight=0; 然后，将 RS 加入到本地队列，每隔 1min 进行删除判断； 源码 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 // proxy/ipvs/graceful_termination.go // GracefulDeleteRS to update rs weight to 0, and add rs to graceful terminate list func (m *GracefulTerminationManager) GracefulDeleteRS(vs *utilipvs.VirtualServer, rs *utilipvs.RealServer) error { // Try to delete rs before add it to graceful delete list ele := \u0026amp;listItem{ VirtualServer: vs, RealServer: rs, } deleted, err := m.deleteRsFunc(ele) if err != nil { klog.Errorf(\u0026#34;Delete rs %q err: %v\u0026#34;, ele.String(), err) } if deleted { return nil } rs.Weight = 0 err = m.ipvs.UpdateRealServer(vs, rs) if err != nil { return err } klog.V(5).Infof(\u0026#34;Adding an element to graceful delete rsList: %+v\u0026#34;, ele) m.rsList.add(ele) return nil } func (m *GracefulTerminationManager) deleteRsFunc(rsToDelete *listItem) (bool, error) { klog.V(5).Infof(\u0026#34;Trying to delete rs: %s\u0026#34;, rsToDelete.String()) rss, err := m.ipvs.GetRealServers(rsToDelete.VirtualServer) if err != nil { return false, err } for _, rs := range rss { if rsToDelete.RealServer.Equal(rs) { // For UDP and SCTP traffic, no graceful termination, we immediately delete the RS // (existing connections will be deleted on the next packet because sysctlExpireNoDestConn=1) // For other protocols, don\u0026#39;t delete until all connections have expired) if utilipvs.IsRsGracefulTerminationNeeded(rsToDelete.VirtualServer.Protocol) \u0026amp;\u0026amp; rs.ActiveConn+rs.InactiveConn != 0 { klog.V(5).Infof(\u0026#34;Not deleting, RS %v: %v ActiveConn, %v InactiveConn\u0026#34;, rsToDelete.String(), rs.ActiveConn, rs.InactiveConn) return false, nil } klog.V(5).Infof(\u0026#34;Deleting rs: %s\u0026#34;, rsToDelete.String()) err := m.ipvs.DeleteRealServer(rsToDelete.VirtualServer, rs) if err != nil { return false, fmt.Errorf(\u0026#34;Delete destination %q err: %v\u0026#34;, rs.String(), err) } return true, nil } } return true, fmt.Errorf(\u0026#34;Failed to delete rs %q, can\u0026#39;t find the real server\u0026#34;, rsToDelete.String()) } kube-proxy GracefulDeleteRS 函数会 deleteRsFunc 进行 RS 删除，删除 RS 条件：\n协议不是 UDP 或 SFTP； ActConn 和 InActConn 都为0； 否则，更新 ipvs 将 RS weight 置为 0，不接收新流量进入。然后会加入到带删除列表中，进行定期（间隔1min）清理：\n1 2 3 4 5 6 7 8 9 10 func (m *GracefulTerminationManager) tryDeleteRs() { if !m.rsList.flushList(m.deleteRsFunc) { klog.Errorf(\u0026#34;Try flush graceful termination list err\u0026#34;) } } // Run start a goroutine to try to delete rs in the graceful delete rsList with an interval 1 minute func (m *GracefulTerminationManager) Run() { go wait.Until(m.tryDeleteRs, rsCheckDeleteInterval, wait.NeverStop) } 优化方案 优点 缺点 方案一 kube-router 开启 \u0026ndash;ipvs-graceful-termination 参数 方案二 开发内网负载均衡组件 参考 http://www.dockone.io/article/9441 https://wsgzao.github.io/post/lvs-nat/ Issue 572 - Graceful termination kube-proxy ipvs support graceful termination ","date":"2021-12-19T00:00:00Z","image":"https://jasonrd.github.io/blog/p/how-endpoint-flush-ipvs/the-creative-exchange-d2zvqp3fpro-unsplash_huf941de4769045cdfa8c9ee7036519a2a_35369_120x120_fill_q75_box_smart1.jpg","permalink":"https://jasonrd.github.io/blog/p/how-endpoint-flush-ipvs/","title":"endpoint 更新后 vip 转发实现探究"},{"content":"背景 近日，接连收到多个云上站点业务出现 502 问题反馈：\n和业务负责人沟通后，应用确认加入了优雅下线逻辑。\n排查过程 首先，查看网关日志：\n两次日志，都是请求 LB IP 出现 503 错误码后，然后网关将 LB IP 摘掉。\n分析为什么出现 503 错误码前，先了解一下容器下线逻辑：容器进行下线时，会调用 prestop 脚本执行下线前的操作。\n在 prestop 脚本中，首先 sleep 15s （不要问我为什么），然后调用 http://127.0.0.1:${APP_PORT}/ok.htm?down=true 接口通知 java 进程进行优雅下线。该接口调用成功后，再请求应用 ok 页面，进入下面逻辑：\n也就是返回 halting 数据和503状态码。\nk8s 中在将 Pod 进行下线（标记为 Terminating 状态）时，k8s endpoint controller 就将该 Pod ip 从 lb 或 service 后端列表中摘除。既然 lb/svc 已经将在该 pod IP 摘除，为什么仍然请求到 halting Pod 呢？\n在进入应用容器中进行抓包，并和应用负责人确认后，网关 -\u0026gt; lb -\u0026gt; pod 是使用 http 长连接方式。\n在 Pod 处于 terminating 状态时，通过 svc 请求时新建立的连接将不会转发到该 pod，但是已经建立的连接在 Pod 完全删除前仍可继续通信。所以，虽然 service 将 Pod IP 摘除，但是为了保证容器的优雅下线，已经建立的连接仍然可以继续处理业务，直到容器彻底被删除。\n我们用 python 简单写了一个使用 http 长连接客户端，进行一下测试：\n1 2 3 4 5 6 7 8 9 10 import requests import time client=requests.session() headers = {\u0026#39;Content-Type\u0026#39;: \u0026#39;application/json\u0026#39;, \u0026#39;Connection\u0026#39;: \u0026#39;keep-alive\u0026#39;} while True: r=client.get(\u0026#34;http://xx.xx.16.137:8088/ok.htm\u0026#34;, headers=headers) # xx.xx.16.137 为应用 lb IP print r.status_code print r.content time.sleep(1) python client 请求应用容器：\n上面可以看到，在 15:30:06 将 pod 进行 kill 后，通过长连接仍然可以将请求转发到处于 terminating 的应用容器。直到 15s 后调用下线接口，请求返回 503。请求处于 halting 状态的 pod，server 端会主动 close 请求：\n另外，我们测试应用只有单个实例，也就是实例被删除后，lb 后端实例为 0，请求 lb 的新连接无法建立。所以，测试脚本会出现 connect refused 报错。\n解决方法 综上可知，问题原因是 kill pod 后仍然会有流量进入到 terminating 状态的 pod，然后 15s 后 prestop 脚本通知进程进行下线逻辑（也就是 ok 页面返回 halting 和 503 状态码），当网关继续请求到该 pod 就会认为 lb 出现异常，将唯一的 lb 标记为不健康，从而出现 502 异常。\n其他站点未出现该问题原因是：网关直接转发到 pod ip，摘掉的是出现异常的 pod ip 。而有问题的站点对接的只是一个公有云 LB ip。\n具体优化逻辑如下(如上图，针对单个pod):\n应用增加 connection filter，在应用进入优雅下线（被调用 http://127.0.0.1:${APP_PORT}/ok.htm?down=true）后，所有请求的 http 响应头中增加 connection:close（也就是使用短连接）； 因为 pod 处于 terminating 时，新连接不会进入该 pod； 参考代码如下： 健康检查接口，修改为去掉 503 异常，避免网关检测到 503 异常时，直接摘掉 lb ip。 参考代码如下： ","date":"2021-11-01T00:00:00Z","image":"https://jasonrd.github.io/blog/p/service-cause-failure/luca-bravo-alS7ewQ41M8-unsplash_hu0a3f1163de68d0b9471979ebf0ecf11e_32400_120x120_fill_q75_box_smart1.jpg","permalink":"https://jasonrd.github.io/blog/p/service-cause-failure/","title":"pod 销毁过程引起短时服务不可用"},{"content":"问题背景 我们在 kubernetes 生产环境上了 GPU 虚拟化后，对原生的 kube-scheduler 进行了扩展，支持 GPU 虚拟化相关调度算法。在 GPU 扩展调度组件上线后，部署推理 业务容器时，出现了 pod 一直处于 Pending 状态。查看 Pending Pod 的 event 日志，提示 pod 资源不足而导致 pod 调度失败。\n查看 event 日志，可以看出是节点 cpu 资源不足导致。但是，我们通过 describe gpu node 返回结果可以看到 GPU、CPU、memory 等资源都非常充足。\n并且，非 gpu 节点并没有发现类似的问题。\n问题定位 为了定位是否是引入 GPU 扩展调度组件导致的，我们修改了 gpu 容器 request 资源，去掉虚拟化 gpu 的配额并且强制调度到 218.27 这台node，容器仍然 Pending。\n到现在仍然不能排除是增加 GPU 扩展调度组件引起的问题，虽然这个组件在上线前在线下做了较久时间的验证都没有出现该问题。我们在 playbox 环境中环境尝试复现该问题，经过多次同样问题出现了。为了定位到具体原因，我们将 kube-scheduler 的日志级别调到最高，希望通过观察日志能够定位到问题的根源。\n我们发现扩容时存在某些 pod 会出现两次调度的情况，pod 虽然最终调度成功并且处于 running 状态，但是仍然出现过调度失败的日志：\n1 I0624 13:11:39.054874 7225 event.go:209] Event(v1.ObjectReference{Kind:\u0026#34;Pod\u0026#34;, Namespace:\u0026#34;gpu\u0026#34;, Name:\u0026#34;gpu-work-85bb88797b-7zf85\u0026#34;, UID:\u0026#34;22f6a277-2812-49af-b12f-6759e47920d4\u0026#34;, APIVersion:\u0026#34;v1\u0026#34;, ResourceVersion:\u0026#34;11045409\u0026#34;, FieldPath:\u0026#34;\u0026#34;}): t ype: \u0026#39;Warning\u0026#39; reason: \u0026#39;FailedScheduling\u0026#39; 0/5 nodes are available: 4 Insufficient xxxx-core, 4 Insufficient xxx-memory. 这给我们带来了以下疑问：\n同一个 pod 为什么会出现两次调度？ 为什么 pod 调度成功，也出现了调度失败的事件呢？ 其他 pod 调度失败是否和 pod 出现两次调度有关系？ 原因排查 一个 pod 为什么会出现两次调度？ 我们只使用了 kube-scheduler 的 predicate extender，pod 在 k8s scheduler 默认的 predicate 流程执行完成后，会回调 GPU 扩展调度组件完成 GPU 辅助调度，最后优选最优的 node 节点。pod 在回调时，GPU 扩展调度组件会将分配 gpu 设备号和调度的 node 等信息 patch 到 pod annotations 中：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 func (gpuFilter *GPUFilter) deviceFilter( ...... annotationMap := make(map[string]string) for k, v := range newPod.Annotations { if strings.Contains(k, util.GPUAssigned) || strings.Contains(k, util.PredicateTimeAnnotation) || strings.Contains(k, util.PredicateGPUIndexPrefix) || strings.Contains(k, util.PredicateNode) { annotationMap[k] = v } } err := gpuFilter.patchPodWithAnnotations(newPod, annotationMap) if err != nil { failedNodesMap[node.Name] = \u0026#34;update pod annotation failed\u0026#34; continue } .... } 然后，kube-scheduler watch 到 pod update 事件后，会将 pod 重新加入到调度队列：\n1 2 3 4 5 6 7 8 9 func (sched *Scheduler) updatePodInSchedulingQueue(oldObj, newObj interface{}) { pod := newObj.(*v1.Pod) if sched.skipPodUpdate(pod) { return } if err := sched.config.SchedulingQueue.Update(oldObj.(*v1.Pod), pod); err != nil { utilruntime.HandleError(fmt.Errorf(\u0026#34;unable to update %T: %v\u0026#34;, newObj, err)) } } 虽然上么有 skipPodUpdate 这个逻辑，但是实际上更新 annotation 后这一步被跳过了，具体后面再讲。\n为什么 pod 调度成功，也出现了调度失败的事件呢？ 更新后的 podA 重新进入调度队列后，相当于 podA 经过了两次调度循环 A 和 B。由于调度队列的优先级算法，在没有优先级更高的 pod 被创建的情况下，A、B 两个循环是相邻的。\n在 node 资源充足的情况下，一个调度循环大概流程：\nscheduling queue pop \u0026mdash;\u0026gt; schedule \u0026mdash;-\u0026gt; predicate \u0026mdash;-\u0026gt; call predicate extender \u0026mdash;\u0026ndash;\u0026gt; priority \u0026mdash;\u0026mdash;\u0026gt; asynchronous binding\n上图为调度的基本流程。调度器在回调 GPU 扩展调度组件时 pod 属性被修改，update 事件进入 kube-scheduler pod 被重新加入调度队列。pod 在第二次调度过程，会再次回调 GPU 扩展调度组件，我们查看 GPU 扩展调度组件日志，在第二次调度时 response 返回的 filteredNode 为空：\nI0628 03:46:20.957317 1 routes.go:81] GPUQuotaPredicate: extenderFilterResult = {\u0026ldquo;Nodes\u0026rdquo;:null,\u0026ldquo;NodeNames\u0026rdquo;:null,\u0026ldquo;FailedNodes\u0026rdquo;:null,\u0026ldquo;Error\u0026rdquo;:\u0026quot;\u0026quot;}\n从而 predicate 过程预选可用 node 集合为空，从而报出 0/5 nodes are available: 4 Insufficient xxx-core, 4 Insufficient xxx-memory 错误日志。我们 GPU 扩展调度组件代码中，如果存在 gpu annotations 时会直接返回一个空列表：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 //deviceFilter will choose one and only one node fullfil the request, //so it should always be the last filter of gpuFilter func (gpuFilter *GPUFilter) deviceFilter( pod *corev1.Pod, nodes []corev1.Node) ([]corev1.Node, schedulerapi.FailedNodesMap, error) { // #lizard forgives var ( filteredNodes = make([]corev1.Node, 0) failedNodesMap = make(schedulerapi.FailedNodesMap) nodeInfoList []*device.NodeInfo success bool sorter = device.NodeInfoSort( device.ByAllocatableCores, device.ByAllocatableMemory, device.ByID) ) for k := range pod.Annotations { if strings.Contains(k, util.GPUAssigned) || strings.Contains(k, util.PredicateTimeAnnotation) || strings.Contains(k, util.PredicateGPUIndexPrefix) { return filteredNodes, failedNodesMap, nil } } ...... } 资源充足调度失败原因 通过上面两小节的分析，我们大致了解了场景触发前提：\nGPU 扩展调度组件在 kube-scheduler 回调时（调度循环期间），会同时 patch pod annotations 属性； pod 第二次调度循环， GPU 扩展调度组件 response 返回空调度队列； 然后，我们对 kube-scheduler 代码进行阅读和调试分析，发现 kube-scheduler 存在一个内存泄漏的bug。\n我们知道，通常情况下 predicate 阶段判断调度失败，kube-scheduler 会进行以下操作：\n将 pod 入队 unschedulableQ 从而在下一个调度循环进行调度； 进入 preempt 流程，如果有可抢占的 node ，则标记 pod nominatedName 为被抢占的 node; 另外，nominatedPodMap 中记录的 pod 只在调度成功后 assumed 阶段会被清理。而 assumed 只有在调度循环成功，进入到 bind 阶段前会执行。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 // scheduler.go func (sched *Scheduler) scheduleOne() { ... ... // assume modifies `assumedPod` by setting NodeName=scheduleResult.SuggestedHost err = sched.assume(assumedPod, scheduleResult.SuggestedHost) if err != nil { klog.Errorf(\u0026#34;error assuming pod: %v\u0026#34;, err) metrics.PodScheduleErrors.Inc() return } // bind the pod to its host asynchronously (we can do this b/c of the assumption step above). go func() { // Bind volumes first before Pod ... ... err := sched.bind(assumedPod, \u0026amp;v1.Binding{ ObjectMeta: metav1.ObjectMeta{Namespace: assumedPod.Namespace, Name: assumedPod.Name, UID: assumedPod.UID}, Target: v1.ObjectReference{ Kind: \u0026#34;Node\u0026#34;, Name: scheduleResult.SuggestedHost, }, }) ... ... } ... ... } 如果，pod 调度失败但是并没有入队 unschedulableQ，则 nominated pod map 就存在资源泄漏的问题。\n我们看一下 unschedulableQ 入队代码：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 ... ... defer runtime.HandleCrash() podID := types.NamespacedName{ Namespace: pod.Namespace, Name: pod.Name, } // When pod priority is enabled, we would like to place an unschedulable // pod in the unschedulable queue. This ensures that if the pod is nominated // to run on a node, scheduler takes the pod into account when running // predicates for the node. if !util.PodPriorityEnabled() { if !backoff.TryBackoffAndWait(podID, stopEverything) { klog.Warningf(\u0026#34;Request for pod %v already in flight, abandoning\u0026#34;, podID) return } } // Get the pod again; it may have changed/been scheduled already. getBackoff := initialGetBackoff for { pod, err := client.CoreV1().Pods(podID.Namespace).Get(podID.Name, metav1.GetOptions{}) if err == nil { if len(pod.Spec.NodeName) == 0 { podQueue.AddUnschedulableIfNotPresent(pod, podSchedulingCycle) } break } if errors.IsNotFound(err) { klog.Warningf(\u0026#34;A pod %v no longer exists\u0026#34;, podID) return } klog.Errorf(\u0026#34;Error getting pod %v for retry: %v; retrying...\u0026#34;, podID, err) if getBackoff = getBackoff * 2; getBackoff \u0026gt; maximalGetBackoff { getBackoff = maximalGetBackoff } time.Sleep(getBackoff) } ... ... 图中 a 和 b 分别表示两次调度循环，\na.3 (asummed) 之前 pod 更新事件进入，触发 pod 的二次调度；\npod 的第二次调度 b predicate 阶段返回 fitsError 错误，从而触发 b.3 调度入队 unschedulableQ，但是这时 pod 在 a 调度循环中完成 node bind 操作，导致 (pod.Spec.NodeName != \u0026quot;\u0026quot;)，从而 b.3 没有入队 unschedulableQ；\nb 调度循环中 在 b.6 中将 pod 加入 nominated pod map，因为 pod 已经绑定了node 所以后续不会再进行调度，pod 会一直在 nominatedPodMap 中；\n接下来，解释以下 nominatedPodMap 的作用。nominatedPodMap 数据结构：\n1 2 3 4 5 6 7 8 9 10 11 12 13 // nominatedPodMap is a structure that stores pods nominated to run on nodes. // It exists because nominatedNodeName of pod objects stored in the structure // may be different than what scheduler has here. We should be able to find pods // by their UID and update/delete them. type nominatedPodMap struct { // nominatedPods is a map keyed by a node name and the value is a list of // pods which are nominated to run on the node. These are pods which can be in // the activeQ or unschedulableQ. nominatedPods map[string][]*v1.Pod // nominatedPodToNode is map keyed by a Pod UID to the node name where it is // nominated. nominatedPodToNode map[ktypes.UID]string } nominatedPodMap 记录了 node 关联的 pod，包括 running 状态和候选 pod。在 pod 调度失败时，会进行 node 抢占，为了避免被抢占的 pod 重新夺回 node 资源，所以，在每次 predicate 阶段会将 nominatedPodMap 中 node 关联的 pod 占用的资源也考虑进去。\n1 2 3 4 5 6 7 8 9 10 11 12 func podFitsOnNode() { ...... for i := 0; i \u0026lt; 2; i++ { metaToUse := meta nodeInfoToUse := info if i == 0 { podsAdded, metaToUse, nodeInfoToUse = addNominatedPods(pod, meta, info, queue) } else if !podsAdded || len(failedPredicates) != 0 { break } ..... } 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 // addNominatedPods adds pods with equal or greater priority which are nominated // to run on the node given in nodeInfo to meta and nodeInfo. It returns 1) whether // any pod was found, 2) augmented meta data, 3) augmented nodeInfo. func addNominatedPods(pod *v1.Pod, meta predicates.PredicateMetadata, nodeInfo *schedulernodeinfo.NodeInfo, queue internalqueue.SchedulingQueue) (bool, predicates.PredicateMetadata, *schedulernodeinfo.NodeInfo) { if queue == nil || nodeInfo == nil || nodeInfo.Node() == nil { // This may happen only in tests. return false, meta, nodeInfo } nominatedPods := queue.NominatedPodsForNode(nodeInfo.Node().Name) if nominatedPods == nil || len(nominatedPods) == 0 { return false, meta, nodeInfo } var metaOut predicates.PredicateMetadata if meta != nil { metaOut = meta.ShallowCopy() } nodeInfoOut := nodeInfo.Clone() for _, p := range nominatedPods { if util.GetPodPriority(p) \u0026gt;= util.GetPodPriority(pod) \u0026amp;\u0026amp; p.UID != pod.UID { nodeInfoOut.AddPod(p) if metaOut != nil { metaOut.AddPod(p, nodeInfoOut) } } } return true, metaOut, nodeInfoOut } 至此，我们也分析清楚了资源泄露的问题。\n查看 kubernetes 代码，在 1.18 版本中已经对次问题进行了优化，在pr #86230 中进行了修复。修复逻辑也比较简单：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 func (sched *Scheduler) scheduleOne(ctx context.Context) { fwk := sched.Framework podInfo := sched.NextPod() // pod could be nil when schedulerQueue is closed if podInfo == nil || podInfo.Pod == nil { return } pod := podInfo.Pod if sched.skipPodSchedule(pod) { return } ... } 在 Pod 进入调度循环前，再次进行 skipPodSchedule 判断，这个函数我们上面提到过，在 scheduler 捕获到 pod Update 事件后，会执行这个函数：\n1 2 3 4 5 6 7 8 9 10 // scheduler/eventhandlers.go func (sched *Scheduler) updatePodInSchedulingQueue(oldObj, newObj interface{}) { pod := newObj.(*v1.Pod) if sched.skipPodUpdate(pod) { return } if err := sched.config.SchedulingQueue.Update(oldObj.(*v1.Pod), pod); err != nil { utilruntime.HandleError(fmt.Errorf(\u0026#34;unable to update %T: %v\u0026#34;, newObj, err)) } } skipPodSchedule函数逻辑:\n1 2 3 4 5 // skipPodUpdate checks whether the specified pod update should be ignored. // This function will return true if // - The pod has already been assumed, AND // - The pod has only its ResourceVersion, Spec.NodeName and/or Annotations // updated. pod 被 assumed 是 pod 成功选择要调度的 Node 后，加入调度器中 assumed cache 中，后面就可以进行异步的 bind 操作。assumed cache 会在 bind 结束 30s 后进行清理。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 // assume signals to the cache that a pod is already in the cache, so that binding can be asynchronous. // assume modifies `assumed`. func (sched *Scheduler) assume(assumed *v1.Pod, host string) error { // Optimistically assume that the binding will succeed and send it to apiserver // in the background. // If the binding fails, scheduler will release resources allocated to assumed pod // immediately. assumed.Spec.NodeName = host if err := sched.config.SchedulerCache.AssumePod(assumed); err != nil { klog.Errorf(\u0026#34;scheduler cache AssumePod failed: %v\u0026#34;, err) ... ... } ... ... } 上面说过我们在 update pod annotations 是在 predicate 阶段，会有一定概率 update 早于 assumed 操作，导致再次加入调度循环。\n修复方案 了解了整个问题的原委，目前有两种修复方案：\nk8s 组件方面： 升级 k8s 到 1.18+ 版本； 1.14 和 1.18 功能特性变化较大存在较多不确定性，升级成本较高 GPU 扩展调度组件层面进行修改： 在第二次调度时，判断已经调度时 GPU 扩展调度组件并不是返回空 node 列表，而是返回 error 错误； 能够避免问题 bug 的触发，但是会存在一定的 schedulefailed event 日志； 最后，经过总和考虑，我们采用“方案2”进行修复。\n相关参考 https://github.com/kubernetes/kubernetes/pull/86230 https://github.com/kubernetes/kubernetes/issues/86942 ","date":"2021-09-08T00:00:00Z","image":"https://jasonrd.github.io/blog/p/pod-pending-with-enough-resources/sleep-boy_hud07293a026e0ecb8feb93a2c733978af_122481_120x120_fill_q75_box_smart1.jpg","permalink":"https://jasonrd.github.io/blog/p/pod-pending-with-enough-resources/","title":"集群资源充足情况 Pod 出现 schedulfailed"},{"content":"问题描述 AI 应用在迁移到 k8s 部署后发现应用在启动阶段耗时长，且非常容易失败，查看日志发现应用包找不到 GPU 设备问题。\n问题分析 排查发现出现问题主要在开启了 kubelet static 节点，频繁的出现业务发布失败的情况，报错信息：\n我们进行了一些研究，发现 kubernetes 社区（参考2）和 nvidia-docker （参考3）有相关 issue 讨论，在讨论中 nvidia 员工 klueska 给出了具体原因：\n究其根本，是 nvidia-plugin 在容器启动前会通过 nividia-docker runtime 将 gpu device mount 到容器内部，这些 mount 信息对 docker 是不可见的。我们把 kubernetes 的内核绑定参数 CPUManager=static 开启后，kubelet CPUManager 会定期 update 容器 cpuset cgroup 配置，这时上面 nvidia mount 的 device 被清除，导致无法分配 gpu 。nvidia-plugin 通过在 kubelet 调用 Allocate 接口时返回 device 列表，修复了该bug。\nsummarize：已定位到原因，nvidia 的 k8s 插件和 k8s 绑核功能不兼容导致，需要升级 nvidia 插件。\n验证 我们在线下搭建了 gpu 环境，分别使用 10.57.33.30、10.57.33.31 两台机器做对比测试。首先，我们替换 Nvidia daemonset 容器镜像为修复后的版本 ps/nvidia/nvidia/k8s-device-plugin:1.0.0-beta6，并且修改 daemonset 的升级策略为 OnDelete，这样删除 33.30 机器上 Nvidia pod ，新建的 pod 使用的镜像为升级后的版本。\n有问题插件验证 我们测试 33.31 上的 ai-face-gen 应用，进入 pod 内部，执行命令 nvidia-smi ，会报 nvm unknown error：\n我们安装参考3中的相关描述，docker inspect 容器，查看一下挂载的 devices 列表：\n会发现列表为空。然后，我们测试升级后应用报错场景。删除 pod af-55666f8bf8-tffxg，容器重建后的 pod 出现启动失败：\n查看应用日志，报错和线上场景相同：\n升级新版插件验证 我们首先将 33.30 机器的 nvidia-plugin 升级到最新版本（其中1.11 版本为内部的版本号，1.0.0-beta6 为社区修复gpu 问题的最新版本）：\n然后和上面同样的操作流程，我们首先进入容器 af-55666f8bf8-7z4ld 内部执行命令 nvidia-smi：\n可以发现命令执行正常。然后 同样使用 docker inspect 命令查看容器挂载 device 列表：\ndevice 列表中挂载了 gpu 显卡和相关驱动。最后，我们删除一下容器 af-55666f8bf8-7z4ld，验证一下容器发布是否会出现失败的情况：\npod 启动正常，执行 nvidia-smi 命令正常：\n参考 Nvidia GPU如何在Kubernetes 里工作 Updating cpu-manager-policy=static causes NVML unknown error NVIDIA NVML Driver/library version mismatch 解决Driver/library version mismatch 升级 nvidia 驱动 https://developer.download.nvidia.cn/compute/cuda/repos/rhel7/x86_64/ https://developer.download.nvidia.cn/compute/cuda/repos/rhel7/x86_64/cuda-rhel7.repo ","date":"2021-06-05T00:00:00Z","permalink":"https://jasonrd.github.io/blog/p/gpu-start-failure/","title":"kubelet 开启 static 引发 gpu 容器部署异常"}]